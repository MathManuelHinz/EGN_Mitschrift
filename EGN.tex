\documentclass{book}
\usepackage[ngerman]{babel}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{xhfill}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{float}
\usepackage{esint}
\usepackage{hyperref}
\usepackage[most]{tcolorbox}
\usepackage[chapter]{algorithm}
\usepackage{algpseudocode}


\graphicspath{ {./imgs/} }
\usepackage[
  a4paper,
  textwidth=175mm,
  textheight=225mm,
  heightrounded,
]{geometry}

\pagestyle{fancy}
\lhead{}
\chead{\thechapter. \chaptername}
\rhead{}
\lfoot{}
\cfoot{\thepage}
\rfoot{}

\author{Manuel Hinz}

\title{Einführung in die Grundlagen der Numerik (WS 22/23)}

\newtheorem{theorem}[algorithm]{Satz}
\newtheorem{corollary}[algorithm]{Korollar}
\newtheorem{lemma}[algorithm]{Lemma}
\newtheorem{definition}[algorithm]{Definition}
\newtheorem{remark}[algorithm]{Bemerkung}
\newtheorem*{mremark}{Eingefügte Bemerkung}
\newtheorem{example}[algorithm]{Beispiel}
\newtheorem*{mdefinition}{Eingefügte Definition}
\newtheorem*{mexample}{Eingefügtes Beispiel}



\def\C{\mathbb{C}}
\def\R{\mathbb{R}}
\def\N{\mathbb{N}}
\def\Z{\mathbb{Z}}
\def\E{\mathbb{E}}
\def\V{\mathbb{V}}
\def\rang{\text{rang}}
\def\cond{\text{cond}}
\def\eps{\text{eps}}
\raggedright

\let\cleardoublepage=\clearpage

\begin{document}

    \maketitle

    \tableofcontents

    \section*{Vorwort}

            Diese Mitschrift von der Vorlesung Einführung in die Grundlagen der Numerik (Dölz,WS 2022/2023) 
            wird von mir neben der Vorlesung geschrieben und ist dementsprechend Fehleranfällig. Fehler gerne an 
            mh@mssh.dev! 

    \chapter{Orthogonalität}

        \section{Grundlegende Definitionen}

            \begin{definition}\label{d1.1}
                Sei $X$ ein $\R$ Vektorraum und $\langle\cdot,\cdot\rangle:X\times X \to \R$ eine Abbildung. $\langle\cdot,\cdot\rangle$ heißt \textbf{Skalarprodukt} oder inneres Produkt, falls
                \begin{equation}
                    \tag{Positiviät}
                    \forall f\in X\setminus 0: \langle f,f \rangle>0
                \end{equation}
                \begin{equation}
                    \tag{Symmetrie}
                    \forall f,g\in X: \langle f,g \rangle = \langle g,f \rangle
                \end{equation}
                \begin{equation}
                    \tag{Linearität im ersten Argument}
                    \forall \alpha,\beta\in\R, f,g,h\in X: \langle \alpha f+\beta g,h \rangle
                    = \alpha \langle f,h \rangle + \beta \langle g,h \rangle
                \end{equation}
            \end{definition}

            \begin{remark}\label{b1.2}
                Symmetrie und Linearität im ersten Argument implizieren, 
                dass $\langle \cdot,\cdot  \rangle$ eine bilineare Abbildung ist. 
            \end{remark}

            \begin{definition}\label{d1.3}
                Sei $X$ ein $\R$-Vektorraum mit Skalarprodukt $\langle \cdot,\cdot \rangle$.
                Wir bezeichnen die zugehörige \textbf{Norm} (in Abhänigkeit von einem Vektor $f\in X$) mit 
                \begin{equation*}
                    \Vert f \Vert =\sqrt{\langle f,f\rangle}.
                \end{equation*}
            \end{definition}

            \begin{lemma}\label{l1.4}
                Sei $X$ ein $\R$-Vektorraum mit Skalarprodukt $\langle \cdot,\cdot \rangle$. Dann gil die \underline{Cauchy-Schwarz-Ungleichung}:
                \begin{equation}
                    \tag{C.S.}
                    \forall f,g\in X:\langle f,g \rangle\leq \Vert f\Vert \cdot\Vert g \Vert
                \end{equation}
                mit Gleichheit genau dann, wenn $f$ und $g$ linear abhängig sind.
            \end{lemma}
            \begin{proof}
                O.B.d.A. $f,g\neq 0$, da sonst offensichtlich Gleichheit gilt.
                Sei $\alpha\neq 0$, dann gilt mit $f,g\in X$ und $\alpha \in \R$:
                \begin{equation*}
                    0\leq \Vert f -\alpha g\Vert^2= \langle f-\alpha g,f-\alpha g \rangle
                    = \Vert f \Vert^2 -2\alpha \langle f,g \rangle+\alpha^2 \Vert g \Vert^2
                \end{equation*}
                Wählen wir jetzt $\alpha=\frac{\langle f,g \rangle}{\Vert g \Vert^2}$ folgt:
                \begin{equation*}
                    0\leq \Vert f \Vert^2-\frac{2 \langle f,g \rangle^2}{\Vert g \Vert^2}+\frac{\langle f,g \rangle^2}{\Vert g \Vert^2}
                \end{equation*}
                \begin{equation*}
                    \implies \langle f,g \rangle^2\leq \Vert f \Vert^2\cdot \Vert g \Vert^2.
                \end{equation*}
            \end{proof}
            \begin{mremark}
                Rechnung zur Begründung von $ \langle f-\alpha g,f-\alpha g \rangle
                = \Vert f \Vert^2 -2 \Vert\alpha \langle f,g \rangle+\alpha^2 \Vert g \Vert^2$:
                \begin{align*}
                    &\langle f-\alpha g,f-\alpha g \rangle\\
                    &=\langle f,f-\alpha g \rangle-\alpha \langle g,f-\alpha g \rangle\\
                    &=\langle f,f \rangle-\alpha \langle f,g \rangle-\alpha \langle g,f \rangle+\alpha^2 \langle g,g \rangle\\
                    &=\Vert f \Vert^2 -2 \Vert\alpha \langle f,g \rangle+\alpha^2 \Vert g \Vert^2
                \end{align*}
            \end{mremark}
            \begin{example}\label{b1.5}
                \begin{enumerate}
                    \item $X=\R^n$ und $\langle x,y \rangle=\sum_{i=1}^n x_iy_i$ (Euklidisches Skalarprodukt)
                    \item $X=\R^n$, $\langle x,y \rangle=x^\perp A y$, wobei $A$ positiv definit und symmetrisch ist
                    \item $I=[a,b],w:I\to\R$ beschränkt und strikt positiv:
                        \begin{equation*}
                            X=\left\{f:I\to\R:\int_a^b f(x)^2w(t)dt<\infty\right\}=L^2(I,w)
                        \end{equation*}
                        mit 
                        \begin{equation*}
                            \langle f,g \rangle = \int_a^b f(t)g(t)w(t)dt
                        \end{equation*}
                \end{enumerate}
            \end{example}
            \begin{mremark}
                Die Definition von $L^2(I,w)$ ist hier nicht ganz richtig, man müsste natürlich noch Äquivalenzklassen, bzgl. Gleichheit bis auf Nullmengen, bilden. 
                Dies wird hier, da Analysis 3 / Wtheo. nicht nicht vorrausgesetzt wird, ignoriert.
            \end{mremark}
            \begin{definition}\label{d1.6}
                Sei $X$ ein $\R$-VR mit Skalarprodukt $\langle \cdot,\cdot \rangle$.
                $f,g\in X$ heißen \textbf{orthogonal}, falls $\langle f,g \rangle=0$.
            \end{definition}
            \begin{remark}\label{b1.7}
                Im $\R^n$ mit dem euklidischen Skalarprodukt stimmt Definition \ref*{d1.6}, wegen 
                \begin{equation*}
                    \langle x,y \rangle = \Vert x \Vert \Vert y \Vert \cos(\theta), \theta=\angle(x,y),
                \end{equation*}
                mit unserem bisherigen Verständnis überein.
            \end{remark}
        \section{Bestapproximationseigenschaft}
            \begin{definition}\label{d1.8}
                Sei $V$ ein $\R$-VR mit Skalarprodukt $\langle \cdot,\cdot \rangle$ und $U$ ein Unterraum.
                \begin{equation*}
                    U^\perp = \left \{ v\in V: \langle v,u \rangle=0,\forall u\in U\right\}
                \end{equation*}
                heißt das \textbf{orthogonale Komplement} von $U$.
            \end{definition}
            \begin{theorem}\label{s1.9}
                Unter den Annahmen von Definition \ref*{d1.8} und der  zusätzlichen Annahme, dass $U$ endlich dimensional ist, gilt folgendes für $v\in V$:
                \begin{equation*}
                    \Vert v-u \Vert = \min_{w\in U}\Vert v-w\Vert
                \end{equation*}
                genau dann, wenn $v-u\in U^\perp$.
            \end{theorem}
            \begin{example}\label{b1.10}
                $V=\R^2$, $U=\text{span}\left\{\begin{pmatrix}1\\1\end{pmatrix}\right\}$ mit euklidischem Skalarprodukt $\langle \cdot,\cdot \rangle$.
                Dann ist $U^\perp = \text{span}\left\{\begin{pmatrix}1\\-1\end{pmatrix}\right\}$. 
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.25\textwidth]{Bild001}
                    \caption{\(U\) und \(U^\perp\)}
                \end{figure}
            \end{example}
            \begin{proof}[Beweis von Satz \ref*{s1.9}]
                Sei $v\in V$ und seien $u,w\in U$. Dann gilt:
                \begin{equation*}
                    \Vert v-w \Vert^2=\langle v-w,v-w \rangle = \langle (v-u)+(u-w),(v-u)+(u-w) \rangle
                \end{equation*}
                \begin{equation*}
                    =\Vert v-u \Vert^2+2 \langle v-u,\underbrace{u-w}_{\in U} \rangle +\Vert u-w \Vert^2\geq \Vert v-u \Vert^2
                \end{equation*}
                mit Gleichheit genau dann, wenn $w-u=0$ (da dann der $\Vert u-w \Vert$ Term verschwindet).
            \end{proof}
            \begin{remark}\label{b1.11} 
                Der Satz sagt, dass es zu \underline{jedem} $v\in V$ ein \underline{eindeutiges, bestmögliches} $u\in U$ gibt.
            \end{remark}
            \begin{definition}\label{d1.12} 
                Die Lösung aus Satz \ref*{s1.9} heißt \textbf{orthogonale Projektion} von $v$ auf $U$. Die Abbildung 
                \begin{equation*}
                    P:V\to U, v\mapsto P(v) \textbf{ mit } \Vert v-Pv \Vert=\min_{w\in U}\Vert v-w \Vert
                \end{equation*}
                ist linear und wird \textbf{orthogonale Projektion} genannt.
            \end{definition}
            \begin{mremark}[Beweis der Linearität]
                Für $v_1,v_2\in V$ und $\alpha\in \R$ gilt:
                \begin{align*}
                    v_1-Pv_1\in U^\perp\\
                    v_2-Pv_2\in U^\perp 
                \end{align*}
                Daher 
                \begin{align*}
                    \alpha(v_1-Pv_1)+(v_2-Pv_2)=(\alpha v_1+v_2) - (\alpha Pv_1 + Pv_2) \in U^\perp.
                \end{align*}
                Aber dann muss $\alpha Pv_1+Pv_2$ schon, wegen der Eindeutigkeit, $P(\alpha v_1+v_2)$ sein.
            \end{mremark}
            \begin{remark}\label{b1.13}
                Satz \ref{s1.9} gilt auch, wenn $U$ durch $W=w_0+U$ ersetzt wird.
                Die orthogonale Projektion ist analog definiert..
            \end{remark}

            \underline{\textbf{Frage:}} Die Orthogonale Projektion hat offenbar gute Eigenschaften. Aber: wie berechnen wir sie? Wie wählen wir $U$?
            
            \begin{itemize}
                \item Berechnung ist leicht
                \item U wählen schwierig
            \end{itemize}

        \section{Orthonormalbasen}

            \begin{definition}\label{d1.14}
                Sei $X$ ein $\R$-VR mit Skalarprodukt $\langle \cdot,\cdot \rangle$ und
                $X_n\subset X$ ein endlich dimensionaler Teilraum mit Basis $\{\varphi_1,\dots,\varphi_n\}$.
                Die Basis heißt \textbf{Orthogonalbasis}, falls 
                \begin{equation*}
                    \forall i\neq j: \langle \varphi_i,\varphi_j \rangle=0
                \end{equation*}
                gilt und Orthonormalbasis (ONB), falls zusätzlich $\Vert\varphi_i\Vert=1$ gilt. Das impliziert:
                \begin{equation*}
                    \langle \varphi_i,\varphi_j \rangle =\delta_{i,j}.
                \end{equation*}
            \end{definition}

            \begin{example}\label{b1.15}    
                \begin{enumerate}
                    \item $\R^n$ mit euklidischem Skalarprodukt und kanonischer Basis
                    \item $X=L^2(I,1)$ mit entsprechendem Skalarprodukt und $X_n$ der Raum der trigonometrischen Polynome bis Grad $n$.
                    Dann ist folgendes eine ONB:
                    \begin{equation*}
                        \left\{\frac{1}{\sqrt{2\pi}},\frac{\sin(x)}{\sqrt{\pi}},\frac{\cos(x)}{\sqrt{\pi}},\dots,\frac{\sin(nx)}{\sqrt{\pi}},\frac{\cos(nx)}{\sqrt{\pi}}\right\}
                    \end{equation*}
                \end{enumerate}
            \end{example}

            \begin{mremark}
                Trigonometrische Polynome sind Funktionen der Form
                \begin{equation*}
                    f(t)=\sum_{k=1}^n a_k \cos(kx)+ b_k \sin(kx).
                \end{equation*}
                Die größte Faktor vor dem $x$ ist der Grad eine trigonometrischen Polynoms.
            \end{mremark}

            \begin{theorem}\label{s1.16}
                Sei $\{\varphi_1,\dots,\varphi_n\}$ eine ONB von $X_n\subset X$. Dann gilt
                \begin{enumerate}
                    \item $f=\sum_{i=1}^n \langle \varphi_i,f \rangle\varphi_i$
                    \item $\Vert f \Vert^2=\sum_{i=1}^n \langle \varphi_i,f \rangle^2$
                    \item Die orthogonale Projektion $f_n$ von $f\in X\setminus X_n$ ist gegeben durch 
                        \begin{equation*}
                            f_n=\sum_{i=1}^n \langle \varphi_i,f \rangle\varphi_i
                        \end{equation*}
                    \item im Fall von 3.:
                        \begin{equation*}
                            \Vert f_n \Vert^2 = \sum_{i=1}^n \langle \varphi_i,f \rangle^2 \leq \Vert f \Vert
                        \end{equation*}
                \end{enumerate}
            \end{theorem}
            
            \begin{proof}
                1.: 
                \begin{align*}
                    &f\in X_n\implies \exists \alpha_i\in \R: f=\sum_{i=1}^n\alpha_i\varphi_i\\
                    &\implies \langle \varphi_i,f \rangle= \langle \varphi_i, \sum_{j=1}^n\alpha_j\varphi_j\rangle
                    =\sum_{j=1}^n \alpha_j \langle \varphi_i,\varphi_j \rangle =\alpha_i
                \end{align*}
                2.:
                \begin{align*}
                    &\Vert f \Vert^2 = \langle f,f \rangle \\
                    &= \langle \sum_{i=1}^n \alpha_i \varphi_i, \sum_{j=1}^n \alpha_j \varphi_j \rangle =\sum_{i,j=1}^n\alpha_i\alpha_j\delta_{i,j}=\sum_{i=1}^n\alpha_i^2
                \end{align*}
                3.:
                \begin{align}
                    f\in X\setminus X_n:&\nonumber\\
                    \Vert f-\underbrace{\tilde{f}_n}_{\in X_n} \Vert &= \langle f-\sum_{i=1}^n\tilde{\alpha}_i\varphi_i,f-\sum_{i=1}^n\tilde{\alpha}_i\varphi_i \rangle\nonumber\\
                    &=\Vert f \Vert^2-2\sum_{i=1}^n\tilde{\alpha}_i \underbrace{\langle \varphi_i,f \rangle}_{\eqqcolon \alpha_i}+\sum_{i,j=1}^n\alpha_i\alpha_j \langle \varphi_i,\varphi_j \rangle \nonumber\\
                    &=\Vert f \Vert^2-\sum_{i=1}^n\tilde{\alpha}_i\alpha_i+\sum_{i=1}^n\tilde{\alpha}_i^2
                    \stackrel{\text{Quadratische Ergänzung}}{=} \Vert f \Vert^2-\sum_{i=1}^n\alpha_i^2+\sum_{i=1}^n\underbrace{(\alpha_i-\tilde{\alpha}_i)^2}_{\geq 0}
                \end{align}
                Dies wird minimiert, wenn $\tilde{\alpha}_i=\alpha_i$ ist.
                
                4.:

                $f\in X_n$ wurde in 2. gezeigt. Sonst:
                \begin{equation*}
                    f\notin x_n\implies \text{ mit } \alpha_i=\tilde{\alpha}_i \text{ in } (1.1):
                \end{equation*}
                \begin{equation*}
                    0\leq \Vert f-f_n \Vert^2=\Vert f \Vert^2-\sum_{i=1}^n\underbrace{\alpha_i^2}_{\langle \varphi_i,f \rangle^2}
                \end{equation*}
                Es folgt die Behauptung.
            \end{proof}

            \underline{\textbf{Vorteile von Orthogonalität:}}
            \begin{itemize}
                \item Bestapproximation
                \item Einfache Basisdarstellung
            \end{itemize}

        \noindent
        \xrfill[0.7ex]{1pt}Ende von Vorlesung 01 am 11.10.2022\xrfill[0.7ex]{1pt}

    \chapter{Das lineare Ausgleichsproblem}

        \section{Problemstellung und Normalengleichung}

            Gegeben seien Punkte $(t_i,b_i)\in\R^2$ mit $i=1,\dots,m$. Wir nehmen an, dass es eine Gestzmäßigkeit im Sinne eines
            parameterabhängigen Modelles 
            \[
                b_i=b(t_i)=b(t_i;\underbrace{x_1,\dots,x_n}_{\text{Parameter}}),
            \]
            wobei die Parameter $x_1,\dots,x_n$ unbekannt seien, gibt. In der Praxis sind die Messungen zusätzlich mit Fehlern
            behaftet und das Modell gilt nur approximativ. Zusätzlich gibt es oft mehr Messungen als Parameter, d.h. $m>n$.

            \underline{\textbf{Frage:}} Gegeben die Messungen, können wir zugehörige Parameter bestimmen?

            \underline{\textbf{Annahme:}} $b$ ist linear in den Parametern, d.h. es gibt Funktionen 
            \[a_i:\R\to\R\]
            s.d.
            \[b(t;x_1,\dots,x_n)=a_1(t)x_1+\dots+a_n(t)x_n.\]
            \underline{\textbf{Idee:}} Formuliere ein lineares Gleichungssystem:
            \[b_i \approx b(t_i;x_1,\dots,x_n)=a_1(t_i)x_1+\dots+a_n(t_i)x_n, i=1,\dots,m\]
            kurz $Ax\approx b$ mit $A\in\R^{m\times n},x\in\R^n,b\in\R^m$.

            \underline{\textbf{Problem:}} Durch Modell- und Messfehler gilt das Gleichungssystem nur ungefähr, und wir 
            mehr Gleichungen als Unbekannte (``das Gleichungssystem ist überbestimmt''). Wir können unser Gleichungssystem 
            also im Allgemeinen nicht lösen.

            \begin{example}\label{b2.1}
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.25\textwidth]{Bild002}
                    \caption{Datenpunkte und approximierte Gerade}
                \end{figure}
            \end{example}

            \underline{\textbf{Idee:}} Finde Parameter, sodass das Modell ``bestmöglich'' mit den Messpunkten übereinstimmt, 
            d.h. finde $(x_1,\dots,x_n)^t=x\in\R^n$ s.d.:
            \begin{equation}\label{g2.1}
                \Vert Ax-b \Vert=\min_{y\in\R^n} \Vert Ay-b \Vert
            \end{equation}
            
            \begin{definition}\label{d2.2}
                Die Gleichung (\ref{g2.1}) heißt \textbf{lineares Ausgleichsproblem}. Der Term $Ax-b$ heißt \textbf{Residuum}.
            \end{definition}

            \underline{\textbf{Bemerke:}} $V=\R^m,U=\text{Bild}(A)\subset V,\dim(\text{Bild}(A))\underbrace{\leq n\leq m}_{\text{Grundannahme}}$

            Statte $V$ mit euklidischem Skalarprodukt aus.

            $\stackrel{Satz \ref{s1.9}}{\implies}$ Es gibt genau ein $Ax\in$ Bild$(A)$ so, dass
            \[\Vert Ax-b \Vert=\min_{w\in U} \Vert w-b \Vert\]
            gilt.

            \underline{\textbf{Aber:}} Wie berechnen wir $x$?

            \begin{theorem}\label{s2.3}
                Sei $A\in\R^{m\times n},b\in\R^m,m\geq n,x\in\R^n$ ist genau dann eine Lösung von (\ref{g2.1}) 
                bezüglich der euklidischen Norm, falls
                \begin{equation}\label{g2.2}
                    A^tAx=A^tb.
                \end{equation}
                Insbesondere ist das lineare Ausgleichproblem genau dann lösbar, falls $\rang(A)=n$.
            \end{theorem}

            \begin{proof}
                \begin{align*}
                    &\Vert Ax-b \Vert=\min_{y\in\R^n} \Vert Ay-b \Vert\\
                    &\stackrel{\text{Satz } (\ref{s1.9})}{\iff}Ax-b\in U^\perp=\text{Bild}(A)^\perp\\
                    &\iff \forall y\in\R^n: \langle Ax-b, Ay\rangle=0\\
                    &\iff \forall y\in\R^n: \langle A^tAx-A^tb,y \rangle=0\\
                    &\iff A^tAx=A^tb
                \end{align*}
                Die letzte Gleichung ist genau dann invertierbar, wenn $A^tA$ vollen Rang hat, also wenn $A$ vollen Rang ($n$) hat.
            \end{proof}

            \begin{remark}\label{b2.4}
                Im beweis verwenden wir, dass $Ax-b$ orthogonal zu $U=\text{Bild}(A)$,

                    \begin{figure}[H]
                        \centering
                        \includegraphics[width=0.25\textwidth]{Bild003}
                        \caption{Hyperebene und Projektion}
                    \end{figure}

                d.h. eine Normale zur Hyperebene Bild$(A)$ im $R^m$, ist. Deshalb heißt (\ref{g2.2}) auch \textbf{Normalengleichung}.
            \end{remark}

            \begin{remark}\label{b2.5}
                Für $m=n$ und $\rang(A)=n$ ist die Lösung des linearen Ausgleichproblems exakt (im mathematischen Sinne).
            \end{remark}

            \begin{theorem}\label{s2.6}
                Für $A\in\R^{m\times n}$ ist $A^tA$ symmetrisch und positiv semidefinit. Falls $m\geq n$ ist $A^tA$ genau 
                dann positiv definit, wenn $\rang(A)=n$.
            \end{theorem}
            \begin{proof}
                \begin{itemize}
                    \item Symmetrisch: klar
                    \item positiv semidefinit:
                    \[\forall x\in \R^n: x^t(A^tA)x=(Ax^t)(Ax)=\Vert Ax \Vert_2^2\geq 0\]
                    \item positiv definit: $\rang(A)=n\implies Ax=0\iff x=0\implies \Vert Ax \Vert_2=0\iff x=0\implies $ Behauptung.
                \end{itemize}
            \end{proof}
        
        Einfachste Möglichkeit zur Lösung von (\ref{g2.2}): Berechne $A^tA,A^tb$, löse LGS mittels Cholesky. Kosten sind ungefähr:
        \[\frac{n^2m}{2}+m\cdot n+\frac{n^3}{6}+\frac{n^2}{2}+\frac{n^2}{2}\approx\frac{mn^2}{2} \text{ für } m\gg n.\]
        
        \begin{mremark}
            Anmerkung vom Donzent: $A^tA$ eig. immer schlecht zu berechnen.
        \end{mremark}

        %Hier Konditionierung nacharbeiten

        \underline{\textbf{Aber:}} Dieser Vorgang ist schlechter konditioniert als das lineare Ausgleichsproblem: 
        
        \begin{tcolorbox}[enhanced,breakable,
            title=Eingeschobene Definition / Wiederholung]
            \[
                \cond(A)=\Vert A \Vert \Vert A^{-1} \Vert
            \]
            \[
                \Vert A \Vert =\max_{\Vert x \Vert=1} \Vert Ax \Vert
            \]
        \end{tcolorbox}

        Falls $A\in\R^{n\times n}$ spd (symmetrisch, positiv definit) gilt $\cond_2((A^tA))=\cond_2(A)^2$. 

        Für $A\in\R^{m\times n}$ gelten ähnliche Überlegungen, siehe Deuflhard \& Hohmann.

        \begin{example}\label{b2.7}
            Sei $A=\begin{bmatrix}
                1 & 1\\\epsilon&0\\0 & \epsilon
            \end{bmatrix}$ mit $\epsilon>\underbrace{\eps}_{\text{Maschienengenauigkeit}},\epsilon^2<\eps$.
            \[\implies A^tA=\begin{bmatrix}
                1 +\epsilon^2 & 1\\
                1 & 1+\epsilon^2
            \end{bmatrix}\stackrel{\text{im Computer}}{=}\begin{bmatrix}
                1 & 1\\
                1 & 1
            \end{bmatrix}\]
            $\implies$ $A^tA$ ist im Computer singulär, obwohl A vollen Rang hat!
        \end{example}

        \underline{\textbf{Idee / Wunsch:}} Gebe einen Algorithmus an, der das lineare Ausgleichsproblem löst und nur 
        auf $A$ arbeitet.

        \section{Methode der Orthogonalisierung}

            \begin{definition}\label{d2.8}
                Eine Matrix $Q\in\R^{n\times n}$ heißt \textbf{orthogonal}, wenn $Q^tQ=I$, d.h. falls die Spalten von $Q$ eine ONB bzgl. des 
                euklidischen Skalarprodukts bilden. Schreibe $Q\in O(n)$.
            \end{definition}

            \underline{\textbf{Notation:}} $\langle \cdot,\cdot \rangle_2,\Vert \cdot \Vert_2$ für das euklidische Skalarprodukt / die euklidische Norm.

            \begin{lemma}\label{l2.9}
                Für alle $Q\in O(n)$ gilt
                \begin{enumerate}
                    \item $\Vert Qx \Vert_2=\Vert x \Vert_2$ (Invarianz der Norm bzgl. orthogonaler Projektionen)
                    \item $\cond_2(Q)=1$
                \end{enumerate}
            \end{lemma}

            \begin{proof}
                1.: $\Vert Qx \Vert_2^2=\langle Qx,Qx \rangle_2=\langle Q^tQx,x \rangle_2=\langle x,x \rangle_2=\Vert x \Vert_2^2$

                2.: $\Vert Q \Vert_2=\max_{\Vert x \Vert_2=1}\Vert Qx \Vert=1$ und auch $\Vert Q^-1 \Vert_2=1\implies$ Behauptung.
            \end{proof}

            \begin{theorem}\label{s2.10}
                $A\in\R^{m\times n},m\geq n,\rang(A)=n$. Dann hat $A$ eine QR-Zerlegung:
                \begin{equation*}
                    A=Q\begin{pmatrix*}
                        R\\
                        0
                    \end{pmatrix*}
                \end{equation*}
                wobei $Q\in O(m),R\in\R^{n\times n}$ eine obere Dreiecksmatrix ist.
            \end{theorem}

            \begin{proof}
                Schreibe das Gram-Schmidt-Orthogonalisierungsverfahren in Matrixform:
                \[
                Q=\underbrace{\begin{bmatrix}
                    &&&\\
                    &&&\\
                    &&&\\
                    A_n & \dots & A_2 & A_1\\
                    &&&\\
                    &&&\\
                    &&&
                \end{bmatrix}
                \underbrace{
                \begin{bmatrix}
                    1 & \dots  & \dots & \dots & \frac{-\langle A_n,A_1 \rangle_2}{\Vert A_1 \Vert_2^2}\\
                    & \ddots & \dots & \dots & \vdots \\
                    &        & 1 & \frac{-\langle A_3,A_2 \rangle_2}{\Vert A_2 \Vert_2^2} & \frac{-\langle A_3,A_1 \rangle_2}{\Vert A_1 \Vert_2^2}\\
                    & \textbf{0} &  & 1 & \frac{-\langle A_2,A_1 \rangle_2}{\Vert A_1 \Vert_2^2} \\
                    & & &  & 1
                \end{bmatrix}}_{R'}}_{\begin{bmatrix}
                    B_n & \dots &  B_1
                \end{bmatrix}} 
                \underbrace{
                \begin{bmatrix}
                    \frac{1}{\Vert B_1 \Vert_2} & & 0\\
                    & \ddots& \\
                    0 & &  \frac{1}{\Vert B_n \Vert_2}
                \end{bmatrix}}_{R''}
                \]

                $\implies Q\in R^{m\times n},R'R''$ ist obere Dreiecksmatrix mit nicht-null Diagonaleinträgen

                $\implies$ invertierbar: $R=(R'R'')^{-1}$
                
                $\implies QR=A$, wenn wir $Q$ zu einer ONB von $R^m$ erweitern.
            \end{proof}
        
            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 02 am 13.10.2022\xrfill[0.7ex]{1pt}
            
            \begin{theorem}\label{s2.11}
                Sei $A\in\R^{m\times n},m\geq n,\rang(A)=n,b\in\R^n$. Sei $A=QR$ eine $QR$-Zerlegung von $A$ und 
                \[\underbrace{Q^tA}_{=R}=Q^tb=\begin{bmatrix}
                    b_1\\
                    b_2
                \end{bmatrix}\begin{array}{c}
                    \in \R^n \\
                    \in\R^{m-n}
                \end{array}.\]
                Dann ist $x=R_1^-1b_1$ die Lösung des linearen Ausgleichsproblems, wobei $R_1\in\R^{n\times n}$ der obere Teil von $R$ ist.
            \end{theorem}

            \begin{proof}
                \begin{align*}
                    \Vert Ax-b \Vert_2^2&\stackrel{\text{Lemma \ref{l2.9}}}{=}\Vert Q^t(Ax-b) \Vert_2^2\\
                    & =\left\Vert \begin{array}{c}
                        R_1 x-b\\ b_2
                    \end{array} \right\Vert_2^2 = \left\Vert R_1x-b_1 \right\Vert_2^2+ \left\Vert b_2 \right\Vert_2^2\\
                    & \geq \left\Vert b_2 \right\Vert_2^2
                \end{align*}

                $n=\rang(A)=\rang(R)=\rang(R_1)\implies$ $R_1$ invertierbar $\implies$ Behauptung

            \end{proof}

            \underline{\textbf{Problem:}}

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild004}
                \caption{Problemstellung}
            \end{figure}

            $w_2=v_2-\frac{\langle v_2,v_1 \rangle_2}{\langle v_1,v_1 \rangle_2}v_1$ ist problematisch, falls $v_1\approx v_2$ (Auslöschung).
            Beim Gram-Schmidt-Verfahren können Rundungsfehler auftreten. Es ist instabil.

            \underline{\textbf{Ziel:}} Stabiler Algorithmus um $QR$-Zerlegungen zu berechnen.

        \section{Grundüberlegungen zu Orthogonalisierungsverfahren}

            \underline{\textbf{Problemstellung:}} Gegeben $v_1=\alpha e_1\in\R^2,v_2\in\R^2$ transformiere $v_2$ auf $\tilde{w_2}=\beta e_2$, gebe $\beta$ an.

            \underline{\textbf{Gram-Schmidt:}} $\beta=\left\Vert w \right\Vert_2$

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild005}
                \caption{Gram-Schmidt}
            \end{figure}


            \underline{\textbf{Drehungen:}} $\tilde{w}_2= Qv_2$
            \[Q=\begin{bmatrix}
                \cos(-\theta) & \sin(-\theta) \\
                -\sin(-\theta) & \cos(-\theta)
            \end{bmatrix}\]
            \[\beta = \left\Vert v_2 \right\Vert_2\]

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild006}
                \caption{Drehungsansatz}
            \end{figure}


            \underline{\textbf{Spiegelungen:}} $\tilde{w}_2=Qv_2$, $Q=I-2\frac{vv^t}{v^tv}$ und $\beta=\left\Vert v_2 \right\Vert_2$

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild007}
                \caption{Spiegelungsansatz}
            \end{figure}


            \underline{\textbf{Idee:}} Benutze orthogonale Transformationen $Q_1,\dots,Q_n$ um $A\in\R^{m\times n}$ 
            ,$\rang(A)=n$, sukzessive zu reduzieren.
            \[A\rightsquigarrow Q_1A\rightsquigarrow Q_2Q_1 A\rightsquigarrow\dots\rightsquigarrow \begin{bmatrix}
                &R_1\\
                0 &\\
                \\
                & 0 &
            \end{bmatrix}\]

            Weil $\cond_2(Q)=1$ ist die Vorgehensweise stabil, bzw. gut konditioniert.

            \underline{\textbf{Aber:}} Wie wählen wir $Q_1,\dots,Q_n$?

        \section{$QR$-Zerlegung mittels Givens-Rotationen}

            \begin{definition}\label{d2.12}
                Eine Matrix der Form
                
                \[
                    \delta_{k,l}=\begin{bmatrix}
                        1 & & & & & & & & \\
                        & \ddots & & & & & & & \\
                        & & c & &  & s & & & \\
                        & & & 1 &  & & & & \\
                        & & & &  \ddots  & & & & \\
                        & & -s & & & c & & & \\
                        & &  & & & & 1 & & \\
                        & &  & & & & & \ddots & \\
                        & &  & & & & &  & 1\\
                    \end{bmatrix}
                \]
                , wobei die $s,c$ Einträge in der $k,l$ten Zeile / Spalte sind, heißen Givens-Rotationen.
            \end{definition}

            \underline{\textbf{Bemerke:}} Für $c=\cos(\theta),s=\sin(\theta)$ ist $\delta_{k,l}$ eine Drehung um $\theta$ in 
            in der Koordinaten $(k,l)$. $\delta_{k,l}$ ist Orthogonal.

            \underline{\textbf{Frage:}} Wie wählen wir $c,s$? 
            
            Gegeben $x\in\R^n$, elemeniere $l$te Koordinate zu $0$.

            \[\begin{bmatrix}
                c & s\\ -s & c
            \end{bmatrix}=\begin{bmatrix}
                x_k\\ x_l
            \end{bmatrix}=\begin{bmatrix}
                r\\0
            \end{bmatrix}\]

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild008}
                \caption{Trigonometriesetting}
            \end{figure}

            \begin{equation*}
                r^2=x_k^2+x_l^2
                \implies \pm \sqrt{x_k^2+x_l^2}
            \end{equation*}

            \underline{\textbf{Aber:}} Diese Berechnungsweise ist nicht unbedingt stabil ($x_k\gg x_l$)

            Stabile Variante:


            \begin{equation}\label{e2.3}
                \begin{array}{c}
                    \text{Falls }\left\vert x_l \right\vert>\left\vert x_k \right\vert\implies \tau =\frac{x_k}{x_l},s=\frac{1}{\sqrt{1+\tau^2}},c=s\tau\\
                    \text{Sonst: }\tau=\frac{x_l}{x_k},c=\frac{1}{\sqrt{1+\tau^2}},s=c\tau
                \end{array}
            \end{equation}
        

            Beispielprozess:

            \[
            \begin{bmatrix}
                * & * & * \\
                * & * & * \\
                \color{red}* & * & * \\
                \color{red}* & * & * \\
            \end{bmatrix}
            \rightsquigarrow
            \begin{bmatrix}
                * & * & * \\
                \color{red}* & * & * \\
                \color{red}* & * & * \\
                0 & * & * \\
            \end{bmatrix}
            \rightsquigarrow
            \begin{bmatrix}
                \color{red}* & * & * \\
                \color{red}* & * & * \\
                0 & * & * \\
                0 & * & * \\
            \end{bmatrix}
            \rightsquigarrow
            \begin{bmatrix}
                * & * & * \\
                0 & * & * \\
                0 & 0 & * \\
                0 & 0 & 0 \\
            \end{bmatrix}
            \]

            \begin{algorithm}[H]
                \caption{}\label{a2.13}
                \textbf{Input:} $A\in\R^{m\times n},m\geq n$\\
                \textbf{Output:} $R$ von der $QR$-Zerlegung ($A$ wird zerstört ``in place'')
                \begin{algorithmic}
                \For{$j=1,\dots, n$}
                    \For{$i=m,m-1,\dots, j+1$}
                        \State Berechne $c,s$ wie in (\ref{e2.3})
                        \State $A[i-1:i,j:n]=\begin{bmatrix}
                            c & s \\
                            -s & c 
                        \end{bmatrix}^t A[i-1:i,j:n]$
                    \EndFor
                \EndFor
                \end{algorithmic}
            \end{algorithm}

            \underline{\textbf{$m\approx n$:}}
        
            $c,s$: In jedem Eintrag einmal Wurzeln ziehen: $\implies \frac{n^2}{2}$ Quadratwurzeln und $\frac{4n^3}{3}$ Multiplikationen

            \underline{\textbf{$m\gg n$:}} $m\cdot n$ Quadratwurzeln und $2m\cdot n^2$ Multiplikationen

            \begin{remark}\label{r2.14}
                Der Algorithmus \ref{a2.13} berechnet nur $R$ von der $QR$-Zerlegung. Zur Berechnung von $Q$ müssten 
                zusätliche Operationen investiert werden um die Givens-Rotation auf $I$ anzuwenden.

                Für das lineare Ausgleichsproblem benötigen wir $Q^tb$, weshalb wir den Algorithmus auf $\left[\begin{array}{c|c}
                    A & b
                \end{array}\right]$ anwenden können (da $R=Q^t A$).
            \end{remark}

            \begin{remark}\label{b2.15}
                Für $m=n$ ist die $QR$-Zerlegung eine (teure) ALternative zur $LR$-Zerlegung.
            \end{remark}

        \section{$QR$-Zerlegung mittels Householder-Transformationen}

            \begin{definition}\label{d2.16}
                Für $v\in\R^n,v\neq 0$, heißt \[Q=I-2\frac{\overbrace{vv^t}^{\in\R^{n\times n}}}{\underbrace{v^tv}_{\in\R}}\]
                Householder-Transformation / Reflexion / Spiegelung.
            \end{definition}

            \begin{tcolorbox}[enhanced,breakable,
                title=Wichtig!]
                Nicht $vv^t$ berechnen, das ist sehr uneffizient!
            \end{tcolorbox}

            Für $a,v\in\R^n,v\neq 0$ ist $Qa=\left(I-2\frac{vv^t}{v^tv}\right)a =a-2\frac{\langle v,a \rangle_2}{\langle v,v \rangle_2}v$

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild009}
                \caption{Householder-Transformationssetting}
            \end{figure}

            $Qa$ ist $a$ an $l$ gespiegelt.

            \begin{lemma}\label{l2.17}
                Für eine Householder-Transformation $Q\in\R^{n\times n}$ gilt:
                \begin{enumerate}
                    \item $Q$ ist symmetrisch
                    \item $Q$ ist orthogonal
                    \item $Q$ ist involutionisch (eine Involution), d.h. $Q^2=I$
                \end{enumerate}
            \end{lemma}
            \begin{proof} % TODO
                Nachrechnen.
            \end{proof}

            \underline{\textbf{Frage:}} Gegeben $a\in\R^n$, wie müssen wir $v$ wählen, so dass $Qa=\alpha e_1$ für $\alpha\in\R$?

            % TODO? Kommentar Skalarprodukt etc. schnell auf PCs

            \underline{\textbf{Beobachte:}}

            \begin{enumerate}
                \item $\left\vert \alpha \right\vert=\left\Vert \alpha e_1 \right\Vert_2 = \left\Vert Qa \right\Vert_2 = \left\Vert a \right\Vert_2$
                \item $a\underbrace{-2\frac{\langle v,a, \rangle}{\langle v,v \rangle}}_{\in\R}v=Qa$
            
                $\implies v\in\text{span}(\alpha e_1-a) \implies \alpha =\pm\left\Vert a \right\Vert_2$ % ??

                Vermeide Auslöschung $\implies \alpha=-\text{sign}(a_1)\cdot\left\Vert a \right\Vert_2$
            \end{enumerate}

            \underline{\textbf{Effiziente Berechnung:}} Beobachte:
            \begin{align*}
                \left\Vert v \right\Vert_2^2 = \langle v,v \rangle_2 &= \langle  a-\alpha e_1, a-\alpha e_1  \rangle_2&\\
                &=\left\Vert a \right\Vert_2^2-2\alpha a_1 +\alpha^2&\\
                &=-2\alpha(a_1-\alpha)&\\
                \implies Qa &= a-2\frac{\langle v,a \rangle_2}{\left\Vert v \right\Vert_2^2}=a+\frac{\langle v,a \rangle_2}{\alpha(a_1-\alpha)}v&
            \end{align*}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 03 am 18.10.2022\xrfill[0.7ex]{1pt}
            
            \begin{lemma}\label{l2.18}
                Sie $\alpha\in\R^n,a\neq 0,a\notin\text{span}\{e_1\}$. 
                Sei \begin{equation}\label{g2.4}
                    v=a-\alpha e_1,\alpha =-\text{sign}(a_1)\cdot \left\Vert a \right\Vert_2
                \end{equation} % oder alpha e_1-a
                Dann ist
                \begin{equation}\label{g2.5}
                    \left(I-2\frac{vv^t}{v^tv}\right)a=a+\frac{v^ta}{\alpha(a_1-\alpha)}v=\alpha e_1.
                \end{equation}
            \end{lemma}

            \begin{proof}
                Siehe oben.
            \end{proof}

            \begin{algorithm}[H]
                \caption{}\label{a2.19}
                \textbf{Input:} $A\in\R^{m\times n},m\geq n$ ``Mehr Zeilen als Spalten''\\
                \textbf{Output:} $A\in\R^{m\times n}$, obere rechte Dreiecksmatrix $R$, Rest Householder-Transformationen
                \begin{algorithmic}
                \For{$j=1,\dots, n$} \Comment Iterieren über die Spalten
                    \State Berechne $v,\alpha$ wie in (\ref{g2.4}) ,mit $a=A[j:m,j]\in\R^{m-j+1}$
                    \State $v=\frac{1}{v_1}v$ \Comment Erster Eintrag wird nicht gespeichert, daher normalisieren wir
                    \State Berechne $A[j:m,j:n]=\left(I-2\frac{vv^t}{v^tv}\right)A[j:m,j:n]$ wie in (\ref{g2.5})
                    \If{$j<m$}
                        \State A[j+1:m,j]=v[2:m-j+1]\Comment Index startet von 1 
                    \EndIf
                \EndFor
                \end{algorithmic}
            \end{algorithm}

            \begin{remark}\label{b2.20}
                Die Skalierung $v=\frac{1}{v_1}v$ stellt sicher, dass die der erste Eintrag von $v$ nicht gespeichert werden muss.
            \end{remark}

            \underline{\textbf{Aufwand:}} $m\sim n \rightsquigarrow \frac{2}{3}n^3$ Multiplikationen

            $m\gg n \rightsquigarrow 2n^2m$ Multiplikationen

            Schneller als Givensrotationen, stabiler als Normalengleichungen

        \section{Pseudoinverse}

            \underline{\textbf{Ausgangspunkt:}} Wir wollen ein stabiles numerisches Verfahren, dass 
            \[
                Ax=b,A\in\R^{m\times n},m\geq n,\rang(A)=n,b\in\R^n    
            \]
            ``lösen'' kann, d.h. es gilt 
            \[
                \left\Vert Ax-b \right\Vert_2=\min_{y\in\R^n}\left\Vert Ay-b \right\Vert_2    
            \]

            Mathematisch können wir die Abbildung $b\mapsto x$, wegen der Normalengleichung (\ref{g2.2}), schreiben als 
            \[
                x=\underbrace{(A^tA)^{-1}A^t}_{\coloneqq A^\dagger}b=A^\dagger b    
            \]
            $A^\dagger\in\R^{n\times m}$. Wegen $A^\dagger A=I$ heißt $A^\dagger$ auch \textbf{Pseudoinverse}.

            \underline{\textbf{Frage:}} Können wir den Begriff der Inversen noch weiter verallgemeinern? Auf beliebige Matrizen?

            Satz \ref{s1.9}: $A\in\R^{m\times n}, U=\text{Bild}(A)$

            \begin{align*}
                &\implies \left\Vert Ax-b \right\Vert_2=\min_{y\in\R^n} \left\Vert Ay-b \right\Vert_2 \stackrel{\text{Satz \ref{s1.9}}}{\iff} Ax-b\in\text{Bild}(A)^\perp\\
                &\iff Ax- Pb -\underbrace{(b-Pb)}_{\in U^\perp:\text{ Satz \ref{s1.9}}}\in\text{Bild}(A)^\perp, Pb \text{ ist die orthogonale Projektion von } b \text{ auf } U\\
                &\iff \underbrace{\underbrace{Ax}_{\in U}-\underbrace{Pb}_{\in U}}_{\in U}\in\text{ Bild}(A)^\perp\\
                & \iff Ax=Pb
            \end{align*}

            Falls $\rang(A)<n$ (z.B., falls $m<n$) ist $Ax=Pb$ nicht eindeutig lösbar (aber es existiert immer eine Lösung).

            Für $\tilde{x}\in\R^n$ mit $A\tilde{x}=Pb,x'\in\text{ker}(A)$ ist $A(\tilde{x}+x')=Pb$.

            \begin{align*}
                L(b)&=\left\{x\in\R^n:\left\Vert Ax-b \right\Vert_2 = \min_{y\in\R^n} \left\Vert Ay-b \right\Vert_2\right\}\\
                &=\{x\in\R^n:Ax=Pb\}\\
                &=\tilde{x}+\ker(A)
            \end{align*}

            Sind gewisse Lösungen sinnvoller als andere?

            Wähle: $x\in\tilde{x}+\ker(A)$ mit minimaler Norm als ``eindeutige'' Lösung von $Ax=b$.

            \begin{align*}
                \stackrel{\text{Bem. \ref{b1.13}}}{\implies} \left\Vert x-0 \right\Vert_2 = \min_{y\in \tilde{x}+ \ker(A)} \left\Vert y-0 \right\Vert_2 &\iff x\in (\tilde{x}+\ker(A))^\perp \\
                &\iff x\in \ker(A)^\perp 
            \end{align*}

            \begin{tcolorbox}[enhanced,breakable,
                title=Anmerkung]
                Hier ist nicht ganz klar, was mit $(\tilde{x}+\ker(A))^\perp$ gemeint ist, da dies z.B. für $\ker(A)=\text{span}\{(0,1)^t\}$ und $\tilde{x}=(1,0)^t$ nur $\{0\}$ ist, was natürlich nicht der Intuition entspricht!
                
                Statt der ursprünglichen Definition müssen wir hier wieder zurück schieben ($-\tilde{x}$ rechnen), was kein Problem ist, da wir o.B.d.A $\tilde{x}\perp \ker(A)$ vorraussetzen dürfen, bevor wir das Skalarprodukt berechnen!

                Zum Beispiel ist also $v=(1,0)^t$ im obigen Beipspiel doch im orthogonalen Komplement, da $\langle v, \tilde{x}+u-\tilde{x}\rangle_2=0$ für $u\in \ker(A)$
            \end{tcolorbox}
            
            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild010}
                \caption{Setting}
            \end{figure}

            \begin{remark}\label{b2.21}
                Diese Wahl von $x$ für $b\mapsto x$ ist linear: Für $b_1,b_2\in\R^m$ ist:
                \[
                \begin{rcases*}
                Ax_1 = b_1 & $x_1 \in\text{ker}(A)^\perp$ \\
                Ax_2 = b_2 & $x_2 \in\text{ker}(A)^\perp$
                \end{rcases*} \implies P(x_1+x_2)=P(x_1)+P(x_2)=Ax_1+Ax_2=A(x_1+x_2),x_1+x_2\in \ker(A)^\perp
                \]
            \end{remark}

            \begin{definition}\label{d2.22}
                Sei $A\in\R^{m\times n}$. Die Abbildungsmatrix $A^\dagger\in\R^{n\times m}$ von $b\mapsto x$ heißt 
                \textbf{Pseudoinverse} oder \textbf{Moore-Pensore-Inverse} von $A$. D.h. gegeben $b\in\R^n$, dann ist $x=A^\dagger b$ die eindeutige Lösung von 
                \[
                    \min_{y\in\ker(A)^\perp} \left\Vert Ay-b \right\Vert_2 = \left\Vert Ax-b \right\Vert_2.    
                \]
            \end{definition}
            
            \begin{theorem}\label{s2.23}
                $A\in\R^{m\times n}$. Dann ist $A^\dagger\in\R^{m\times n}$ eindeutig über die Moore-Penrose-Axiome definiert:
                \begin{enumerate}
                    \item $(A^\dagger A)^t=AA^\dagger$
                    \item $(AA^\dagger )^t=A^\dagger A$
                    \item $A^\dagger AA^\dagger = A^\dagger$
                    \item $AA^\dagger A=A$
                \end{enumerate}
            \end{theorem}

            \begin{proof}
                Siehe Literatur oder später
            \end{proof}

            \underline{\textbf{Frage:}} Wie berechnen wir $x=A^\dagger b$?

            Sei $A\in\R^{m\times n},\rang(A)=p\leq \min(m,n)$. Bringe $A$ mittels orthogonaler Transformationen (z.B. Householder) auf 
            obere Dreiecksgestalt, d.h.:

            \begin{equation}\label{g2.6}
                Q^tA=\begin{bmatrix}
                    &  &  & & & \\
                    & R & & S & & \\
                    * & & & & &\\
                    & 0 & &  0 & & \\ 
                \end{bmatrix}
            \end{equation}
            wobei  $S\in \R^{p\times (n-p)}$.
            Setze Analog $x=\begin{bmatrix}
                x_1\in\R^p\\
                x_2\in R^{n-p}
            \end{bmatrix},Q^t b=\begin{bmatrix}
                b_1\in\R^p\\
                b_2\in \R^{m-p}
            \end{bmatrix}$

            \begin{lemma}\label{l2.24}
                Mit obigen Bezeichungen ist $x=A^\dagger b$ genau dann, wenn \[x_1=R^{-1}b_1-R^{-1}Sx_2.\]
            \end{lemma}
            \begin{proof}
                \begin{align*}
                    \left\Vert Ax-b \right\Vert_2^2&=\left\Vert Q^t(Ax-b) \right\Vert_2^2\\
                    &=\left\Vert \begin{pmatrix}
                        Rx_1+Sx_2-b\\
                        -b_2
                    \end{pmatrix} \right\Vert_2^2\\
                    &= \left\Vert Rx_1+Sx_2 -b_1\right\Vert_2^2+\left\Vert b_2 \right\Vert_2^2
                \end{align*}
                ist minimal, falls $Rx_1=b_1-Sx_2$.
            \end{proof}

            Wir sehen $p=\rang(A)=n\implies$ wie vorher, lineares Ausgleichsproblem!

            Sonst: $x_2=$?

            \begin{lemma}\label{l2.25}
                Sei $p<n,V=R^-1S\in\R^{n\times (n-p)}$ und $u=R^{-1}b_1\in\R^p$. Dann ist
                \begin{align*}
                    x&=A^\dagger b\\
                    \iff &(I+V^tV)x_2=V^tu\\
                    &x_1=u-Vx_2
                \end{align*}
            \end{lemma}

            \begin{proof}
                \begin{align*}
                    \left\Vert x \right\Vert_2^2 &= \left\Vert x_1 \right\Vert_2^2 + \left\Vert x_2 \right\Vert_2^2\\
                    &\stackrel{\text{Lemma \ref{l2.24}}}{=}\left\Vert u-Vx_2 \right\Vert_2^2+ \left\Vert x_2 \right\Vert_2^2\\
                    &=\left\Vert u \right\Vert_2^2-2 \langle u,Vx_2 \rangle_2+\langle Vx_2,Vx_2 \rangle_2+\langle x_2,x_2 \rangle_2\\
                    &=\left\Vert u \right\Vert_2^2+\langle x_2,(I+V^tV)x_2-2V^tu \rangle_2 = \varphi(x_2)
                \end{align*}
                Minimiere $\varphi(x_2)$:
                \begin{align*}
                    \varphi'(x_2)=-2V^tu+2(I+V^tV)x_2\\
                    \varphi'(x_2)=2(I+V^tV) \implies \text{spd}
                \end{align*}
                $\varphi$ minimal $\iff \varphi'(x_2)=0\implies $ Behauptung.
            \end{proof}

            \begin{algorithm}[H]
                \caption{}\label{a2.26}
                \textbf{Input:} $A\in\R^{m\times n},b\in\R^m$\\
                \textbf{Output:} $x=A^\dagger b$
                \begin{algorithmic}
                \State Berechne $QR$-Zerlegung (\ref{g2.6}) von $A$
                \State $\begin{bmatrix}b_1\\b_2\end{bmatrix}=Q^t b$
                \State $V=R^{-1}S$ mittels Rückwertssubstitution 
                \State $u=R^{-1}b_1$ mittels Rückwertssubstitution
                \State Löse $(I+V^tV)x_2=V^tu$ mittels Cholesky-Zerlegung 
                \State $x_1=u-Vx_2$
                \State $x=\begin{bmatrix}x_1\\x_2\end{bmatrix}$
                \end{algorithmic}
            \end{algorithm}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 04 am 20.10.2022\xrfill[0.7ex]{1pt}
            

    \chapter{Iterative Verfahren für große, dünn besetzte, Gleichungsysteme}

        \section{Motivation}

            Sei $\Omega\subset \R^d,d\in\N$. Betrachte die stationäre Wärmeleitungsgleichung, eine partielle Differenzialgleichung
            \begin{equation}\label{g3.1}
                \begin{cases}
                    -\Delta u(x)=f(x) & \in\Omega\\
                    u(x)=0 & x\in \delta\Omega
                \end{cases}
            \end{equation}

            mit Wärmequelle $f\in C(\Omega)$ und dem Laplace-Operator:

            \begin{equation}\label{g3.2}
                \Delta u =\sum_{i=1}^n \frac{\partial^2 u(x)}{\partial x_i^2}. 
            \end{equation}

            Die Lösung $u\in C^2(\Omega)$, falls existent, beschreit die Temperaturverteilung im Raum $\Omega$.

            Diese Gleichung ist i.A. nicht von Hand lösbar! 

            \underline{\textbf{Idee:}} Berechne approximative Lösung im Computer.

            \underline{\textbf{Ansatz:}} Für $g\in C^2(\R)$ ist
            \begin{align*}
                g''(x)=\lim_{h\searrow 0} \frac{g'(x+h)-g(x)}{h} &\approx \frac{g'(x+h)-g(x)}{h} \\
                & \approx \frac{\frac{g(x+h)-g(x)}{h}-\frac{g(x)-g(x-h)}{h}}{h}\\
                &\approx \frac{g(x+h)-2g(x)+g(x-h)}{h^2}
            \end{align*}

            $\rightsquigarrow$ Ersetze $\frac{\partial^2 u}{\partial x_i^2}$ in (\ref{g3.2})

            $\rightsquigarrow$ Überziehe $\Omega$ mit einem regelmäßigen Gitter mit Maschenweite $h=\frac{1}{n},n\in\N$.

            \begin{figure}[H]
                \centering
                \includegraphics[width=0.25\textwidth]{Bild011}
                \caption{Gitter}
            \end{figure}

            Bezeichne die Gitterpunkte mit $x_{ij}$ und $u_{ij}=u(x_{ij})$.

            \[\stackrel{d=2}{\implies}\frac{1}{h^2}\left(4u_{ij}-u_{i+1j}-u_{i-1j}-u_{ij+1}-u_{ij-1}\right)=f_{ij}:i,j\in1,\dots,n-1\] 

            \[u_{ij}=0,i\in\{0,n\}\text{ oder }j\in\{0,n\}\]

            Wir erhalten ein lineares Gleichungssystem mit $N=(n-1)^2$ Unbekannten und $O(1)$ Einträgen pro Zeile. 
            $\implies A\in\R^{n\times n}$ hat $O(N)$ Einträge. Wir haben das Lösen eines (linearen) partiellen Differentialgleichung durch das Lösen eines linearen 
            Gleichungssystems ersetzt.

            \begin{example}\label{b3.1}
                $\Omega=(0,1)^2,n=4\implies h=\frac{1}{4}$. Erhalte:

                \[
                    \left[
                        \begin{array}{ccc|ccc|ccc}
                            4 & -1 & &-1 & & & & & \\
                            -1 & 4 & -1 & & -1 & & & & \\
                            & -1 & 4 & & & -1 & & & \\ \cline{1-9}
                            -1 & & &  4 & -1 & & -1 & & \\
                            & -1 & &  -1 & 4 & -1 & &-1 &\\
                            & & -1 & & -1 & 4 & & &  -1\\\cline{1-9}
                            & & & -1 & & & 4 & -1 & \\
                            & & & & -1 & & -1 & 4 & -1 \\
                            & & & & & -1 & & -1 & 4  
                        \end{array}  
                    \right]
                    \begin{bmatrix}
                        u_{11}\\
                        u_{12}\\
                        u_{13}\\
                        u_{21}\\
                        u_{22}\\
                        u_{23}\\
                        u_{31}\\
                        u_{32}\\
                        u_{33}
                    \end{bmatrix} = 
                    \begin{bmatrix}
                        f_{11}\\
                        f_{12}\\
                        f_{13}\\
                        f_{21}\\
                        f_{22}\\
                        f_{23}\\
                        f_{31}\\
                        f_{32}\\
                        f_{33}
                    \end{bmatrix}
                \]
            \end{example}


            \underline{\textbf{Aber:}} Um die Lösung von (\ref{g3.1}) gut zu approximierenm ist oft $N\gg 1$ erforderlich.
            Für kleine bis mittlere $N$, d.h. in 2022 je nach Modell $\sim$ 10 Millionen, sind graphenbasierte Löser eine Option.

            Was tun für große $N$?

            \underline{\textbf{Beobachtung:}} Matrix-Vektor-Multiplikation sind für dünn besetze Matrizen in $O(N)$ berechenbar.

            \underline{\textbf{Frage:}} Wie bauen wir gute Löser für LGS (lineare Gleichungssysteme) nur unter Anwendung von 
            Matrix-Vektor-Multiplikationen?

            \underline{\textbf{Idee:}} Benutze  Orthogonalität um eine Bestapproximationseigenschaft zu erhalten.

        \section{Grundidee von Projektionsmethoden}

            Sei $A\in\R^{n\times n},b\in\R^n$ und $K,L$ Unterräume vom $\R^n$.

            \underline{\textbf{Idee:}} Finde eine approximative Lösung $\tilde{x}$ zu $Ax=b$ mit 
            \[\tilde{x}\in K \text{ und } b-A\tilde{x}\perp_2 L\]
            Kanonische Wahl: $L=AK$.

            Falls wir eine Startnährung $x_0$ zu $x$ kennen, können wir $\tilde{x}$ in $x_0+K$ suchen:

            Finde $\tilde{x} \in x_0+K \text{ mit } b-A\tilde{x}\perp_2 L$

            \underline{\textbf{Beobachtung:}} $\tilde{x}\in x_0+K\implies \exists d\in K: \tilde{x}=x_0+d$

            \[\implies \underbrace{b-A(x_0}_{r}+d)\perp_2 L\]

            \[\iff r_0-Ad \perp_2 L\]

            Eine approximative Lösung $\tilde{x}=x_0+d$ muss also erfüllen:

            \begin{equation}\label{g3.3}
                \begin{cases}
                    \tilde{x}=x_0+d & \\
                    \langle r_0-Ad,w \rangle_2 = 0  & \forall w\in L
                \end{cases}
            \end{equation}

            \underline{\textbf{Idee:}} Wähle $x_0,K,L$, berechne $d\in K$ durch Lösen eines Unterproblems. Setze 
            $x_1=x_0+d$, wähle neue Unterräume, beginne von vorne.

            Wie implementieren wir diese Idee im Computer?

            Sei $K=\text{span}\{v_1, \dots, v_n\},L=\{w_1,\dots,w_n\}$

            $V=[v_1\vert \dots\vert v_n]$ und $W=[w_1\vert \dots\vert w_n]$

            (\ref{g3.3}) ist äquivalent zu 

            \begin{equation}\label{g3.4}
                \begin{cases}
                    \tilde{x} = x_0+Vy &  y\in\R^m\\
                    W_i^t A Vy = W_i^t r_0 & i=1,\dots n \iff \underbrace{W^tA}_{m\times m}Vy = W^tr_0
                \end{cases}
            \end{equation}

            $\implies \tilde{x}=x_0+V(W^tAV)^{-1}W^t r_0$ % TODO: Überprüfen

            \begin{algorithm}[H]
                \caption{Prototyp einer interativen Projektionsmethode}\label{a3.2}
                \textbf{Input:} $A\in\R^{n\times n},b\in\R^n$, Fehlertoleranz $\alpha$\\
                \textbf{Output:} Nährung $x_{i+1}\approx x$
                \begin{algorithmic}
                \State i=0
                \While {Fehlertoleranz noch nicht erreicht}
                    \State Wähle $K_i,L_i$
                    \State Wähle Basen $V,W$ von $K_i,L_i$
                    \State $r_1=Ax_i$
                    \State $y=(W^tAV)^{-1}W^tr_i$
                    \State $x_{i+1}=x_i+Vy_i$
                    \State $i=i+1$
                \EndWhile
                \end{algorithmic}
            \end{algorithm}

            \underline{\textbf{Aber:}} $W^tAV$ ist nicht notwendigerweise invertierbar:

            \begin{example}\label{b3.3}
                \[
                    A=\left[
                        \begin{array}{c|c}
                            0 & I\\\cline{1-2}
                            I & I
                        \end{array}
                    \right]\in\R^{2m\times 2m}
                \]
                \[
                    K=L=\text{span}\{e_1,\dots,e_m\}\implies V=W=\
                    \left[
                        \begin{array}{c}
                            I_m\\\cline{1-1}
                            0
                        \end{array}
                    \right]\in K^{2m\times m}
                \]
                \[\implies W^t A V =0 \text{ ist nicht invertierbar.}\]
            \end{example}

            \begin{lemma}\label{3.4}
                Sei einer der folgenden Bedingungen erfüllt:
                \begin{enumerate}
                    \item $A$ ist spd, $K=L$
                    \item $A$ invertierbar, $L=AK$
                \end{enumerate}
                Dann ist $W^tAV$ für alle Basen von $K,L$ invertierbar.
            \end{lemma}

            \begin{proof}
                \textbf{1.:} $L=K\implies W=V\delta$ mit $\delta \in R^{m\times m}$ invertierbar.

                $\implies B=W^tAV=\delta^tV^tAV$ % TODO: überprüfen

                \[0<\underbrace{y^tAy}_{=\underbrace{x^tV^t A V x}_{\text{spd, invertierbar}}},y=Vx\]

                \textbf{2.:} $L=AK \implies W=AV\delta,\delta\in\R^{m\times m}$ invertierbar

                \[\implies B=W^tAV=\delta^t\underbrace{V^tA^tAV}_{\text{spd}} \implies\text{ invertierbar } \implies \text{ Beh.}\]
            \end{proof}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 05 am 25.10.2022\xrfill[0.7ex]{1pt}

        \section{Verfahren des steilsten Abstiegs}

            \underline{\textbf{Idee:}} Wähle $K=L=\text{span}\{r_i\}=\text{span}\{b-Ax_i\}$

            \[\implies x_{i+1}=x_i+\underbrace{\alpha_i r_i}_{d_i\in K}\]

            \[\implies \alpha = \frac{r_i^tr_i}{r_i^tAr_i}\]
            
            \begin{algorithm}[H]
                \caption{Verfahren des steilsten Abstiegs}\label{a3.5}
                \textbf{Input:} $A,b$, Startvektor $x_0$, Fehlertoleranz\\
                \textbf{Output:} Nährung $x_{i+1}\approx x$
                \begin{algorithmic}
                \While {Fehlertoleranz noch nicht erreicht} \Comment{Praxis $\epsilon=10^{-8}$, $\left\Vert r_i \right\Vert_2<\epsilon$}
                    \State $r_i=b-Ax_i$
                    \State $\alpha_i= \frac{r_i^tr_i}{r_i^tAr_i}$
                    \State $x_{i+1}=x_i+\alpha_i r_i$
                \EndWhile
                \end{algorithmic}
            \end{algorithm}

            \begin{remark}\label{b3.6}
                Wegen (\ref{g3.3}) gilt:
                \begin{align*}
                    0&=\langle r_i-Ad_i,r_i \rangle_2\\
                    &=\langle b-Ax_i-Ad_i,r_i \rangle_2\\
                    &=\langle b-Ax_{i+1},r_i \rangle_2\\
                    &= \langle Ax-Ax_{i+1}, r_i\rangle_2\\
                    &= \langle x-x_{i+1}, r_i\rangle_A = (\star)
                \end{align*}
            \end{remark}

            Mit 
            \[
                \langle \cdot,\cdot \rangle_A=\langle A\cdot,\cdot \rangle_2
            \]

            Aus $(\star)$ folgt 

            \begin{align*}
                0=(\star) &\iff x-x_{i+1} \perp_A r_i \\
                &\iff x-x_{i+1} \perp_A x_i+\text{span}\{r_i\}\\
                &\stackrel{\text{Satz \ref*{s1.9}}}{\iff} \left\Vert x-x_{i+1} \right\Vert_A=\min_{y\in x_i+\text{span}\{r_i\}} \left\Vert x-y \right\Vert_A\\
                &\iff \frac{1}{2} \left\Vert x-x_{i+1}  \right\Vert_A^2 = \min_{\alpha\in \R}=\frac{1}{2} \left\Vert  x-x_i-\alpha r_i \right\Vert_A^2
            \end{align*}

            Betrachte $f(x_i)=\frac{1}{2}\left\Vert x-x_i \right\Vert_A^2 = \frac{1}{2} \langle A(x-x_i),x-x_i \rangle_2$

            \[
                f'(x_i)=-\underbrace{Ax}_{=b}+Ax_i=-r_i    
            \]

            D.h. am Punkt $x_i$ gehen wir in Richtung des steilsten Abstiegs,

            \begin{theorem}\label{s3.7}
                Sei $A\in\R^{n\times n}$ spd. Dann gilt für die Iterierung des Verfahrens des steilsten Abstiegsm dass:
                \begin{align*}
                    \left\Vert x-x_{i+1} \right\Vert_A &\leq \frac{\lambda_{\max}(A)-\lambda_{\min}(A)}{\lambda_{\max}(A)+\lambda_{\min}(A)} \left\Vert x-x_i \right\Vert_A\\
                    &\stackrel{\frac{1}{\lambda_{\min}}}{=}\frac{\cond_2(A)-1}{\cond_2(A)+1}\left\Vert x-x_i \right\Vert_A
                \end{align*}
                wobei $\lambda_{\max},\lambda_{\min}$ die größten, kleinsten Eigenwerte sind.
            \end{theorem}

            \begin{proof}
                Übung %TODO
            \end{proof}

            \begin{remark}\label{b3.8}
                Im Prinzip lassen sich mit Hilfe der Normalengleichung auch allgemeinere (invertierbare) Matrizen behandeln. 
                Hierbei wird die Kondition verschlechtetertm d.h. die Konvergenz verschlechtert sich.
            \end{remark}

        \section{Krylovräume}

            \underline{\textbf{Beobachte:}} Im Verfahren des steilsten Abstiegs gilt:
            \[
                x_i=x_0+\alpha_0r_0+\dots+\alpha_{i-1}r_{i-1} = x_0+\alpha_0r_0+\dots+(\alpha_{i-2}I+\alpha_{i-1}(I-\alpha_{i-2}))r_{i-2}
            \]
            \[=x_0 q_{i-1}(A)r_0, q_{i-1}\in\Pi_{i-1}\]

            \underline{\textbf{Idee:}} Finde eine bessere Approximation von $x_i$ in $x_0+K_{i-1}(A,r_0)$.

            \begin{definition}\label{d3.9}
                Sei $A\in\R^{n\times n},v\in\R^n,n\geq 1$. Der Raum 
                \[K_m=\text{span}(v,Av,A^2v,\dots,A^{m-1}v)\]
                heißt \textbf{Kylovraum} von $A$ zu $v$.                
            \end{definition}

            Es gilt $K_m(A,v)\subseteq K_{m+1}(A,v)$

            \begin{lemma}\label{l3.10}
                Sei $\R^{n\times n},v\in\R^n$. Dann gilt:
                \begin{enumerate}
                    \item $\dim K_m(A,v)\leq \min\{m,n\}$
                    \item $\dim(K_m(A,v))=\dim(K_{m+1}(A,v))=m\implies \dim(K_{m+i}(A,v))=m,i=0,1,\dots$
                    \item Für $m$ wie im 2. gilt \[\{Ax:x\in K_m(A,v)\}\subseteq K_m(A,v)\]
                    D.h. $K_m(A,v)$ ist invariant unter $A$.
                \end{enumerate}
            \end{lemma}

            \begin{proof}
                Übung   
            \end{proof}

            \begin{remark}\label{b3.11}
                Betrachte $Ax=b$ mit Startnährung $x_0$, Residuum $r_0=b-Ax_0$. Für $i=0,\dots$. Wähle
                \[x_{i+1}\in x_0+K_{i-1}(A,r_0)\]
                \[\implies x_{i+1}x_0=q_i(A)r_0\]
                \[\implies r_{i+1}=b-Ax_{i+1}=\underbrace{b-Ax_0}_{r_0}-Aq_i(A)r_0\]
                \[=q_{i+1}(A)r_0\in K_{i+2}(A,r_0)\]
            \end{remark}

            \underline{\textbf{Das bedeutet:}} Sind $K,L$ geeignete Krylovräume in einer Projektionsmethode, dann können 
            wwir immer garantieren, dass das Ergebnis in einem Krylovraum ist.

            \underline{\textbf{Aber:}} Die Vektoren $v,Av,\dots,A^nv$ sind numerisch keine guten Basen der Krylovräume, da sie zunehmend in eine 
            änhliche Richtung zeigen.

        \section{Arnoldi-Verfahren}

            \underline{\textbf{Gesucht:}} Numerisch gutartige Basis von $K_m(A,v)$, welche einfach zu einer gutartigen Basis von $K_{m+1}(A,v)$ erweitert werden kann.

            \underline{\textbf{Idee:}} Arrangiere die Vektoren $v,Av,\dots$ in einer ``wachsenden'' Matrix
            \[[v\vert Av\vert A^2v\vert \dots]\]
            und wende auf jede Spalte ein Orthogonalisierungsverfahren (Gram-Schmidt, Householder,Givens) an.

            \begin{algorithm}[H]
                \caption{Arnoldi-Verfahren (Gram-Schmidt Variante)}\label{a3.12}
                \textbf{Input:} $A\in\R^{n\times n},b\in\R^n,m\in\N$\\
                \textbf{Output:} $V_m=[v_1\vert \dots\vert v_m]$ ONB von $K_m(A,v),v_{m+1}\in\R^n,H_{m+1,m}\in\R^{(m+1)\times m}$
                \begin{algorithmic}
                \State $v_1=\frac{v}{\left\Vert v \right\Vert_2}$
                \For{$j=1,m$}
                    \State $z=Av_j$ \Comment $\neq A^{j-1}r_0$
                    \State $h_{ij}=\langle z,v_i \rangle_2,i=1,\dots j$
                    \State $w_j=z-\sum_{i=1}^j h_{ij}v_i$
                    \State $h_{j+1,j}=\left\Vert w_j \right\Vert_2$
                    \If{$h_{j+1,j}=0$} 
                        \State stop \Comment Krylovraum stagniert
                    \EndIf
                \State $v_{j+1}=\frac{w_j}{h_{j+1,j}}$
                \EndFor
                \end{algorithmic}
            \end{algorithm}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 06 am 27.10.2022\xrfill[0.7ex]{1pt}

            \begin{lemma}\label{l3.13}
                Falls das Arnoldi-Verfahren nicht vorzeitig abbricht, ist $v_1,\dots,v_m$ eine 
                ONB von $K_m(A,r_0)$.
            \end{lemma}
            \begin{proof}
                Orthogonalität: Ok

                Orthonormal: Ok

                Basis von $K_m(A,r_0)$: $j=1$ ok

                $j\implies j+1$

                $h_{j+1,j}v_j = w_j$ (folgt aus der letzten Zeile von Algorithmus \ref{a3.12}).
                \[
                    w_j=\underbrace{Av_j}_{\in K_j(A,r_0)\implies v_j=q_{j-1}(A)v}-\underbrace{\sum_{i=1}^2h_{i,j}v_i}_{\tilde{q}_{j-1}(A)v, \tilde{q}_{j-1}\in \Pi_{j-1}}  = (\star)
                \]
                \[
                    (\star) = Aq_{j-1}(A)v_j - \tilde{q}_{j-1}(A)v_j=\underbrace{q_{j}(A)v}_{\in K_{j+1}(A,r_0)}    
                \]
            \end{proof}

            \underline{\textbf{Bemerke:}} Diese Matrix $H_{h+1,j}\in\R^{(j+1)\times j}$ hat eine bestimmte Sturktur,
            die Hessenberg-Struktur genannt wird.

            \begin{tcolorbox}[enhanced,breakable,
                title=Vorteile dieser Struktur]
                Zum Beipspiel kann man eine $QR$-Zerlegung finden, in dem man Pro Spalte eome Givensrotation anwendet.
            \end{tcolorbox}

            \begin{lemma}\label{l3.14}
                Seien $V_m\in\R^{n\times n}, H_{m+1,m}\in\R^{(m+1)\times m}$, wie im Arnoldi-Verfahren erzeugt. Sei $H_{m,m}\in\R^{m\times m}$ wie $H_{m+1,m}$, aber 
                ohne die letzte Zeile. Dann gilt:
                \begin{equation}\label{g3.5}
                    \underbrace{A}_{\in \R^{n\times n}}\underbrace{V_m}_{\in \R^{n\times m}}=\underbrace{V_m}_{\in \R^{n\times m}}\underbrace{H_{m,m}}_{\in \R^{m\times m}}+\underbrace{w_me_m^t}_{\in \R^{m\times m}}=\underbrace{V_{m+1}}_{\in \R^{n\times m+1}}\underbrace{H_{m+1,m}}_{\in \R^{m+1\times m}}
                \end{equation}
                
                \begin{equation}\label{g3.6}
                    V_m^tAV_m=H_{m,m}
                \end{equation}

            \end{lemma}

            \begin{proof}
                Gemäß Algorithmus haben wir 
                \[
                    Av_j=z=\underbrace{w_j}_{h_{j+1,j}v_j} + \sum_{i=1}^{j}h_{ij} v_i = \sum_{i=1}^{j+1} h_{ij} v_i : j=1,\dots, m    
                \]
                Daher gilt (\ref{g3.5}) (folgt aus Matrix Schreibweise).

                Für (\ref{g3.6}):

                \[
                    V_m^t A V_m = \underbrace{V^t_m V_m}_{=I} H_{m,m}+ \underbrace{V_m^t w_m e_m^t}_{=0} = H_{m,m} 
                \]
            \end{proof}

            \begin{lemma}\label{l3.15}
                Sei $j$ der Iterationsindex, bei dem das Arnoldi-Verfahren das erste Mal abbricht. Dann gilt:

                \[
                  K_j(A,r_0)=\dots = K_m(A,r_0)  
                \]
                \[
                  AV_m=V_m H_{m,m}  
                \]
            \end{lemma}

            \begin{proof}
                Übung.
            \end{proof}

        \section{Verfahren der vollständigen Orthogonalisierung}

            \underline{\textbf{Ziel:}} Kombinieren von unserem Wissen über Projektionsmethoden mit demjenigen 
            Wissen über Krylovräume.

            \underline{\textbf{Zutaten:}}
            \begin{itemize}
                \item Finde $\tilde{x}\in x_0+K$ s.d. $bA\tilde{x}\perp L$
                \item Wähle $K,L$ als Krylovräume in jeder Iteration
                \item Wähle Basen $V,W$ von $K,L$ in jeder Iteration
            \end{itemize}

            \[
                (\ref{g3.4}) \implies \begin{cases}
                    \tilde{x}=x_0+Vy\\
                    W^t A Vy = w^tr_0
                \end{cases}    
            \]

            \underline{\textbf{Idee:}} Setze $r_0=b-Ax_0,K=L=K_m(A,r_0)$ im $m$-ter Iteration, 
            $V=W$ ONB von $K_m(A,r_0)$, berechnet mittels Arnoldi-Verfahren.   

            \underline{\textbf{Zutaten: aktualisiert}}:
            \begin{itemize}
                \item $r_0=b-Ax_0$
                \item $\beta=\left\Vert r_0 \right\Vert_2,v_1=\frac{r_0}{\beta}$
                \item \[
                  \begin{cases}
                    x_m=x_0+V_my_m\\
                    \underbrace{V_m^t AV_m}_{\stackrel{\ref{g3.6}}{=}H_{m,m}}\underbrace{y_m}_{\beta v_1} = V_m^t r_0 \iff H_{m,m}y_m=\beta e_1
                  \end{cases}  
                \]
            \end{itemize}

            \begin{algorithm}[H]
                \caption{Verfahren der vollständigen Orthogonalisierung}\label{a3.16}
                \textbf{Input:} $A\in\R^{n\times n},b\in\R^n,x_0\in\R^n,m\in\N$\\
                \textbf{Output:} $x_m\in x_0+K_m(A,r_0),b-Ax_m\perp K_m(A,r_0), x_m\approx A^{-1}b$
                \begin{algorithmic}
                \State $r_0=b-Ax_0,\beta=\left\Vert r_0 \right\Vert_2,v_1=\frac{r_0}{\beta}$
                \For{$j=1,m$}
                    \State Bestimme $V_j, H_{j,j}$ mit Arnoldi-Verfahren (Stop beim Abbruch)
                    \State Löse $\underbrace{H_{j,j}}_{\in\R^{j\times j}}y_j = \beta e_1$
                    \State $x_j=x_0+V_jy_j$
                    \State Konvergenztest
                \EndFor
                \end{algorithmic}
            \end{algorithm}

            Was ist ein geeigneter Konvergenztest?

            \begin{lemma}\label{l3.17}
                Im Algorihtmus \ref{a3.16} gilt 
                \[r_m=b-Ax_m=-h_{m+1,m}(e_m^ty_m)V_{m+1}\]
                d.h. es gilt auch 
                \[
                    \left\Vert r_m \right\Vert_2 = \vert \underbrace{h_{m+1,m}}_{>0} (e^t_m y_m)\vert = h_{m+1,m} \left\vert e_m^ty_m \right\vert    
                \]
            \end{lemma}

            \begin{proof}
                \begin{align*}
                    b-Ax_m&=b-A(x_0+V_my_m)\\
                    &= r_0-  AV_my_m = (\star)
                \end{align*}
                \begin{align*}
                    (\star)&= \underbrace{r_0}_{\beta v_1}-\underbrace{AV_m}_{\stackrel{\ref{g3.5}}{=}V_mH_{m,m}-w_me_m^t}\\
                     &= \beta v_1 - \underbrace{V_m\underbrace{H_{m,m}y_m}_{=\beta e_1}}_{\beta v_1}- \underbrace{w_m}_{=h_{m+1,m}V_{m+1}}(e_m^ty_m)\\
                     &= -h_{m+1,m}(e_m^ty_m)v_{m+1}
                \end{align*}
            \end{proof}

            \underline{\textbf{Bemerke:}}

                \begin{itemize}
                    \item Hauptkosten (Alles mit Vektoren der Länge \underline{$n$})
                        \begin{itemize}
                            \item Matrix-Vektor-Multiplikation (1, Mal, sehr teuer)
                            \item Skalarprodukte
                            \item Vektor-Updates
                        \end{itemize}
                    $\implies$ Berechnen des Residuums wie in Lemma (\ref{l3.17}) lohnt sich.
                    \item Speicherbedarf und Aufwand per Iteration werden in jeder Iteration teuer!
                \end{itemize}

            \begin{tcolorbox}[enhanced,breakable,
                title=Umgehen von großen $m$]
                Wir können Neustarten, um $m$ wieder auf $1$ zu setzen und hohe Kosten von großen $m$ zu verhindern.
            \end{tcolorbox}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 07 am 03.11.2022\xrfill[0.7ex]{1pt}

        \section{Das GMRES-Verfahren}

            \underline{\textbf{Idee:}} Lemma (\ref{l3.17}) gibt uns eine explizite Darstellung
            von $\left\Vert r_j \right\Vert_2$. Können wir die Idee dahinter benutzen, um $\left\Vert r_j \right\Vert_2$
            in jeder Iteration zu minimieren?


            \begin{align*}
                r_j&=b-A\underbrace{x_j}_{=x_0+V_jy_j}\\
                &= \underbrace{b-Ax_0}_{=r_0=\beta v_1=V_{m+1}\beta \underbrace{e_1}_{\in\R^{m+1}}}-\underbrace{AV_j}_{\stackrel{\ref{g3.5}}{=}V_{j+1}H_{j+1,j}}y_j\\
                &= V_{j+1}(\beta e_1-H_{j+1,j}y_j)
            \end{align*}

            \[\implies \left\Vert r_j \right\Vert_2=\left\Vert V_{j+1}(\beta e_1-H_{j+1,j}y_j) \right\Vert_2\]
            
            Da $ \left\Vert V_{j+1}z_{j+1} \right\Vert_2^2=z_{j+1}^tz_{j+1}=\left\Vert z_{j+1} \right\Vert_2^2$


            \begin{equation}\label{g3.7}
                \implies \left\Vert r_j \right\Vert_2=\left\Vert \underbrace{\beta e_1}_{\in\R^{j+1}} - \underbrace{H_{j+1,j}}_{\in\R^{j+1\times j}}\underbrace{y_j}_{\in\R^{j}} \right\Vert  
            \end{equation}
            
            $\implies \left\Vert r_j \right\Vert_2$ wird minimal, falls $y_j$ als Lösung des linaren Ausgleichsproblems 
            $H_{j+1,j}y_j=\beta e_1$ gewählt wird.

            \begin{algorithm}[H] % Vllt. auch 3.18?
                \caption{GMRES-Verfahren, Generalized Minimal Residual Method, Prototyp}\label{a3.18}
                \textbf{Input:} $A\in\R^{n\times n},b\in\R^n,x_0\in\R^n,m\in\N$\\
                \textbf{Output:} $x_m\in x_0+K_m(A,r_0),\left\Vert r_m \right\Vert_2=\left\Vert b-Ax_m \right\Vert_2$ minimal
                \begin{algorithmic}
                \State $r_0=b-Ax_0,\beta=\left\Vert r_0 \right\Vert_2,v_1=\frac{r_0}{\beta}$
                \For{$j=1,m$}
                    \State Bestimme $V_{j+1}, H_{j+1,j}$ mit Arnoldi-Verfahren (Stop beim Abbruch)
                    \State Löse $H_{j+1,j}y_j=\beta e_1$
                    \State $x_j=x_0+V_jy_j$
                    \State Konvergenztest
                \EndFor
                \end{algorithmic}
            \end{algorithm}

            \begin{remark}\label{b3.19} % vllt. 3.19
                \begin{enumerate}
                    \item Bis auf die Minimalitätseigenschaft von $\left\Vert r_j \right\Vert_2$ stimmen die Algotihmen \ref{a3.16} und \ref{a3.18} weitgehen überein.
                    D.h. sie haben auch ähnliche Nachteile.
                \item Algorithmus \ref{a3.18} kann auch als Projektionsmethode mit $K=K_m(A,r_0)$ und 
                      $L=AK$ hergeleitet werden. (2 Zeiler, wenn man die richtige Formel sieht)
                \end{enumerate}
            \end{remark}

            \underline{\textbf{Frage:}} wie können wir den Algortihmus \ref{a3.18} praxistauglich machen.
            D.h.: Was ist ein geeigneter Konvergenztest und wie lösen wir das lineare Ausgleichsproblem?

            \underline{\textbf{Beobachte:}} ()\ref{g3.7}) sagt: $\left\Vert r_j \right\Vert_2$ ist genau der Fehler des linearen 
            Ausgleichsproblems. Diesen Fehler können wir explizit berechnen!

            Analog zum Beweis von Satz \ref{s2.11}:

            Sei \(H_{j+1,j}=Q_jR_j=\underbrace{Q_j}_{\in\R^{j+1,j+1}}\begin{bmatrix}
                R_j\\\underbrace{0}_{\in\R}
            \end{bmatrix}\) eine QR-Zerlegung.

            Sei 
            \[
                Q^t\beta e_1 = \begin{bmatrix}
                    \overbrace{b_1}^{\in\R^j}\\\underbrace{b_2}_{\in\R}
                \end{bmatrix}
            \]

            \[
                \left\Vert r_j \right\Vert_2^2 \stackrel{(\ref{g3.7})}{=} \left\Vert \beta e_1- H_{j+1,j}y_j \right\Vert_2^2 = \left\Vert  Q^t(\beta e_1- H_{j+1,j}y_j)  \right\Vert_2^2 = (\star)
            \]

            
            \[
                (\star) = \left\Vert \begin{pmatrix}
                    b_1-\tilde{R}_jy_j\\
                    b_2
                \end{pmatrix} \right\Vert_2^2    
            \]

            \[
                = \underbrace{\left\Vert  b_1-\tilde{R}_jy_j\right\Vert_2^2}_{=0\text{ falls $H_{j+1,j}$ bzw. $\tilde{R}_j$ vollen Rang hat}} + \underbrace{\left\Vert b_2 \right\Vert_2^2}_{=\vert b \vert^2} 
            \]
             
            \[
                \implies \left\Vert r_j \right\Vert_2 = \left\vert b_2 \right\vert    
            \]  

            Wir berechnen $b_2$ als Nebenprodukt beim Lösen des linearen Ausgleichsrpoblems.

            \underline{\textbf{Aber:}} In jeder Iteration eine QR-Zerlegung zu berechnen ist teuer. Wie geht es besser?

            \underline{\textbf{Beobachte:}} \begin{itemize}
                \item $H_{j+1,j}$ hat Hessenbergstruktur, d.h. \(H_{j+1,j}\) hat Ist eine rechte obere Dreiecksmatrix, wo zusätzlich die utere Diagonale nicht notwendigerweise 0 ist. 
                Dafür kann mit $j$ Givensrotaionen eine QR-Zerlegung berechnet werden.
                \item Beim Iterationsschritt $j\implies j+1$ werden jediglich eine neue Spalte und eine neue Zeile an $H_{j+1,j}$ drangehängt um 
                um $H_{j+2,j+1}$ zu erhalten. Die restlichen Einträge bleiben unverändert.
            \end{itemize}

            Sei $H_{j+1,j}=Q_jR_j$ eine QR-Zerlegung.

            \begin{equation*}
                \begin{bmatrix}Q_j^t & 0\\ 0 & 1\end{bmatrix} H_{j+2,j+1} = 
                \begin{bmatrix}Q_j^t & 0\\ 0 & 1\end{bmatrix} \begin{bmatrix}H_{j+1,j} & \star\\ 0 & \star \end{bmatrix} =
            \end{equation*}

            % Matrix einfügen

            Die letzte Matrix kann mittels einer einzigen Givensrotation in obere Dreiecksgestalt gebracht werden.

            $\implies$ Erweitere QR-Zerlegung iin jedem Schrit.

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 08 am 08.11.2022\xrfill[0.7ex]{1pt}
            
            \begin{algorithm}[H] % Vllt. auch 3.18?
                \caption{GMRES}\label{a3.20}
                \textbf{Input:} $A\in\R^{n\times n},b\in\R^n,x_0\in\R^n,m\in\N$\\
                \textbf{Output:} $x_m\in x_0+K_m(A,r_0),\left\Vert r_m \right\Vert_2=\left\Vert b-Ax_m \right\Vert_2$ minimal
                \begin{algorithmic}
                \State $r_0=b-Ax_0,\beta=\left\Vert r_0 \right\Vert_2,v_1=\frac{r_0}{\beta}$
                \State $\hat{b}=\beta e_1$
                \For{$j=1,m$}
                    \State Bestimme $V_{j+1}, H_{j+1,j}$ mit Arnoldi-Verfahren \Comment Durch Anhängen von Spalten/ Zeilen, Stop beim Abbruch
                    \State Wende $G_{12},G_{23},\dots,G_{j-1,j}$ auf die letzte Spalte von $H_{j+1,j}$ an
                    \State Bestimme $G_{j,j+1}$ so, dass $H_{h+1,j}=G_{j,j+1}H_{j+1,j}=\begin{bmatrix}
                        \tilde{R}_{j+1}\\0
                    \end{bmatrix}$
                    \State $\hat{b}=G_{j,j+1}\hat{b}$
                    \If{$\hat{b}$ klein}
                        \State Löse $\tilde{R}_jy_j = [\hat{b}_i]_{i=1,\dots,j}$
                        \State Gebe $x_j=x_0+V_jy_j$ zurück
                    \EndIf
                \EndFor
                \State Berechne $x_m$ wie oben, beginne von vorne mit $x_0=x_m$
                \end{algorithmic}
            \end{algorithm}

            \begin{remark}\label{b3.21}
                \begin{enumerate}
                    \item Außer, dass GMRES für invertierte Matrizen für $m=n$ konvergiert,
                    ist bis heute wenig über Konvergenzaussagen bekannt.
                    \item Außer der Matrix-Vektor-Multiplikation im Arnoldi-Verfahren sind an der j-ten Iteration 
                    nur Vektor/ Matrizen der größe $~j$, bzw. $~j\times j$ beteiligt.
                    Der Lösungsvektor wird erst nach erfülltem Konvergenzkriterium zusammengesetzt.
                    \item Für $m\ll n$ sprechen wir von einem \textbf{Restarted-GMRES}. Oft ist z.B. $m=20$
                    ausreichend.
                    \item Das Verfahren der vollständigen Orthogonalisierung kann analog abgeändert werden.
                    \item Falls das Arnoldi-Verfahren abbricht, ist die Nährungslösung exakt (``Lucky Breakdown'', Übung).
                    \item Das GMRES-Verfahren ist heutzutage (2022) eines der beliebtesten Verfahren zum Lösen LGS ohne
                    weitere, besondere Eigenschaften.
                \end{enumerate}
            \end{remark}

        \section{Der symmetrische Lanczos-Prozess}

            \underline{\textbf{Frage:}} Können wir ein besseres Verfahren herleiten, wenn wir zusäztliche
            Annahmen zu unserer Matrix treffen?
            z.B. Symmetrie oder spd?

            \underline{\textbf{Beobachte:}} Für $A$ symmetrisch ist 
            \begin{equation*}
                H_{m,m} \stackrel{(\ref{g3.6})}{=} V_m^t A V_m
            \end{equation*}
            symmetrisch und hat Hessenberg-Struktur 
            \begin{equation*}
                H_{m,m}=\begin{bmatrix}
                    \alpha_1 & \beta_1 & & & & \\
                    \beta_1 & \alpha_2 & & & & \\
                     & \beta_2 & \ddots & \ddots&  & \\
                     &  & \ddots & \ddots & \ddots  & \\
                     &  & & \beta_{n-1} & \alpha_{n-1} & \beta_{n-1}\\
                     &  & & & \beta_n & \alpha_n 
                \end{bmatrix}
            \end{equation*} 

            Die vom Arnoldi-Verfahren generierte Matrix $T_m=H_{m,m}$ ist tridiagonal und symmetrisch.
            \underline{\textbf{Idee:}} 
            \begin{itemize}
                \item Die null-Einträge $h_{ij}=\langle v_j,v_i \rangle, i=1,\dots j-2$ müssen gar nicht erst berechnet werden
                \item Wir können die Synmmetrie im Algorithmus explizit ausnutzen.
            \end{itemize}
            
            \begin{algorithm}[H] % Vllt. auch 3.18?
                \caption{Lanczos-Verfahren}\label{a3.22}
                \textbf{Input:} $A\in\R^{n\times n}$ symmetrisch, $v\in\R^n,x_0\in\R^n,m\in\N$\\
                \textbf{Output:} $V_m=\begin{bmatrix}
                    v_1 & \dots v_m
                \end{bmatrix}$ von $K_m(A,v), v_{m+1}\in\R^n,\beta\in\R,T_m\in\R^{m\times m}$
                \begin{algorithmic}
                \State $\beta_1=0,v_0=0$
                \For{$j=1,m$}
                    \State $h_{ij}=0,i=1,\dots,j_2$
                    \State $h_{j-1,j}=\beta_j$
                    \State $w_j=z-\sum_{i=1}^{j-1}h_{ij}v_i=Av_j-\beta_jv_{j-1}$ \Comment In der Praxis hat $w$ keinen Index
                    \State $\alpha_j = \langle w_j, v_j\rangle_2=h_{jj}$
                    \State $w_j=w_j-\alpha_j v_j$ \Comment $w=Av_j-\alpha_jv_j-\beta_jv_{j-1}$
                    \State $\beta_{j+1}=\left\Vert w_j \right\Vert_2$
                    \If{$\beta_{j+1}=0$} \Comment in der Praxis: testen ob Betrag klein
                        \State Stop  
                    \EndIf
                    $v_{j+1}=\frac{w_j}{\beta_{j+1}}$
                \EndFor
                \State Berechne $x_m$ wie oben, beginne von vorne mit $x_0=x_m$
                \end{algorithmic}
            \end{algorithm}

            \begin{remark}\label{b3.23}
                Das Lanczos-Verfahren implementiert die Berechnung der ONB als Drei-Term-Rekursion,
                die für höhere Iterationszahlen instabil werden kann.
            \end{remark}

            \underline{\textbf{Idee:}} Wende unser neues Verfahren auf die Methode der vollständigen
            Orthogonalisierung an.

            \begin{algorithm}[H] % Vllt. auch 3.18?
                \caption{Lanczos-Verfahren für lineare GLeichungssysteme}\label{a3.24}
                \textbf{Input:} $A\in\R^{n\times n}$ symmetrisch, $b\in\R^n,x_0\in\R^n,m\in\N$\\
                \textbf{Output:} $x_m\in x_0 +K_m(A,r_0), x_m\approx A^{-1}b$
                \begin{algorithmic}
                \State $r_0=b-Ax_0,\beta=\left\Vert r_0 \right\Vert_2,v_1=\frac{r_0}{\beta_1}$
                \For{$j=1,m$}
                    \State Bestimme $V_j,T_j$ mittel Lanczos-Verfahren (Stop bei Abbruch)
                    \State Löse $T_jy_j = \beta e_1$
                    \State $x_j =x_0 V_jy_j$
                    \State Konvergenztest
                \EndFor
                \State Berechne $x_m$ wie oben, beginne von vorne mit $x_0=x_m$
                \end{algorithmic}
            \end{algorithm}

            \begin{remark}\label{b3.25}
                \begin{enumerate}
                    \item Lemma \ref{l3.17}, d.h. 
                    \[\left\Vert r_j \right\Vert_2=\left\Vert b-Ax_j \right\Vert_2=\beta_{j+1}\left\vert e_j^ty_j \right\vert\]
                    gilt weiterhin. Wir können den Konvergenztest also ohne Berechnung von $x_j$ durchführen.
                    \item Da $T_j$ tridiagonal ist, kann $T_jy_j=\beta e_1$ in $O(j)$ gelöst werden.
                    \item Speicherbedarf und Rechenaufwand wachsen immer noch mit $j$. 
                \end{enumerate}
            \end{remark}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 09 am 10.11.2022\xrfill[0.7ex]{1pt}
            
            \underline{\textbf{Frage:}}  Können wir es vermeiden die Basis $V_j$ vom Krylovraum 
            zu speichern?

            \underline{\textbf{Beobachte:}} Betrachte $LR$-zerlegung
            \begin{equation*}
                T_m=L_mR_m = \begin{pmatrix}
                    1 & & &\\
                    \lambda_2 & \ddots & & \\
                    & \ddots & \ddots &\\
                    && \lambda_m & 1
                \end{pmatrix}
                \begin{pmatrix}
                    \eta_1 & \beta_2& &\\
                     & \ddots & \ddots& \\
                    && \ddots & \beta_m \\
                    &&  & \eta_m
                \end{pmatrix}
            \end{equation*}

            Zeile und Spalte ergänzen:
            \[\lambda_m=\frac{\beta_m}{\eta_{m+1}}, \eta_m = \alpha_m -\lambda_m\beta_m\]

            Wobei $\alpha_i$ die Einträge auf der Diagonalen und $\beta_j$ die Einträge der Nebendiagonalen von $T_j$ sind.

            \[\implies x_m= x_0 + \underbrace{V_my_m}_{T^{-1}_m\beta e_1=R^{-1}_mL^{-1}_m \beta e_1}\]
            \[x_0=\underbrace{V_mR^{-1}_m}_{P_m}\underbrace{L^{-1}_m\beta e_1}_{z_m}\]

            \[P_mR_m=V_m\implies v_m=\beta_m p_{m-1}+\eta_m p_m\]
            \[\implies p_m=\frac{1}{2}(v_m-\beta_m p_{m-1})\]

            D.h. $P_m$ kann mit wachsendem $m$ einfach und effizient berechnet werden.

            % Falsch
            \[z_m=L_m^{-1}\beta e_1=\begin{pmatrix}
                &  &&& \textbf{0}\\
                & &L_{m-1}^{-1}& & \vdots\\
                &  & &&\textbf{0}\\
                0 & \dots & 0 & -\lambda_m & 1
            \end{pmatrix}
            \begin{pmatrix}
                \beta \\
                \\
                \\
                \\
                0
            \end{pmatrix}
            =\begin{pmatrix}
                \\
                \\
                z_{m-1}
                \\
                \\
                \zeta_m 
            \end{pmatrix}\]
            %Falsch Ende 
            \[\begin{bmatrix}
                &  &&& \\
                & &L_{m-1}^{-1}& &\\
                &  & &&\\
                0 & \dots & 0 & -\lambda_m & 1
            \end{bmatrix}\begin{bmatrix}
                \\
                \\
                z_{m-1}
                \\
                \\
                \zeta_m 
            \end{bmatrix}=L_mz_m=\beta e_1 = \begin{bmatrix}
                \beta \\
                0\\
                \vdots\\
                0
            \end{bmatrix}\]
            Mit $\zeta_m=-\lambda_m \zeta_{m-1}$

            \begin{align*}
                \implies x_m&=x_o+p_mz_m\\
                &= x_0 + \begin{bmatrix}
                    p_{m-1} & p_m
                \end{bmatrix}\begin{bmatrix}
                    z_{m-1}\\
                    \zeta_m
                \end{bmatrix}\\
                &=\underbrace{x_0+P_{m-1}z_{m-1}}_{x_{m-1}}+\zeta_mp_m\\
                &=x_{m-1}+\zeta_mp_m
            \end{align*}

            \begin{algorithm}[H]% Vllt. auch 3.18?
                \caption{Direkes Lanczos-Verfahren für LGS}\label{a3.26} 
                \textbf{Input:} $A\in\R^{n\times n}$ symmetrisch, $b\in\R^n,x_0\in\R^n,m\in\N$\\
                \textbf{Output:} $x_m\in x_0 +K_m(A,r_0), x_m\approx A^{-1}b$
                \begin{algorithmic}
                \State $r_0=b-Ax_0,\zeta_1=\beta=\left\Vert r_0 \right\Vert_2,v_1=\frac{r_0}{\beta_1}$
                \State $\beta_1=0\in\R,\eta_0=1\in\R$
                \State $p_0=0\in\R^n$
                \For{$j=1,m$}
                    \State $W_j=Av_j-\beta_jv_{j-1}$ \Comment Wie beim Lanczos
                    \State $\alpha_j=\langle w_j,v_j \rangle$ \Comment Wie beim Lanczos
                    \State $\frac{\beta_j}{\eta_{j-1}},\eta_j=\alpha_j-\lambda_j\beta_j$
                    \State $\zeta_j=-\lambda_j\zeta_{j-1}$ \Comment: Für $j>1$
                    \State $p_j=\frac{1}{2j}(v_j-\beta_jp_{j-1})$
                    \State $x_j=x_{j-1}+\zeta_jp_{j-1}$
                    \State Konvergenztest 
                    \State $w_j=w_j-\alpha_jv_j$
                    \State $\beta_{j+1}=\left\Vert w_j \right\Vert_2$, stop, falls $\beta_j=0$
                    \State $v_{j+1}=\frac{1}{\beta_{j+1}}w_j$
                \EndFor
                \State Berechne $x_m$ wie oben, beginne von vorne mit $x_0=x_m$
                \end{algorithmic}
            \end{algorithm}

            \begin{remark}\label{b3.27}
                \begin{enumerate}
                    \item Wir müssen nur $v_j,v_{j-1},w_j,p_j$ und $x_j$ aktiv im Speicher behalten
                    Der Speicherbedarf und Rechenaufwand pro Iteration ist unabhängig von $j$.
                    \item Algorithmus \ref{a3.24} und \ref{a3.26} sind matemathisch äquivalent, d.h.bei 
                    exater Arithmetik erhalten wir die gleichen Iterierten. Wir müssen aber keine Vektoren vom Krylovraum speichern.
                    \item Bisher haben wir nur eine schnelle Version des Verfahrens der vollständigen Orthogonalisierung (Algorithmus \ref{a3.16})
                    für symmetrische Matrizen hergeleitet. Unser Verständnis der Fehleranalyse hat sich nicht verbessert.
                \end{enumerate}
            \end{remark}

        \section{Das Verfahren der konjugierten Gradienten}
    
            Auch: Method of conjugate gradients / CG-Verfahren

            \begin{lemma}\label{l3.28}
                Seien $v_j,r_j=b-Ax_j$ und $p_j$, $j=0,1,\dots$ wie in dem Algorithmen \ref{a3.24} und \ref{a3.26} generiert.
                Dann gilt:
                \begin{enumerate}
                    \item $r_j=\sigma_{j+1} v_{j+1}$ mit $\sigma_{j+1}\in\R$. D.h. insbesondere, dass
                    die Residuuen $r_j$ orthogonal sind.
                    \item Die $p_j$ sind $A$-orthogonal, d.h. es gilt \[\langle p_i,p_j \rangle_A=\langle Ap_i,p_j \rangle_2\]
                    für $i\neq j$. 
                \end{enumerate}
            \end{lemma}

            \begin{proof}
                \textbf{1.} folgt aus Lemma \ref{l3.17}:
                \[r_j=\underbrace{-h_{j+1,j}(e_j^ty_j)}_{\in\R}v_{j+1}\]

                \textbf{2.}: Es reicht zu zeigen, dass $P_m^t A\underbrace{P_m}_{V_m R_m^{-1}}=R_m^{-t} \underbrace{V_m^t A V_m}_{=T_m=L_mR_m} R_m^{-1} = R_m^{-t}L_m$

                $P_m^tA P_m$ ist symmetrisch und $R_m^{-t}L_m$ ist untere Dreiecksmatrix $\implies P_m^tA P_m$ is diagonal.

             \end{proof}

            \underline{\textbf{Idee:}} Vereinfache Algorithmus \ref{a3.26} zu einem Verfahren der Form
            \begin{equation}\label{g3.9} % Finde 3.8
                x_{j+1}=x_j+\tilde{\alpha}_j\tilde{p}_j, \tilde{p}_{j+1}=r_{j+1}+\tilde{\beta}_j\tilde{p}_j    
            \end{equation}
            benutze dazu de Eigenschaften aus Lemma \ref{l3.28}.
            
            Wie sieht $r_{j+1}$ aus?
            \begin{align*}
                r_{j+1}&=b-Ax_{j+1}\\   
                &\stackrel{\ref{g3.9}}{=} b-Ax_j-\tilde{\alpha}_j A  \tilde{p}_j
            \end{align*}

            \begin{equation}\label{g3.10}
                =r_j-\tilde{\alpha}_j A  \tilde{p}_j
            \end{equation}

            Außerdem \begin{align*} 
                0=\langle r_{j+1},r_j \rangle_2 = \langle r_j-\tilde{\alpha}_j A  \tilde{p}_j,r_j \rangle_2 = \langle r_j,r_j \rangle_2-\tilde{\alpha}_j^2 \langle A\tilde{p}_j,r_j \rangle_2
                \\ \implies \tilde{\alpha_j}=\frac{\langle r_j,r_j \rangle_2}{\langle A\tilde{p}_j,r_j \rangle_2}=\frac{\langle r_j,r_j \rangle}{\langle A\tilde{p}_j,\tilde{p}_j \rangle_2}
            \end{align*}

            \begin{align*}
                0\stackrel{!}{=}\langle A\tilde{p}_j,r_j \rangle_2 = \langle Ar_{j+1},\tilde{p}_j \rangle_2 +\tilde{\beta}_j \langle A\tilde{p}_j, \tilde{p}_j\rangle_2\\
                \implies \tilde{\beta}_j=-\frac{\langle r_{j+1},A\tilde{p}_j \rangle_2}{\langle A\tilde{p}_j,p_j \rangle_2}\\
                \stackrel{(\ref{g3.10})}{=}\frac{1}{\alpha_j} \frac{\langle r_{j+1}, r_{j+1}\rangle}{\langle A\tilde{p}_j,\tilde{p}_j \rangle_2}=\frac{\langle r_{j+1},r_j \rangle_2}{\langle r_j,r_j \rangle_2}
            \end{align*}
                

            \begin{algorithm}[H]% Vllt. auch 3.18?
                \caption{CG-Verfahren}\label{a3.29} 
                \textbf{Input:} $A\in\R^{n\times n}$ spd, $b\in\R^n,x_0\in\R^n$\\
                \textbf{Output:} $x_m\approx A^{-1}b$
                \begin{algorithmic}
                \State $r_0=b-Ax_0,\tilde{p}_0=r_0$
                \For{$j=1,m$}
                    \State $\tilde{\alpha_j}=\frac{\langle r_j,r_j \rangle_2}{\langle A\tilde{p}_j,\tilde{p}_j \rangle_2}$
                    \State $x_{j+1} = x_j +\tilde{\alpha}_j\tilde{p}_j$
                    \State $r_{j+1}=r_j-\tilde{\alpha}_jA\tilde{p}_j$
                    \State $\tilde{\beta}_j=\frac{\langle r_{j+1},r_j \rangle_2}{\langle r_j,r_j \rangle_2}$
                    \State $\tilde{p}_{j+1}=r_{j+1}+\tilde{\beta}_j\tilde{p}_j$
                    \State Konvergenztest
                \EndFor
                \end{algorithmic}
            \end{algorithm}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 10 am 15.11.2022\xrfill[0.7ex]{1pt}
            
            \underline{\textbf{Nachtrag}}:
            
            Beim direken Lanczosverfahren: 
            \[\rho_j=\frac{1}{2j}(v_j-\beta_jp_{j-1})\]
            \[x_j=x_{j-1}+\zeta_jp_j\]
            Es folgt mit $r_j=\sigma_{j+1}v_{j+1},\sigma_{j+1}\in\R$, dass die $r_j$ orthogonal sind.
            Dann gilt \[\langle Ap_j,p_i \rangle_2=0, i\neq j\]
            \[\implies x_{j+1}=x_j+\tilde{\alpha}_j\tilde{p}_j\] mit 
            \[\tilde{p}_{j+1}=r_{j+1}+\tilde{\beta}_{j}\tilde{p}_j\]
            wobei es von $\tilde{p}$ zu $p$ einen Indexshift gibt!

            \begin{remark}\label{b3.30}
                \begin{enumerate}
                    \item Man kann per Induktion zeigen, dass die Iterierten $x_j$ (und damit die Residuen $r_j$)
                    des CG-Verfahrens stimmen mit denen des direkten Lanczos-Verfahrens überein.
                    Die $\tilde{p}_i$ sind vielfache der $p_{j+1}$. 
                    \item Da CG-Verfahren muss nur $x_j,r_j\tilde{p}_j$ und (zur Beschleunigung) $A\tilde{p}_j$ speichern.
                    \item Mit $\langle r_j,r_j \rangle_2=\left\Vert r_j \right\Vert_2^2$ wird $\left\Vert r_j \right\Vert_2$ bis auf Kosten einer Quadratwurzel berechnet.
                \end{enumerate}
            \end{remark}

            \underline{\textbf{Was ist neu?}} $A$ spd $\implies \langle A\cdot,\cdot \rangle_2=\langle \cdot,\cdot \rangle_A$ ist ein Skalarprodukt und $\left\Vert \cdot \right\Vert_A = \sqrt{\langle \cdot,\cdot \rangle_A}$ ist eine Norm.

            Die Iterierten von $CG$ und Lanczos sind gleich $\implies $ CG-Verfahren ist ein Projektionsverfahren.

            Finde $x_j\in x_0+K_j(A,r_0)$ mit $r_j=b-Ax_j\perp R_j(A,r_0)$.

            \begin{align*}
                \implies 0 &= \langle r_j,y \rangle_2,y\in K_j(A,r_0)\\
                &=\langle b-Ax_j,y \rangle_2 \\
                &= (\star)
            \end{align*}

            \begin{align*}
                (\star)&=\langle Ax-Ax_j,y \rangle_2\\
                &= \langle A(x-x_j),y \rangle_2\\ 
                &= \langle x-x_j,y \rangle_A
            \end{align*}

            \begin{equation*}
                \implies x-x_j\perp_A K_j(A,r_0)
            \end{equation*}


            \begin{equation}\label{g3.11}
                \stackrel{\text{Satz \ref{s1.9}}}{\implies} \left\Vert x-x_j \right\Vert_A=\min_{y\in K_j(A,r_0)} \left\Vert x-y \right\Vert_A
            \end{equation}

            \begin{remark}\label{b3.31}
                (\ref{g3.11}) ist eine Aussage über den \underline{Fehler}, nicht das Residuum!
            \end{remark}

            \begin{lemma}\label{l3.32}
                Sei $x_j$ die j-te Iterierte des CG-Verfahrens. Dann ist $x_j=x_0 +q_{j-1}(A)r_0$, wobei $q_{j-1}\in \Pi_{j-1}$ mit 
                \begin{equation}
                    \left\Vert \underbrace{(I-q_{j-1}(A)A)(x-x_0)}_{=x-x_j} \right\Vert_A=\min_{p\in\Pi_{j-1}} \left\Vert (I-p(A)A)(x-x_0) \right\Vert
                \end{equation}
            \end{lemma}

            \begin{proof}
                $x_j=x_0+K_j(A,r_0)\implies x_j=x_0+p_{j-1}(A)\underbrace{r_0}_{=b-Ax_0=A(x-x_0)},p_{j-1}\in\Pi_{j-1}$.
                Mit (\ref{g3.11}) folgt die Behauptung.
            \end{proof}

            \begin{lemma}\label{l3.33}
                Für die Iterierten $x_j$ des CG-Verfahrens gilt 
                \begin{equation}\label{g3.12}
                    \frac{\left\Vert x-x_j \right\Vert_A}{\left\Vert x-x_0 \right\Vert_A}\leq \min_{p\in\Pi_j,p(0)=1} \max_{\lambda \text{ Ew von } A} \left\vert p(\lambda) \right\vert
                \end{equation}
            \end{lemma}

            \begin{proof}
                Seien $(\lambda_i,w_i)_{i=1}^n$ die Eigenpaare von $A$. $x-x_0=\sum_{i=1}^n\xi_i w_i$.
                \begin{align*}
                    \left\Vert x-x_j \right\Vert_A^2 &\stackrel{\text{Lemma \ref{l3.32}}}{=}\min_{p\in\Pi_j, p(0)=1}\left\Vert p(A)(x-x_0) \right\Vert_A^2\\
                    &=\left\Vert p(A)\sum_{i=1}^n \xi_i w_i\right\Vert_A^2\\
                    &=\left\vert\sum_{i=1}^n \xi_i p(\lambda_i)w_i\right\Vert_A^2\\
                    &= \langle A\sum_{i=1}^n \xi_ip(\lambda_i)w_i,\sum_{i=1}^n \xi_ip(\lambda_i)w_i \rangle_2\\
                    &=\langle \sum_{i=1}^n \lambda_i\xi_ip(\lambda_i)w_i,\sum_{i=1}^n \xi_ip(\lambda_i)w_i \rangle_2\\
                    &=\sum_{i=1}^n \lambda_i \xi_i^2 p(\lambda_i)^2\\ 
                    &\leq \max_{i=1,\dots n} \left\vert p(\lambda_i) \right\vert \underbrace{\sum_{i=1}^n\lambda_i^2\xi_i^2}_{\left\Vert x-x_0 \right\Vert_A^2}
                \end{align*}
            
            \end{proof}

            \begin{corollary}\label{k3.34}
                Falls $A$ spd $l$ verschiedene Eigenwerte hat, dann konvergiert das Verfahren in $l$ Schritten.
            \end{corollary}

            \begin{proof}
                Wähle
                \begin{equation*}
                    p(x)=\left(1-\frac{x}{\lambda_1}\right)\cdot \dots \cdot \left(1-\frac{x}{\lambda_l}\right)
                \end{equation*}
                in Lemma \ref{l3.33}.
            \end{proof}

            \underline{\textbf{Ziel:}} Finde eine ähnliche Konvergenzaussage für Eigenwerte $0<\lambda_{\min}=\lambda_1\leq \lambda_2\leq \dots \leq \lambda_n=\lambda_{\max}<\infty$ von spd Matrizen.

            \underline{\textbf{Idee:}} Beschränke die rechte Seite von (\ref{g3.12}) geeignet:
            \begin{equation}\label{g3.13}
                \min_{p\in\Pi_j,p(0)=1}\max_{\lambda \text{ Ew von }A} |p(\lambda)|\leq \min_{p\in\Pi_j,p(0)=1}\underbrace{\max_{\lambda\in [\lambda_{\min},\lambda_{\max}]} |p(\lambda)|}_{\left\Vert p \right\Vert_C([\lambda_{\min},\lambda_{\max}])}                 
            \end{equation}

            \underline{\textbf{Frage:}} Für welche oder welches Polynom ist dieser Ausdruck minimal?
 
            \begin{definition}[Alma 2: Definition 15.10]\label{d3.35}
                Für $n\in\N_0$ sind die \underline{\textbf{Tschebyscheff-Polynome}} definiert als 
                \[T_n(x)=\cos(n\cos^{-1}(x))\in\Pi_n, x\in [-1,1]\]
            \end{definition}

            \begin{theorem}[Alma 2: Satz 15.12]\label{s3.36}
                Unter allen Polynomen $p_n(x)=x^n+ \dots \in \Pi_n$ ist $\left\Vert p_n \right\Vert_{C([-1,1])}$ für $p_n=2^{1-n}T$ minimal.
            \end{theorem}

            \underline{\textbf{Idee:}} Modifiziere diese Aussage um (\ref{g3.13}) zu beschränken.

            \begin{lemma}\label{l3.37}
                Sei $[a,b]\subset \R$ und $t_0\in\R\setminus[a,b]$. Dann minimiert 
                \begin{equation*}
                    \hat{T}_n(t)=\frac{T_n(x(t))}{T_n(x(t_0))},x(t)=2\frac{t-a}{b-a}-1
                \end{equation*}
                den Ausdruck $\left\Vert \cdot \right\Vert_{C([a,b])}$ unter allen Polynomen $p\in\Pi_n$ mit $p(t_0)=1$
            \end{lemma}

            \begin{proof}
                Übung.
            \end{proof}

            \begin{equation}\label{g3.14}  
                \implies \min_{p\in\Pi_j,p(0)=1}\max_{\lambda \text{ Ew von }A} |p(\lambda)| \stackrel{(\ref{g3.13}), \text{Lemma \ref{l3.37}}}{\leq} \left\Vert \hat{T}_j \right\Vert_{C([\lambda_{\min},\lambda_{\max}])}=\frac{1}{\left\vert T_j(x(0)) \right\vert}
            \end{equation}

            \begin{theorem}\label{s3.38}
                Für den Approximationsfehler $x-x_0$ in der j-ten Iteration des CJ-Verfahrens gilt
                \begin{equation*}
                    \left\Vert x-x_j \right\Vert_A\leq 2\left(\frac{\sqrt(\cond_2(A))-1}{\sqrt{\cond_2(A)}+1}\right)^j\left\Vert x-x_0 \right\Vert_A
                \end{equation*}
            \end{theorem}

            \begin{proof}
                In (\ref{g3.14}) ist 
                \begin{align*}
                    x_0&=2\frac{0-\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}-1\\
                    &=-\frac{\lambda_{\max}+\lambda_{\min}}{\lambda_{\max}-\lambda_{\min}}\\
                    &=-\frac{\cond_2(A)+1}{\cond_2(A)-1}
                \end{align*}

                Außerdem: 
                \begin{align*}
                    T_j(x(0))&=T_j\left(-\frac{\cond_2(A)+1}{\cond_2(A)-1}\right)\\
                    &\stackrel{\text{Übung}}{=}\frac{1}{2}\left(\left(\frac{\sqrt(\cond_2(A))+1}{\sqrt{\cond_2(A)}-1}\right)^j+\left(\frac{\sqrt(\cond_2(A))-1}{\sqrt{\cond_2(A)}+1}\right)^j\right)\\
                    &\geq \frac{1}{2}\left(\frac{\sqrt(\cond_2(A))+1}{\sqrt{\cond_2(A)}-1}\right)^j
                \end{align*}

                Dann folgt mit (\ref{g3.14}) die Behauptung.

            \end{proof}

            \begin{remark}\label{b3.39}
                \begin{enumerate}
                    \item Das Verfahren des steilsten Absties hat eine ähnliche Abschätzung aber mit $\cond_2(A)$ statt $\sqrt{\cond_2(A)}$.
                    \item Für spd Matrizen führen die Algorithmen \ref{a3.16}, \ref{a3.24}, \ref{a3.26} auf die gleiche Abschätzung, sind aber teurer. Das 
                    CG-Verfahren ist das wichtigste aller Verfahren (auch in der Prüfung!) zum Lösen von LGS. 
                \end{enumerate}
            \end{remark}

            $$\underbrace{M}_{\approx A^{-1}}Ax=Mb$$ % anderer Ansatz, bei schlechter Kondition

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 11 am 17.11.2022\xrfill[0.7ex]{1pt}
            
    \chapter{Lineare Eigenwertprobleme}

        \section{Problemstellung und Beispiele}

            \begin{definition}\label{d4.1}
                Gegeben sei eine Matrix $A\in\C^{n\times n}$ mit $n\in\N$. $(\lambda,x)\in\C\times \C^n\setminus{0}$ heißt 
                \textbf{Eigenpaar} (EP) von $A$, falls 
                \begin{equation}\label{g4.1}
                    Ax=\lambda x
                \end{equation}
                $\lambda$ heißt Eignewert (Ew) und $x$ Eigenvektor (EV). (\ref{g4.1}) heißt \textbf{spezielles Matrixeigenwertproblem}.
            \end{definition}

            \begin{remark}\label{b4.2}
                \begin{enumerate}
                    \item Gegeben $A,B\in\C^{n\times n}$ heißt 
                        \begin{equation}\label{g4.2}
                            Ax=\lambda Bx
                        \end{equation}
                        das \textbf{allgemeine Matrixeigenwetrproblem}. Falls $B$ invertierbar ist, dann ist (\ref{g4.2}) äquivalent zu (\ref{g4.1}), d.h. 
                        \[B^{-1}Ax=\lambda x.\]
                        Falls $B$ eine Cholesky-Zerlegung hat, ist (\ref{g4.2}) äquivalent zu (\ref{g4.1}), d.h.
                        \[L^{-1}A x=\lambda \underbrace{L^tx}_{\coloneqq y}\]
                        \[L^{-1}AL^{-t}y=\lambda y\]
                    \item Wir betrachten nur spezielle Matrixeigenwertprobleme und sprechen nur von \textbf{Eigenwertproblemen}. Es gibt 
                        aber auch Algorithmen, doe (\ref{g4.2}) in der allgemeinsten Form lösen.
                    \item Ist $A=A^t\in\R^{n\times n}$ in (\ref{g4.1}) sprechen wir vom \textbf{symmetrischen Eigenwertproblem}. 
                \end{enumerate}
            \end{remark}

            \begin{example}\label{b4.4}
                Viele Räuber-Beute Modelle in Biologie, Epidemiologie und Wirtschaft können als gewöhnliche Differentialgleichungen geschrieben werden
                \begin{equation*}
                    y'(t)=Ay(t), y(0)=y_0\in\R^n,A\in\R^{n\times n}
                \end{equation*}
                geschrieben werden. 

                Sei $A=V\Lambda V^*$ eine Diagonalform von $A$ mit $\Lambda=\text{diag}(\lambda_1,\dots,\lambda_n)$. Dann ist 
                \begin{equation*}
                    y(t)=e^{At}y_0,y_0=Ve^{\Lambda t}V^*y_0=V\begin{bmatrix}
                        e^{\lambda_1t} & &\\
                        & \ddots & \\ 
                        && e^{\lambda_nt}
                    \end{bmatrix}
                \end{equation*}

                Mit \begin{equation*}
                    e^{At}=\sum_{i=0}^\infty \frac{(At)^i}{i!}=V\sum_{i=0}^\infty \frac{(\Lambda t)^i}{i!}V^*
                \end{equation*}

            \end{example}

            \begin{example}\label{b4.5}
                Gegeben einer Reihe von Messewerten  oder Datenpunkten $x_i\in\R^n,i=1,\dots m$, welche in einer Datenmatrix angeordnet sind:
                \begin{equation*}
                    X=[x_1\vert\dots\vert x_n]\in\R^{n\times n}
                \end{equation*}
                Der Einfachheit halber sei \[\frac{1}{m}\sum_{i=1}^nx_i=0\]
                Die Kovarianzmatrix 
                \[C=\frac{1}{m}XX^t=\frac{1}{m}\sum_{i=1}^n x_ix_i^t\]
                Seien $(\lambda_i,v_i),i=1,\dots n$ die EP von $C$, mit $\lambda_1\geq \lambda_2\dots\geq \lambda_n\geq 0$ 
                \begin{figure}[H]
                    \centering
                    \includegraphics[width=0.25\textwidth]{Bild012}
                    \caption{PCA Beispiel}
                \end{figure}
                Dann geben die Eigenwerte $\lambda_i$ die Varianzen der $x_i$ in Richtung $v_i$ an. D.h. die $x_i$ haben in Richtung 
                $v_i$ die größte Varianz, etc.
            \end{example}

            \begin{example}\label{b4.6}
                
                Das Schwingen einer Trommel der Form $\Omega\subset \R^2$, offen, mit Frequenz $\lambda$ erfüllt
                \begin{align*}
                    \Delta u(x)&=\lambda u(x), & x\in\Omega \\ 
                    u(x)&=0, & x\in\delta\Omega 
                \end{align*}
                Mit einem ähnlichen Vorgehen wie in Kapitel 3.1 erhalten wir ein Matrixeigenwertproblem 
                \[
                    Ax=\lambda x    
                \]
                Ein Beispiel für $A$ ist Beispiel \ref*{b3.1}. $A$ ist immer spd. Alternative Diskretisierungsmethoden 
                füren auf 
                \[
                    Ax=\lambda Bx    
                \]
                mit $A,B$ spd, sehr groß und dünn besetzt.

            \end{example}

            \begin{remark}\label{b4.7}
                Sei $A\in\C^{n\times n}$.
                \begin{enumerate}
                    \item Die EW von A sind die Nullstellen de charakteristischen Polynoms
                        \[p_A(\lambda) = \det(A-\lambda I)\]
                        \[p_A\in\Pi_n\implies p_A\text{ hat } n \text{ komplexe Nullstellen}\]
                        $\implies A $ hat $n$ (möglicherweise gleiche)  Eigenwerte.
                    \item Für $A$ hermitisch $A=A^*$ sind die EW reel. Die zugehörigen EV können als ONB genutz werden. Falls $A$ zusätzlich reell ist, können die EV reell
                        und orthogonal gewählt werden.
                    \item $A\in\R^{n\times n},Ax=\lambda x$ ($\lambda\in \C$ möglich), so folgt 
                        \[A\overline{x}=\overline{Ax}=\overline{\lambda x} = \overline{\lambda}\overline{x}\]
                        $\implies (\overline{\lambda},\overline{x})$ ist auch ein EP.
                    \item Die EW zu einem Gegeben EW sind \underline{nicht} eindeutig:
                        \[Ax=\lambda x \implies Ay=\lambda y,y=\alpha x,\alpha\in\C\setminus\{0\}\]
                        \begin{align*}
                            Ax_1=\lambda x_1 \\
                            Ax_2=\lambda x_2\\
                            \implies A\alpha x_1 +\beta x_2 = \lambda(\alpha x_1 +\beta x_2)
                        \end{align*}
                    \item hnliche Matrizen, d.h. Matrizen der Form $B=CAC^{-1}$, $C$ regulär, haben die gleichen EW.
                \end{enumerate}
            \end{remark}

            \underline{\textbf{Frage:}} Was können wir sonst noch sagen? 
        
        \section{Gerschgorin-Kreise (Abschätzungen für EW)}

            Sei $(\lambda,x)$ ein EP von $A\in\C^{n\times n}$. Betrachte die $l$te Zeile von $Ax=\lambda x$:

            \begin{equation}\label{g4.3}
                \lambda x_l = \sum_{j=1}^n a_{l_j}x_j=a_{ll}x_l+\sum_{j=1,j \neq l}^n a_{l_j}x_j
            \end{equation}


            Sei $i$ so, dass $\vert x_i\vert =\max_{j=1,\dots, n}|x_j|>0$.

            Dann gilt: 
            \begin{align*}
                \left\vert \lambda -a_{ii} \right\vert \left\vert x_i \right\vert &= \left\vert (\lambda-a_{ii})x_i \right\vert\\
                &\stackrel{(\ref{g4.3})}{=}\left\vert \sum_{j=1,j\neq i} a_{ij}x_j \right\vert\\
                &\leq \sum_{j=1,j\neq i}^n \left\vert a_{ij} \right\vert \left\vert x_i \right\vert \\ 
                &\leq \vert x_i \vert \sum_{j=1,j\neq i}^n \left\vert a_{ij} \right\vert \coloneqq r_i
            \end{align*}

            \[\implies \left\vert \lambda a_{ii} \right\vert\leq \sum_{j=1,j\neq i}^n \vert a_{ij}\vert= r_i\] % TODO: Fix

            $\implies$ $\lambda$ liebt in einem Kres mit Radius $r_i$.

            \underline{\textbf{Aber:}} $x$ ist unbekannt $\implies$ $i$ ist unbekannt 

            $\implies \lambda$ liebt in der Vereinigun aller solche Kreise

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 12 am 22.11.2022\xrfill[0.7ex]{1pt}
        
            \begin{theorem}\label{s4.7}[Satz von Gerschgorin]
                Alle Eigenwerte einer Matrix $A\in\C^{n\times n}$ liegen in 
                \[\bigcup_{i=1}^n K_i\] mit 
                \[K_i=\{z\in\C: \left\vert z-a_{ii} \right\vert \leq r_i\},r_i=\sum_{j=1,j\neq i}^n \left\vert a_{ij} \right\vert, i=1,\dots,n\]
            \end{theorem}

            \begin{proof}
                Oben.
            \end{proof}

            %Bild

            \begin{remark}\label{b4.8}
                \begin{enumerate}
                    \item Bilden $k$ Geschkorin-Kreise eine von den restlichen Kreisen disjunkte 
                    Punktmenge, so liegen in dieser Punktmenge genau $k$ EW.
                    \item Ist $A\in\R^{n\times n}$, dann haben $A$ und $A^t$ die gleichen Eigenwerte. 
                    $\implies$ EW von A liegen im Durchschnitt der jeweiligen Kreise.
                    \item 
                \end{enumerate}
            \end{remark}

        \section{Kondition des Eigenwertproblems}

            \underline{\textbf{Wiederholung:}} Die Kondition deschreibt, wie sich Änderungen in den
            Eingabedaten auf die Werte der Ausgabe auswirken. 
            \[
                \kappa_{\text{abs}}=\left\vert f'(x) \right\vert = \frac{\Delta y}{\Delta x}    
            \]

            \underline{\textbf{Frage:}} Ist das Eigenwertproblem gut- oder Schlechtkonditioniert?

            \begin{tcolorbox}[enhanced,breakable,
            title=Kondition des Problems]
            In Matrix \& Computations ist das ausführlich beschrieben. 
            \end{tcolorbox}
            

            \begin{example}\label{b4.9}
                Sei $\begin{bmatrix}
                    0 &1\\ 0&0
                \end{bmatrix}\implies $ EW sind $\lambda_{1/2}=0$.

                Sei $\tilde{A}=\begin{bmatrix}
                    0 & 1 \\ \delta & 0
                \end{bmatrix},\delta>0 \implies $ EW sind $\lambda_{1/2}=\pm\sqrt{\delta}$.

                \begin{equation*}
                    \kappa_{\text{abs}}=\frac{\left\vert 0-\pm\sqrt{\delta} \right\vert}{\left\Vert A-\tilde{A} \right\Vert_2} = \frac{\left\vert \sqrt{\delta} \right\vert}{\left\vert \delta \right\vert} \stackrel{\delta\to 0}{\to}\infty
                \end{equation*}

                Daher ist das Eigenwertproblem in diesem Fall schlechtkonditioniert.

            \end{example}

            \begin{remark}\label{b4.10}
                Allgemeiner kann man zeigen, dass (DAH Satz 5.2) die absolute Kondition eines einfachen Eigenwerts 
                $\lambda$ von $A\in\C^{n\times n}$ bzgl. $\left\Vert \cdot \right\Vert_2$ gegeben ist durch 
                \[\kappa_{\text{abs}}=\frac{\left\Vert x \right\Vert\left\Vert y \right\Vert}{\langle x,y \rangle_2} = \frac{1}{\left\vert \cos(\angle(x,y)) \right\vert}\]
                wo bei $Ax=\lambda x$ (Rechtseigenvektor) und $A^* y = \bar{\lambda} y$ (Linkseigenvektor).
                D.h. einfache Eigenwerte hermitischer Matrizen sind gutkonditioniert, da $x$ parallel zu $y$ ist. 
                
                Bei mehrfachen oder nahe zusammenliegenden EW ist die Berechnung einzelner EV-Suche schlechtkonditioniert.
                Die Berechnung einer ONB des zugehörigen Eingenpaars ist aber gutkonditioniert.
            \end{remark}
           
            \begin{remark}\label{b4.11}
                Die kanonische Idee die Eingenwerte als Nullstellen des charakteristischen Polynoms zu berechnen ist schlechtkonditioniert. Tatsächlich wird die Nullstellenberechnung in der Praxis
                als Matrixeigenwertproblem gelöst.
            \end{remark}

            \begin{tcolorbox}[enhanced,breakable,
                title=Berechnung der EW]
                    $(x-1)(x-2)\cdot \dots \cdot (x-20)$ ist ein gutes Beipsiel, warum das berechnen von Nst (hier Eigenwerten) schwer ist, weil die Koeffizienten sehr groß werden.
                    Außerdem ist das Berechnen des Polynoms, durch Berechnung der Determinante, teuer.
            \end{tcolorbox}

        \section{Vektoriteration (Potenzmethode, etc)}

            \underline{\textbf{Idee:}} Multipliziere die Matrix mit einem Vektor.

            \underline{\textbf{Iterationsfolge:}} Gegeben $A\in\C^{n\times n},x_0\in\C^n$, setze \[x_{k+1}=Ax_k\]

            \begin{theorem}\label{s4.12}
                Sei $A\in\R^{n\times n},x\in\R^n$ mit $A=A^t$ symmetrisch, mit einfachem betragsmäßig 
                größtem EW $\lambda_1$, d.h. 
                \[\vert \lambda_1\vert > \left\vert \lambda_2 \right\vert \geq \dots \geq \left\vert \lambda_n \right\vert\geq 0.\]
                Sei $x_0^tu_1\neq 0,u_1$ EV zu $\lambda_1$. Dann konvergiert 
                \begin{equation*}
                    y_k=\frac{x_k}{\left\Vert x_k \right\Vert}, x_{k+1}=Ax_k,k=0,1,\dots 
                \end{equation*}

                zu $u_1$. Die Konvergenz ist alternierend, falls $\lambda_1<0$.

                Außerdem gilt 
                \begin{equation}\label{g4.4}
                    y_k^t Ay_k\stackrel{k\to\infty}{\to}\lambda_1
                \end{equation}

            \end{theorem}

            \begin{proof}
                Sei $u_1,\dots,u_n$ eine ONB von $\R^n$ aus EV von $A$, mit $u_i$ EV von $\lambda_i$. 
                Sei

                \begin{equation}\label{g4.5}
                    x_0=\sum_{i=1}^n\alpha_i u_i
                \end{equation}
                \begin{equation*}
                    \implies x_k=A^kx_0\stackrel{\ref{g4.5}}{=}\sum_{i=1}^n\alpha_i A^k u_i = (\star)
                \end{equation*}

                \begin{align*}
                    (\star) &= \sum_{i=1}^n\alpha_i \lambda_i^k u_i\\
                    &= a_1\lambda_1^ku_1 + \sum_{i=2}^n\alpha_i \lambda_i^k u_i\\
                    &= a_1\lambda_1^k \underbrace{\left(u_1+\sum_{i=2}^n\frac{\alpha_i}{\alpha_1} \frac{\lambda_i^k}{\lambda_1^k} u_i\right)}_{=z_k\stackrel{k\to\infty}{\to} u_1}
                \end{align*}

                \begin{align*}
                    y_k&=\frac{x_k}{\left\Vert x_k \right\Vert}\\
                    &=\frac{\alpha_1\lambda_1^kz_k}{\left\Vert \alpha_1\lambda_1^kz_k \right\Vert}
                    &=\begin{cases}
                        =\pm \frac{z_k}{\left\Vert z_k \right\Vert_2}\to \pm u_1 & \lambda_1 >0 \\ 
                        =\pm(-1)^k\frac{z_k}{\left\Vert z_k \right\Vert_2}\to \pm u_1 & \lambda < 0
                    \end{cases}
                \end{align*}

                \underline{\textbf{z.z.:}} (\ref{g4.4})
                \begin{align*}
                    \left\vert y_k^tAy_k-\lambda_1 \right\vert &= \left\vert y_k^tAy_k-\lambda_1y_k^ty_k \right\vert\\ 
                    &= \left\vert \frac{x_k^t(A-\lambda_1I)x_k}{x_k^t x_k} \right\vert \\
                    &= \left\vert \frac{x_0^tA^{2k}(A-\lambda_1 I)x_0}{x_0^t A^{2k}x_0} \right\vert\\ 
                    &\stackrel{\ref{g4.5}}{=}\left\vert \frac{\sum_{i=1}^n \alpha_i^2\lambda_i^{2n}(\lambda_i-\lambda_1)}{\sum_{i=1}^n\alpha_i^2\lambda_i^{2k}} \right\vert = (\star \star)
                \end{align*}

                \begin{align*}
                    (\star\star) = \max_{i=2,\dots,n}\left\vert \lambda_i-\lambda_1 \right\vert \underbrace{\left\vert \frac{\sum_{i=2}^n\alpha_i^2\lambda_i^{2k}}{\alpha_1^2\lambda_1^{2k}} \right\vert}_{=\frac{1}{\alpha_1^2}\sum_{i=2}^n \alpha_i^2 \left(\frac{\lambda_i}{\lambda_1}\right)^{2k}} %Todo}
                \end{align*}

            \end{proof}

            \begin{remark}\label{4.13}
                \begin{enumerate}
                    \item Um Overflow/Underflow zu vermeiden, muss die Iteration angepasst werden:
                        \[y_0=\frac{x_0}{\left\Vert x_0 \right\Vert_2},y_1=Ay_k,y_k=\frac{x_k}{\left\Vert x_k \right\Vert_2}, k=0,1,\dots \]
                        Die Eigenschaften bleiben unverändert.
                    \item Der Satz \ref{s4.12} gilt aich für mehrfache EW $\lambda_1$.
                    \item In der Praxis ist $x_0^tu_1\neq 0$ wegen Rundungsfehlern nicht von Bedeutung, muss also nicht übeprüft werden.
                    \item Die Konvergenzgeschwindigkeit hängt von $\left\vert \frac{\lambda_2}{\lambda_1} \right\vert$ ab. Falls $ \vert \lambda_1 \vert \approx \vert \lambda_2 \vert $ konvergietr das Verfahren sehr langsam.    
                \end{enumerate}
            \end{remark}



            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 13 am 24.11.2022\xrfill[0.7ex]{1pt}
            
            \begin{algorithm}[H]
                     \caption{Vektoriteration}\label{a4.14}
                  \textbf{Input:} $A\in\R{n\times n,x_0\in\R^n}$ 
                  \textbf{Output:} $\lambda_1^{(1)}\approx \lambda_1,y_k\approx \pm u_1$
                  \begin{algorithmic}
                    \State $y_0=\frac{x_0}{\left\Vert x_0 \right\Vert_2}$
                    \For{$j=0,1,2,\dots$}
                        \State $x_{k+1}=Ay_k$
                        \State $y_{k+1}=\frac{x_k}{\left\Vert x_k \right\Vert_2}$
                        \State $\lambda_{1}^{(k+1)}=y_{k+1} ^t Ay_{k+1}$
                        \State Kovergenztest
                    \EndFor
                  \end{algorithmic}
               \end{algorithm}
                        
            \underline{\textbf{Frage:}} Wie können wir beliebige EW und EV berechnen?

            \underline{\textbf{Idee:}} Verwende Nährung $\tilde\lambda$ an $\lambda_i$, welche 
            näher an $\lambda_i$ als allen anderen EW ist.

            $\rightsquigarrow$ die Vektoriteration auf 
            \[\left(A-\tilde\lambda\right)^{-1}\]
            an (shift \& invert). $\implies \left(A-\tilde\lambda\right)^{-1}$ hat $(\lambda_k-\tilde\lambda)^{-1}, j=1,\dots n$
            als EW. $\implies $ gößter Eigenwert von $\left(A-\tilde\lambda\right)^{-1}$ ist $(\lambda_i-\tilde\lambda)^{-1}$. Daraus bekommen wir $\lambda_i$.

            Dieses Verfahren heißt \textbf{inverse Vektoriteration}.

            \underline{\textbf{Aber:}} Wir brauchen $\tilde\lambda$ und müssen in jeder Iteration ein lineares Gleichungsystem lösen.


        \section{Das QR-Verfahren}

            \underline{\textbf{Ziel:}} Algorithmus zum Berechnen aller EW einer Matrix $A\in\C^{n\times n}$.

            \underline{\textbf{Idee:}} Formuliere Iterationsvorschrift
            \begin{equation}\label{g4.6} % Alles in eins
            A_0=A
            \end{equation}
            Berechne $QR$ Zerlegung von $A_k-\mu_kI=Q_kR_k$ für $\mu_k\in \C$

            Setze $A_{k+1}=R_kQ_k+\mu_kI$.

            \begin{lemma}\label{l4.15}
                Seien $\mu_k,A_k,Q_k,R_k$ wie in (\ref{g4.6}). Dann gilt 
                \begin{enumerate}
                    \item $A_{k+1}=Q^*_kA_kQ_k$
                    \item $A_{k+1}=(Q_0Q_1\dots Q_k)^*A(Q_0Q_1\dots Q_k)$
                    \item \[\prod_{j=0}^k (A-\mu_k I) = \underbrace{(Q_0Q_1\dots Q_k)}_{\underline{Q_k}}\underbrace{(R_{k}R_{k-1}\dots R_0)}_{\underline{R_k}}\]
                \end{enumerate}
            \end{lemma}

            \begin{proof}
            Übung :)
            \end{proof}

            \begin{remark}\label{b4.16}
                Lemma \ref{l4.15} sagt, dass alle $A_k$ die gleichen EW haben. 
            \end{remark}

            Wie kommen wir auf (\ref{g4.6})? Betrachte $\mu_k=0$

            \underline{\textbf{Beobachtung 1}} 

            \begin{align*}
                \underbrace{A^{k+1}e_1}_{\text{Vektoriteration!}}&\stackrel{\text{Lemma }\ref{l4.15}}{=} \underline{Q_k}\underline{R_k}e_1\\
                &=\begin{bmatrix}
                    &&
                    q_1^{(k)} & \dots& q_n^{(k)}
                    &&
                \end{bmatrix}
                \begin{bmatrix}
                    r_{11}^{(k)} &  \dots r_{1n}^{(k)}\\
                    &\ddots  &\vdots \\
                    0 && r_{nn}^{(k)}
                \end{bmatrix}
                e_1\\
                &=r_{11}^{(k)}q_1^{(k)}
            \end{align*}

            $\implies q_1^{(k)}$ ist approximativer Eigenvekor zum betragsmäßig größten EW $\lambda_1$.

            $$\implies A_{k+1}e_1\stackrel{\text{Lemma }\ref{l4.15}}{=} \underline{Q_k^*}\underbrace{A\underbrace{\underline{Q_k}e_1}_{q_1^{(k)}}}_{\approx \lambda_1 q_1^{(k)}}$$

            \[\approx \lambda_1\underline{Q_k}^*q_1^{(k)}=\lambda_1e_1\]

            \[\implies A_{k+1}=\begin{bmatrix}
                \lambda_1 & \star & \dots & \star\\
                0& \star & \dots & \star\\
                \vdots & \vdots &&\vdots \\
                0 &  \star& \dots & \star
            \end{bmatrix}\]

            \underline{\textbf{Beobachtung 2}}

            \begin{align*}
                \left(q_n^{(k)}\right)^*&=e_n^t \underline{Q_k}^*\\
                &=\underbrace{e^t_n\underline{R_k}}_{r_nn^{(k)}e_n^t}A^{-(k+1)}=r_{nn}^{(k)}e_n^t A^{-(k+1)}\\
            \end{align*}

            $q_n^{(k)}$ ist ein appriximativer Eigenvektor zum betragsmäßig kleinsten EW $\lambda_n$ von $A$.

            \begin{align*}
                \implies e_n^t A_{k+1}&\stackrel{\text{Lemma }\ref{l4.15}}{=}\underbrace{e_n^t\underline{Q_k}^*}_{= \left(q_n^{(k)}\right)^*}A\underline{Q_k}\\
                &\approx \lambda_n  \left(q_n^{(k)}\right)^* \underline{Q_k}\\
                &=\lambda_n e_n^t\\
            \end{align*}

            \begin{equation*}
                A_{k+1}=
                    \begin{bmatrix}
                        \lambda_1 & \star & \dots & \star\\
                        0& \star & \dots & \star\\
                        \vdots & \vdots &&\vdots \\
                        0 &  \star& \dots & \star\\
                        0 & 0& \dots & \lambda_n
                    \end{bmatrix}
            \end{equation*}

            \begin{theorem}\label{s4.17}
                Sei $A\in\C^{n\times n}$ mit paarweise verschiedenen Eigenwerten $\left\vert \lambda_1 \right\vert>\left\vert \lambda_2 \right\vert>\dots >\left\vert \lambda_n \right\vert>0$.
                
                $\Lambda=\text{diag}(\lambda_1,\dots,\lambda_n)$ und $X=\begin{bmatrix}
                    &&\\
                    x_1 & \dots & x_n\\
                    &&\\
                \end{bmatrix}$

                die zugehörige EV Matrix $(A=X \Lambda X^{-1})$. Existiert die LR-Zerlegung $X^{-1}=LR$, dann sind die Matrizen
                $A_k$ aus (\ref{g4.6}) mit $\mu_k=0$ asymptoisch obere Dreiecksmatrizen. Außerdem konvergiert $\text{diag}(\text{diag}(A_k))$ mindestens linear gegen $\Lambda$

            \end{theorem}

            \underline{\textbf{Idee:}} Leite zwei erschiedene $QR$-Zerlegungen von $A^k$ mit verschiedenen Eigenschaften 
            her. 

            \underline{\textbf{$QR$-Zerlegung 1:}} Lemma \ref{l4.15}.3 $\implies A^k=\underline{Q_{k-1}}\underline{R_{k-1}}$.

            \underline{\textbf{$QR$-Zerlegung 2:}} 
            \begin{align*}
                A^k&=X\Lambda^k\underbrace{X^{-1}}_{LR}\\
                &=X\Lambda^k LR\\
                &=\underbrace{(X\Lambda^k L\Lambda^{-k})}_{X_k\text{ mit QR-Zerlegung } X_k=P_kU_k}(\Lambda^k R)\\
                &=\underbrace{P_k}_{\underline{P_k}}\underbrace{(U_k\Lambda^kR)}_{\underline{U_k}}
            \end{align*}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 14 am 29.11.2022\xrfill[0.7ex]{1pt}

            \begin{proof}

                \underline{\textbf{Bemerke:}} 
                \begin{align*}
                    \Lambda^k L \Lambda^{-k}&=\begin{bmatrix}
                        \lambda_1^k & & \\
                        & \ddots & \\
                        &&\lambda_n^k
                    \end{bmatrix}
                    \begin{bmatrix}
                            1 & & \\
                            \star& \ddots & \\
                            \star &\star &1
                    \end{bmatrix}
                    \begin{bmatrix}
                        \lambda_1^{-k} & & \\
                        & \ddots & \\
                        &&\lambda_n^{-k}
                    \end{bmatrix}\\
                    &\implies \left(\Lambda^k L \Lambda^{-k}\right)_{ij}=\begin{cases}
                        0 & j>1\\
                        1 & i=j\\
                        I_{ij}\left(\frac{\lambda_i}{\lambda_j}\right)^k & i>j
                    \end{cases}\\
                    &\implies X_k=X(I+E_k), \left\Vert E_k \right\Vert_2=O(q^k) 
                \end{align*}

                $q\in(0,1)$ hängt von $\frac{\lambda_i}{\lambda_j},i>j$ ab.

                Eindeutigkeit der QR-Zerlegung bis auf unitäre Skalierungen (Übung)

                \begin{align*}
                    \implies \underline{Q_{k-1}}=\underline{P_k}S_k^*, \underline{R_{k-1}}=\underline{U_k}S_k
                \end{align*}
                und $S_k$ unitäre Diagonalmatrix (d.h. die diagonaleinträge haben Betrag 1).
                \begin{align*}
                    \implies A_k &\stackrel{\ref{l4.15}.2}{=}\underline{Q_{k-1}}^* A\underline{Q_{k-1}}\\
                    &=S_k\underline{P_k}^* A \underline{P_k}S_k^* \\
                    &=S_k U_k \underbrace{U_k^{-1}\underline{P_k}^*}_{=X_k^{-1}}\underbrace{A \underline{P_k}}_{=A^{k+1}\underline{U_k}^{-1}=\underbrace{X\Lambda^{k+1}L\Lambda^{-(k+1)}}_{=X_{k+1}}\Lambda^{(k+1)}R\underbrace{U_k^{-1}}_{R^{-1}\Lambda^{-k}U_k^{-1}}}S_k^* = (\star)\\
                    &= S_k U_k \underbrace{X^{-1}_kX_{k+1}}_{=(I+E_k)^{-1}(I+E_k)=I+F_k, \left\Vert F_k \right\Vert_2=O(q^k)}\Lambda U_{k}^{-1} S_k^*\\
                    &= \underbrace{S_k U_k\Lambda U_k^{-1}S_k^*}_{\text{obere Dreiecksmatrix}}+\underbrace{S_k U_kF_k\Lambda U_k^{-1}S_k^*}_{(+)}
                \end{align*}

                Mit Nebenrechnung 
                \begin{align*}
                    \left\Vert U_k \right\Vert_2 = \left\Vert P_k^* X_k \right\Vert_2 = \left\Vert X_k \right\Vert_2
                \end{align*}

                \begin{align*}
                    \left\Vert (+) \right\Vert_2 &\leq \underbrace{\left\Vert S_k \right\Vert_2}_{=1} \left\Vert U_k \right\Vert_2 \left\Vert F_k \right\Vert_2 \left\Vert U_k^{-1} \right\Vert_2 \underbrace{\left\Vert S_k \right\Vert_2}_{=1} \underbrace{\left\Vert \Lambda \right\Vert_2}_{=\left\vert \lambda_1 \right\vert}\\
                    &\leq \underbrace{\cond_2(X_k)}_{\leq 2\cond_2 (X)}\left\vert \lambda_1 \right\vert \left\Vert F_k \right\Vert_2=O(q^k)
                \end{align*}

                $\implies A_k$ konvergiert gegen eine obere Dreiecksamtrix.

                \underline{\textbf{z.z.}} $\text{diag}(\text{diag}(A_k))\stackrel{k \to\infty}{\to}\Lambda$
                \begin{align*}
                    X_k\stackrel{k \to\infty}{\to}X & \implies P_k,U_k\stackrel{k \to\infty}{\to} I \\
                    &\implies \text{diag}(\text{diag}(A_k))=\text{diag}(\text{diag}(S_kU_k\Lambda U_k^{-1}S_k^*))\\
                    &=S_k\text{diag}(\text{diag}(U_k\Lambda U_k^{-1}))S_k^*\stackrel{k\to\infty}{\to}\Lambda
                \end{align*}

            \end{proof}

            \begin{remark}\label{b4.18}
                \begin{enumerate}
                    \item Der Fehler im $k$-ten Schritt hängt von $\left|\frac{\lambda_i}{\lambda_k}\right|^k,i>j$ ab.
                        D.h. falls $\left\vert \lambda_i \right\vert\approx \left\vert \lambda_j \right\vert$,
                        dann konvergiert das Verfahren sehr langsam.
                    \item Wir müssen pro Iteration eine QR-Zerlegung mit Aufwand $O(n^3)$ berechnen.
                \end{enumerate}
            \end{remark}

        \section{Implementierung des QR-Verfahrens}

            \underline{\textbf{Problem 1:}}  Der Aufwand pro Iteration ist zu hoch.

            \underline{\textbf{Idee:}} Transformiere $A\in\C^{n\times n}$ zuerst mittels Ähnlichkeitstransformationen auf
            Hessenberg-Form.
            \begin{align*}
                \begin{bmatrix}
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star 
                \end{bmatrix}
                \stackrel{P_1\cdot}{\rightsquigarrow} \begin{bmatrix}
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star \\
                    0 & \star & \star & \star \\
                    0 & \star & \star & \star 
                \end{bmatrix}
                \stackrel{\cdot P_1}{\rightsquigarrow} \begin{bmatrix}
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star \\
                    0 & \star & \star & \star \\
                    0 & \star & \star & \star 
                \end{bmatrix}
                \stackrel{P_2\cdot}{\rightsquigarrow}\begin{bmatrix}
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star \\
                    0 & \star & \star & \star \\
                    0 & 0 & \star & \star 
                \end{bmatrix}\stackrel{P_2\cdot }{\rightsquigarrow} \dots
            \end{align*}

            $\implies $ mit geeigneten orthogonalen Ähnlichkeitstransformationen  $P_1,\dots,P_{n-1}$ hat $A=(P_{n-2}\cdot\dots\cdot P_1)A (P_{n-2}\cdot\dots\cdot P_1)^*$
            hat obere Hessenberg-Form.

            Gesamtaufwand: 
            \begin{align*}
                \sum_{k=2}^{n-1} 2((k+1)k)+nk\leq 3n^3
            \end{align*}

            \underline{\textbf{Vorteil:}} Die $QR$-Zerlegung einer Hessenberg-Matrix kann in $(2n^2)$ Multiplikationen 
            statt $O(n^3)$ berechnet werden.
            
            $\rightsquigarrow \underbrace{(G_{n-1,n}\dots G_{2,3}G_{1,2})}_{=Q_0^*}(A_0-\mu_0 I)=R_0$

            \underline{\textbf{Aber:}} hat $R_0Q_0+\mu_k I$ wieder Hessenberg-Form?

            \begin{align*}
                \begin{bmatrix}
                    \star & \star & \star & \star \\
                     & \star & \star & \star \\
                    & & \star & \star \\
                    & & & \star \\
                \end{bmatrix}\stackrel{\cdot G_{1,2}^*}{\to} 
                \begin{bmatrix}
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star \\
                    & & \star & \star \\
                    & & & \star \\
                \end{bmatrix}\stackrel{\cdot G_{1,2}^*}{\to} 
                \begin{bmatrix}
                    \star & \star & \star & \star \\
                    \star & \star & \star & \star \\
                    & \star & \star & \star \\
                    & & & \star \\
                \end{bmatrix}
            \end{align*}

            $\implies$ Ja! Gesamtaufwand pro Iteration: $4n^2$.

            \underline{\textbf{Problem 2}} Die Konvergenz des $QR$-Verfahrens ohne Shift, d.h. $\mu_k=0$,
            kann sehr langsam sein.

            \underline{\textbf{Frage:}} Wie müssen wir $\mu_k$ in (\ref{g4.6}) wählen?

            \begin{corollary}\label{k4.19}
                Unter den gleichen Voraussetzungen wie in \ref{s4.17}, aber mit $\mu_k\neq 0$, hängt 
                der Fehler des $QR$-Verfahrens im $k$ten Schritt von
                \begin{align*}
                    \left\vert \frac{\lambda_i-\mu_0}{\lambda_j-\mu_0} \right\vert\cdot \dots \cdot \left\vert \frac{\lambda_i-\mu_{k-1}}{\lambda_j-\mu_{k-1}} \right\vert,i>j
                \end{align*} 
                ab.
            \end{corollary}

            \begin{proof}
                Analog zu Satz \ref{s4.17}.
            \end{proof}

            \underline{\textbf{Idee 1:}} Wähle $\mu_k=a_{nn}^{(k)}$

            \underline{\textbf{Idee 2:}} Wähle $\mu_k$ als EW von $\begin{bmatrix}
                a_{n-1n-1}^{(k)} & a_{n-1n}^{(k)}\\
                a_{nn-1}^{(k)} & a_{nn}^{(k)}
            \end{bmatrix}$
             der näher an $a_{nn}^{(k)}$ liegt.

            In beiden Fällen kann man zeigen, dass nach wenigen Schritten

            \begin{align*} % TODO FIX
                A_k\approx \begin{bmatrix}
                    \star &\dots&\star &\star\\
                    \star&\ddots&&\vdots\\
                    &\ddots&\star&\vdots\\
                    &&\star&\star\\
                    0&\dots&0&\lambda_n
                \end{bmatrix}
            \end{align*}

            Diesen Verfahren heißt Deflation.y

            In der Praxis benötigt das Verfahren nur $O(n)$ Iterationen.
            $\implies$ Gesamtaufwand $P(n^3)$.

            \underline{\textbf{Problem 3:}} Wie berechnen wir Eigenvektoren?

            $A=Q^*RQ$ betrachte $R$ (siehe Notizen)

            $A=PA_0P^*$ Die obere Hessenbergmarix $A_0=P^* A P$ hat die gleichen EW wie $A$.

            $\rightsquigarrow$ Berechne die EV von $A_0$ mittels inverser Vektoriteration,
            Sift-Parameter Eigenwetrnährung aus dem QR-Verfahren.

            Benutze $QR$-zerlegung um LGS zu lösen. $\rightsquigarrow O(n^2)$ prob EV, da die inverse Vektoriteration meistens in einem Schritt konvergiert.

            $\implies $ Zusatzaufwand $O(n^3)$ um EV zu berechnen.

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 15 am 01.12.2022\xrfill[0.7ex]{1pt}
            
        \section{Singulärwertzerlegung}

            \begin{definition}\label{d4.20}
                Sei $A\in\C^{m\times n},m\geq n$. Eine \textbf{Singulärwertzerlegung von $A$} (SVD) ist eine Faktorisierung
                \[A=U\Sigma V^*\]
                mit unitären Matrizen $U=\begin{bmatrix}
                    u_1&\dots &v_m
                \end{bmatrix}\in\C^{m\times m},V=\begin{bmatrix}
                    v_1 & \dots & v_n\in\C^{n\times n}
                \end{bmatrix}$
                mit \[\Sigma = \begin{bmatrix}
                    \hat{\sigma}=\text{diag}(\sigma_1,\dots,\sigma_n)\in\R^{n\times n}\\
                    0
                \end{bmatrix}\]
                mit $\sigma_i\geq 0$. Die $u_i$ heißen linke Singulärvektoren, die $v_i$ rechte Singulärvektoren und die $\sigma_i$ Singulärwerte.
            \end{definition}

            \begin{theorem}\label{s4.21}
                Für jede Matrix $A\in\C^{m\times n},m\geq n$ existiert eine SVD.
            \end{theorem}

            \begin{proof}
                \underline{\textbf{Idee:}} Zeige, dass es unitäre Matrizen $U,V$ gibt, mit:
                
                \begin{equation}\label{g4.7}
                    U^* A V = \begin{bmatrix}
                        \sigma & &0&\\
                        0 & & B &
                    \end{bmatrix}, 0\leq \sigma\in \R
                \end{equation}
                Der Rest folgt dann durch Induktion.

                Sei \[\sigma = \left\Vert A \right\Vert_2=\max_{x\in\C^n, \left\Vert x \right\Vert_2=1}\left\Vert Ax \right\Vert_2\geq 0\]
            
                Das Maximum wird angenommen $\implies\exists x\in\C^n, \left\Vert x \right\Vert_2=1$ mit 
                \[\sigma^2 = \left\Vert Ax \right\Vert_2^2=x^*A^*Ax = (\star)\]

                Setze 
                \begin{align*}
                    u=\frac{1}{\sigma}Ax \stackrel{(\star)}{\implies} \left\Vert u \right\Vert_2=1\\
                    v=x \implies \left\Vert v \right\Vert_2=1\\
                    \implies Av=\sigma u = (\star \star)
                \end{align*}

                Ergänze zur ONB:
                \begin{align*}
                    U=\begin{bmatrix}
                        &&\\
                        u_1 & \dots & u_m\\
                        &&
                    \end{bmatrix}\\
                    V=\begin{bmatrix}
                        &&\\
                        v_1 & \dots & v_m\\
                        &&
                    \end{bmatrix}\\
                    \implies U^* A V = \begin{bmatrix}
                        &u^*&\\
                        &u_2^*& \\
                        &\vdots& \\
                        &u_m^*&
                    \end{bmatrix}A\begin{bmatrix}
                        &&\\
                        v & v_2 & \dots & v_n\\
                        &&
                    \end{bmatrix}\\
                    =\begin{bmatrix}
                        \sigma & & w^* & \\
                        0 & & &\\
                        \vdots & & B &\\
                        0 & & & 
                    \end{bmatrix}=A_1,w\in \C^{n-1}
                \end{align*}

                \begin{align*}
                    \sigma^2&=\left\Vert A \right\Vert_2^2 \\
                    &\stackrel{\text{Invarianz unter oth. Proj.}}{=} \left\Vert A_1 \right\Vert_2^2\\
                    &=\sup_{0\neq x\in\C^n}\frac{\left\Vert A_1 x \right\Vert_2^2}{\left\Vert x \right\Vert_2^2}\\
                    \stackrel{x=\begin{pmatrix}\sigma\\w\end{pmatrix}}{\geq }\frac{1}{\sigma^2+\left\Vert w \right\Vert_2^2}\left\Vert\begin{bmatrix}
                        \sigma & & w^* & \\
                        0 & & &\\
                        \vdots & & B &\\
                        0 & & & 
                    \end{bmatrix}\begin{pmatrix}\sigma\\w\end{pmatrix}  \right\Vert\\
                    =\frac{1}{\sigma^2+\left\Vert w \right\Vert_2^2}\left\Vert \begin{pmatrix}\sigma^2 + \left\Vert w \right\Vert_2^2\\Bw\end{pmatrix} \right\Vert\\
                    \geq \sigma^2 + \left\Vert w \right\Vert_2^2 \implies w=0\implies (\ref{g4.7})
                \end{align*}
            \end{proof}

            \begin{corollary}\label{k4.22}
                Sei $U^+ A V= \Sigma$ eine SVD von $A\in\C^{m\times n}$. Dann gilt:
                \begin{enumerate}
                    \item $Av_i=\sigma_iu_i$ und  $A^*u_i = \sigma_i v_i$
                    \item Falls $\sigma_1\geq \dots \geq \sigma_r >\sigma_{r+1}=\dots = \sigma_n=0$ gilt
                        \begin{enumerate}
                            \item Rang$(A)=r$
                            \item Kern$(A)=\text{span}(v_{r+1},\dots, v_n)$
                            \item Bild$(A)=\text{span}(u_1,\dots,u_r)$
                        \end{enumerate}
                    \item $\left\Vert A \right\Vert_2 = \sigma_1$
                    \item $\left\Vert A \right\Vert_F=\sqrt{\sum_{i=1}^m\sum_{j=1}^n\left\vert a_{ij}^2 \right\vert} = \sqrt{\sum{i=1}^n\sigma_i^2}$
                    \item $\cond_2(A)=\frac{\sigma_1}{\sigma_n}$
                    \item Für $\lambda_i=\sigma_i^2$ gilt $\lambda_i$ sind EW von $AA^*$ und $A^*A$ zu den Eigenvektoren $u_1,\dots, u_n$ und $v_1,\dots, v_n$
                \end{enumerate}
            \end{corollary}

            \begin{proof}
                Übungen.
            \end{proof}

            Wozu brauchen wir eine SVD?

            z.B. als theoretisches Hilfmittel:
            \begin{lemma}\label{l4.23}
                Sei $A\in \C^{m\times n},m\geq n$ mit $\rang(A)=r$. Sei $U\Sigma V^*$ eine SVD von $A$.
                Dann ist 
                \begin{equation*}
                    A^\dagger = V \Sigma^\dagger U ^*, \Sigma^\dagger=\begin{bmatrix}
                        \sigma_1^{-1} & & &&\\
                        & \ddots & & &&\\
                        &&\sigma_r^{-1} & &&\\
                        &&&0&&\\
                        &&&&\ddots&\\
                        &&&&&0
                    \end{bmatrix}
                \end{equation*}
            \end{lemma}

            \begin{proof}
                Übungen.
            \end{proof}

            \begin{remark}\label{b4.24}
                Die charakterisierung von Lemma \ref{l4.23} bietet eine einfache Möglichkeit Satz \ref{s2.23} zu beweisen.
            \end{remark}

            z.B. als Hilfsmittel zur Datenkompression:

            \begin{theorem}[Eckhart-Young]\label{s4.25}
                Sei $A\in\C^{m\times n},m\geq n,$ und $k<\rang(A)=r$. Sei $A=U\Sigma V^*$
                eine SVD und 
                \begin{equation*}
                    A_k=\sum_{j=1}^k \sigma_k u_j v_j^*
                \end{equation*}
                Dann ist 
                \begin{equation*}
                    \min_{\rang(B)=k} \left\Vert A-B \right\Vert_2 \stackrel{1.}{=} \left\Vert A-A_k \right\Vert_2 \stackrel{2.}{=} \sigma_{k+1}
                \end{equation*}
            \end{theorem}

            \begin{proof}
                $\rang(A_k)=k$ ist klar.
                \begin{align*}
                    \left\Vert A-A_k \right\Vert_2=\left\Vert U\left(\begin{pmatrix}
                        \sigma_1 & &\\
                        &\ddots & \\
                        &&\sigma_n\\
                        &0&
                    \end{pmatrix}-\begin{pmatrix}
                        \sigma_1 & &&&\\
                        &\ddots & &&\\
                        &&\sigma_k&&\\
                        &&&0&\\
                        &&&&\ddots \\
                        &&0&&
                    \end{pmatrix}\right) \right\Vert_2^2=\sigma_{k+1}\implies 2.
                \end{align*}

                \underline{\textbf{z.z.}} 1.:

                Sei 

            \begin{align*}
                &B \in\C^{m\times n},\rang(B)=k\\
                &\implies \dim(\ker(B))=n-k\\
                &\implies \underbrace{\ker(B)\cap \text{span}(v_1,\dots v_{k+1})}_{Z}\neq \{0\}
            \end{align*}

            Sei $z\in Z,z\neq 0,\left\Vert z \right\Vert_2=1$

            \begin{align*}
                &\implies Bz &=0\\
                &\implies Az&=U\Sigma V^* z\\
                &&=\begin{bmatrix}
                    &&\\
                    u_1 & \dots & u_m\\
                    &&
                \end{bmatrix}
                \begin{bmatrix}
                    \sigma_1 && \\
                    & \ddots &\\
                    &&\sigma_n \\
                    &0&
                \end{bmatrix}
                \begin{bmatrix}
                    v_1\\
                    \vdots \\
                    v_n
                \end{bmatrix}z \\
                &&= \sum_{j=1}^{k+1}\sigma_i(v_i^*z)u_i
            \end{align*}

            \begin{align*}
                \left\Vert A-B \right\Vert_2^2 &= \max_{\left\Vert x \right\Vert_2=1} \left\Vert (A-B)x \right\Vert_2^2 \\
                &\stackrel{x=z}{\geq} \left\Vert (A-B)z \right\Vert_2^2 \\
                &= \left\Vert Az \right\Vert_2^2\\
                &=z^* A^* A z\\
                &=\sum_{j=1}^{k+1}\sigma_j^2 (V_j^*z)^2 \\
                \geq \sigma_{k+1}^2 \underbrace{\sum_{j=1}^{k+1} (V_j^*z)^2}_{\stackrel{\text{Satz} \ref{s1.16}}{=}\left\Vert z \right\Vert_2^2=1}\geq \sigma_{k+1}^2
            \end{align*}

            \end{proof}

            \begin{remark}\label{4.26}
                \begin{enumerate}
                    \item Satz \ref*{s4.25} ist eine Bestapüroximationsaussage zur Approximation von belibiegen Matrizen durch Matrizen von niedriegerem Rang.
                    \item Die Aussage gilt ähnlich auch für die Frobeniusnorm.
                \end{enumerate}
            \end{remark}

            Wie berechnen wir eine SVD?

            \underline{\textbf{Möglichkeit 1.}}

            Berechne EW $\lambda_i=\sigma_i^2$ und zugeh EV von $A^*A$ oder $AA^*$. Die Matrizen sind hermitisch, d.h.
            die Berechnung der EW \& EV-Berecnung gutgestellt/ gutkonditioniert \textbf{falls} $AA^*, A^*A$ nicht 
            berechnet werden müssen.

            \begin{example}\label{b4.27}
                Rundungsgenauigkeit $4$ Ziffern.
                \begin{align*}
                    A=A^t = \begin{bmatrix}
                        1.005 & 0.995 \\
                        0.995 & 1.005
                    \end{bmatrix} \implies \sigma_1=2,\sigma_2=0.01
                \end{align*}
                \underline{\textbf{Aber:}} 
                \begin{align*}
                    \text{round}(A^* A)=\begin{bmatrix}
                        2&2\\
                        2&2
                    \end{bmatrix}\\
                    \implies \lambda_1=4,\lambda_2=0\\
                     \implies \sigma_1 =\sqrt(\lambda_1),\sigma_2=\sqrt{\lambda_2}
                \end{align*}
            \end{example}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 16 am 06.12.2022\xrfill[0.7ex]{1pt}
            
            \underline{\textbf{Möglichkeit 2:}} SVD-Algorithmus

        \section{SVD-Algorithmus}

            \underline{\textbf{Idee:}} Führe QR-Algorithmus auf $A^*A$ durch, ohne $A^*A$ zu berechnen.

            \underline{\textbf{Beobachtung:}} Sei $\C^{m\times n} \ni A=U\Sigma V^*$ eine SVD. Seien $\tilde{U}\in\C^{m\times m},\tilde{V}\in\C^{n\times n}$ unitäre Matrizen.

            \[\implies \tilde{A}=\tilde{U}A\tilde{V}^*=\underbrace{\tilde{U}U}_{\hat{U}}\Sigma \underbrace{V^*\tilde{V}^*}_{V^*}\]
            ist eine SVD von $\tilde{A}$ und $\hat{U},\hat{V}$ sind unitär.

            \underline{\textbf{Idee:}} Analog zum $QR$-Verfahren:

            \underline{\textbf{Schritt 1:}}  Transformieren $A\in\C^{m\times n}$ mittels unitärer 
            Ähnlichkeitstransformationen  auf obere Dreiecksgestalt $B\in\C^{m\times n}$

            \begin{equation*}
                B=\begin{bmatrix}
                    \text{diagonal Matirix mit einer Diagonalen über ihr besetzt}\\ % Ändern
                    0
                \end{bmatrix}
            \end{equation*}

            $\implies A,B$ haben die gleichen Singulärwerte

            $\implies B^*B$ ist hermitisch und tridiagonal, hat also Hessenbergform.

            \underline{\textbf{Schritt 2:}} Wende modifizierten QR-Algorithmus an (auf $B^*B$, ohne 
            dieses Produkt zu berechnen).

            \underline{\textbf{Zu Stufe 1:}}

            \begin{lemma}\label{l4.28}
                Zur Matrix $A\in\C^{m\times n},m\geq n$, existieren unitäre Matrizen $P\in\C^{m\times m},Q\in\C^{n\times n}$ s.d. 
                \[B=PAQ=\begin{bmatrix}
                    C\\0
                \end{bmatrix}\]
                mit $C\in\C^{n\times n}$ eine Bidiagonalform.
            \end{lemma}

            \begin{proof}
                z.B. mit Householder-Transformationen:

                \begin{align*}
                    \begin{bmatrix}
                        \star & \dots & \star \\
                        \vdots & \ddots & \vdots \\
                        \star & \dots  & \star
                    \end{bmatrix}\stackrel{P_1\cdot}{=}\begin{bmatrix}
                        \star & \star&\dots & \star \\
                        0& \star&\dots & \star \\
                        0&\star & \dots & \star \\
                        0&\vdots & \ddots & \vdots \\
                        0&\star & \dots  & \star
                    \end{bmatrix}
                    \stackrel{\cdot Q_1}{=}\begin{bmatrix}
                        \star & \star & 0 &\dots & 0\\
                        0 & \star & \star &\dots & \star \\
                        \vdots & \vdots &\ddots &\vdots\\
                        0&\star &\dots & \dots &\star
                    \end{bmatrix}=\dots = B=PAQ
                \end{align*}
            \end{proof}

            \underline{\textbf{Zu Stufe 2.}} Bemerke $A^* A, B^*B,C^*C$ haben die gleichen EW.
            D.h. $A,B,C$ haben die gleichen Singulärwerte. 
            
            $C$ ist am ``einfachsten'' $\implies$ Modifiziere $QR$-Algorithmus für $C^*C$

            \underline{\textbf{Beobachte:}}

            \begin{equation*}
                C^*C=\begin{bmatrix}
                    c_{11} & & & \\
                    c_{12} & \ddots & & \\
                    &  \ddots & \ddots & \\
                    & & c_{n-1,n} & c_{nn}
                \end{bmatrix}
                \begin{bmatrix}
                    c_{11} & c_{21} & & \\
                     & \ddots &\ddots & \\
                    &  & \ddots & c_{n-1,n} \\
                    & &  & c_{nn}
                \end{bmatrix}=\begin{bmatrix}
                    \vert c_{11}\vert^2 % TODO
                \end{bmatrix}
            \end{equation*}

            \begin{equation*}
                G_{12}C^* C = \begin{bmatrix}
                    \star & \star & \star & &\\
                    0 & \ddots & \ddots  & & \\
                    &&\star %TODO
                \end{bmatrix}
            \end{equation*}

            \begin{equation*}
                \underbrace{\tilde{Q}G_{12}}_{Q^*}C^*C=R=\begin{bmatrix}
                    \star &  \dots &  \star\\
                    &\ddots &\vdots \\
                    &&\star
                \end{bmatrix}
            \end{equation*}

            für passendes $\tilde{Q}$.

            \begin{equation}\label{g4.9} % Was ist 4.8???
                RQ=\underbrace{Q^* C^* C} Q
            \end{equation}

            Ausserdem: $G_{12}$ hängt nur von $\begin{bmatrix}
                |c_{11}|^2\\
                c_{11}\overline{c_{12}}
            \end{bmatrix}$ ab:

            \begin{equation*}
                \begin{bmatrix}
                    \star \\
                    0
                \end{bmatrix}=G_{12}\begin{bmatrix}
                    \star \\ \star
                \end{bmatrix}
            \end{equation*}

            d.h. $G_{12}$ ist unabhängig von der Skalierung dieses Vektors. 

            $\implies G_{12}$ hängt nur von $\begin{bmatrix}
                \bar{c_{11}}\\
                \overline{c_{12}}
            \end{bmatrix}$
            ab.

            \underline{\textbf{Frage:}} Wie berechnen wir $\tilde{Q}$?

            \begin{theorem}[Implizietes $Q$ Theorem]\label{s4.29}
                Sei $\in\C^{n\times n}$, $Q\in\C^{n\times m}$ mit orthonormalen Spalten so, dass $H=Q^*AQ$
                obere Hessenberg-Form hat. Falls die untere Nebendiagonaleinträge alle nicht Null sind, dann sind diese Matrizen $Q$ und $H$
                (bis auf Skalierung der Spalten von $Q$ mit $\zeta_i\in\C,|\zeta_i|=1$) eindeutig durch die erste oder die letzte Spalte von $Q$ bestimmt.
            \end{theorem}

            \begin{proof}
                Ähnlich zur $QR$-zerlegung, siehe z.B. Golub \& van Loan.
            \end{proof}

            \underline{\textbf{Das bedeutet:}} Die erste Spalte von $Q$ hängt nur von $G_{12}$ ab. D.h. de erste Spalte von $Q$ hängt nur von $C$ ab.

            \underline{\textbf{Beobachte: }} $C$ ist eine obere Bidiagonalmatrix. Wir wenden dann $G_{12}$ von rechts an und eleminieren den ersten Eintrag der oberen Nebendiagonale (bekommen aber ggf. einen Eintrag in der ersten Spalte unter dem ersten Eintrag).
            Mit einer weiteren Givenrotation $U_2^*$ können wir dann diesen Eintrag elemenieren, bekommen aber einen neuen Eintrag usw. Am Schluss sollten wir dann eine Bidiagonalform $\tilde{C}$ haben.

            Im englischen sagt man ``chasing down the diagonal''.

            \begin{equation*}
                \tilde{C}=\underbrace{(U_n^*\cdot\dots\cdot U_2^*)}_{\tilde{U}}C\underbrace{(G_{12}^* V_3\cdot\dots\cdot V_n)}_{\tilde{V}}
            \end{equation*}

            ist bidiagonal und $\tilde{U},\tilde{V}$ unitär.

            $\implies \tilde{C}^*\tilde{C}=\tilde{V}^*C^*\tilde{U}^*\tilde{U}C\tilde{V}=\tilde{V}^*C^* C\tilde{V}$
            ist tridiagonal, d.h. hat auch obere Hessenberg-Form. Die erste Spalte von $\tilde{V}$ ist durch $G_{12}$, also $C$, bestimmt.

            $\implies RQ\stackrel{(\ref{g4.9})}{=}Q^*C^*CQ\stackrel{\text{Satz }\ref{s4.29}}{=}\tilde{V}^*C^*C\tilde{V}\tilde{C}^*\tilde{C}$ unter der Annahme, dass die untere Nebendiagonale von $C^*C$ tatsächlich nur nicht-Null-Einträge hat
            
            und  $Q=\tilde{V}\text{diag}(\zeta_1,\dots\zeta_n),|\zeta_i|=1$.

            \begin{algorithm}[H]
                \caption{SVD-Algorithmus}\label{a4.30}
                  \textbf{Input:} $A\in\C^{m\times n}$\\
                  \textbf{Output:} Singulärwerte von $A$
                  \begin{algorithmic}
                     \State \underline{\textbf{Stufe 1:}} Transformiere $A$ wie im Beweis von Satz \ref{s4.28} auf Bidiagonalgestalt
                    \[PAQ=B=\begin{bmatrix}
                        C\\0
                    \end{bmatrix}\text{ C bidiagonal}\]
                    \State \underline{\textbf{Stufe 2:}} $C_0=C$
                    \For {$k=0,\dots$}
                    \State  ``chasing down the diagonal'': Berechne \[C_{k+1}(U_n^*\cdot\dots \cdot U_2^*)C(G_{12}V_3\cdot\dots\cdot V_n)\]
                    \State Konvergenzkriterium
                    \EndFor 
                  \end{algorithmic}
               \end{algorithm}
            
            \begin{remark}\label{b.4.31}
                \begin{enumerate}
                    \item Konvergenzkriterien, Shifts, etc. gilt alles analog zum $QR$-Algorithmus.
                    \item Aufwand (für $m=n$, pro Iteration): 
                    
                        Stufe 1: $O(n^3)$

                        Stufe 2: $O(n)$ 
                \end{enumerate}
            \end{remark}

            

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 17 am 08.12.2022\xrfill[0.7ex]{1pt}
            
        \section{Große, dünnbesetzte lineare Eigenwertprobleme}

            \underline{\textbf{Projektionsmethoden!}} 

            \underline{\textbf{Erinnerung:}} Projektionsmethode zum Lösen von $Ax=b$, $A\in\R^{n\times n},x,b\in\R^n$:

            \begin{equation}\label{g4.10}% einen zu hoch
                \text{Finde }\tilde{x}\in K\text{ so, dass }b-Ax\perp K,
            \end{equation}
            wobei $K\subset \R^n$ ein endlichdimensionaler Unterraum sei.

            \underline{\textbf{Idee:}}

            Setzte $b=\lambda x$, d.h. für $A\in\C^{n\times n}$:

            Finde $(\tilde{\lambda},\tilde{u})\in \C\times \C^n$ so, dass $A\tilde{u}-\tilde{\lambda} \tilde{u}\perp K$,
            wobei $K\in\C^n$ ein endlichdimensionaler Unterraum sei.

            Sei $V=[v_1,\dots,v_m]$, $m<n$, eine ONB von K. 

            $\implies \tilde{u}\in K \iff \tilde{u}=Vy,y\in\C^m$

            Außerdem: 

            \begin{align}\label{g4.11}%  Nummerierung anpasse
                (\ref{g4.10}) &\iff AVy-\tilde{\lambda}Vy\perp K\notag\\
                &\iff \langle AVy-\tilde{\lambda}Vy,v_i \rangle_2=0,i=1,\dots,m\notag\\
                &\iff V^* A V^*y-\tilde{\lambda}\underbrace{V^*V}_{=I}y = 0\notag\\
                &\iff V^*A Vy=\tilde{\lambda}y 
            \end{align}

            Das ist ein EW-Problem der Größe $m\times m$ und kannn für $m$ nicht zu groß z.B. 
            mit dem QR-Verfahren gelöst werden.

            \begin{algorithm}[H]
                     \caption{Rayleigh-Ritz-Verfahren}\label{a4.32}
                  \textbf{Input:} $A\in\C^{n\times n}$, Unterraum $K\in\C^n$, $m$-dimensional\\
                  \textbf{Output:} $m$ approximierte Eigenpaare $(\tilde{\lambda_i},\tilde{u_i})_i,i=1,\dots,m$
                  \begin{algorithmic}
                    \State Berechne OBN von $K \rightsquigarrow V$
                    \State Berechne Matrix $V^*AV$
                    \State Berechne EP $(\tilde{\lambda_i},\tilde{y_i})$ $i=1,\dots,m$ von $V^*AV$
                    \State Berechne $(\tilde{\lambda_i},\tilde{u_i})=\tilde{\lambda_i},\tilde{Vy_i}, i=1,\dots,m$
                  \end{algorithmic}
               \end{algorithm}
            
            \underline{\textbf{Frage:}} Wie genau sind die Approximationen der EP?

            \begin{lemma}\label{l4.33}
                Sei $K$ invariant unter A, d.h. $AK\subset K$. Dann sind die mit dem Rayleigh-Ritz-Verfahren berechneten EP exakt.
            \end{lemma}

            \begin{proof}
                \begin{align*}
                    \tilde{\lambda}\tilde{u}=\tilde{\lambda} Vy=V\tilde{\lambda}y\stackrel{\ref{g4.11}}{=}VV^*\underbrace{A\underbrace{Vy}_{\tilde{u}}}_{A\tilde{u}\in K}
                \end{align*}
                \begin{align*}
                    A\tilde{u}\in K&\implies A\tilde{u}&=\sum_{i=1}^m\alpha_iv_i,\alpha_i\in\C\\
                    &\implies VV^*A\tilde{u}&=VV^*\sum_{i=1}^m\alpha_iv_i \\
                    &&=\sum_{i=1}^m\alpha_i\underbrace{V\underbrace{V^*v_i}_{e_i}}_{v_i}\\
                    &&=A\tilde{u}
                \end{align*}
            \end{proof}

            \begin{remark}\label{b4.34}
                Es können auch allgemeinere Projektionsmethoden

                Fine $(\tilde{\lambda},\tilde{u})\in\C\times K$ so, dass 
                \begin{equation*}
                    A\tilde{u}-\tilde{\lambda}\tilde{u}\perp L
                \end{equation*}
                für endlichdimensionale Unterräume $K,L$ verwendet werden.

                Solche Methoden können relativ allgemein analysiert werden, bringen aber bis auf gewisse Spezialfälle wenig 
                konkrete Aussagen.
            \end{remark}

            In der Praxis werden eigentlich fast immer Krylovraumverfahren verwendet.

            \underline{\textbf{Idee:}} Das Arnoldi-Verfahren erzeugt iterativ eine ONB $v_1,\dots,v_m$ von 
            $K_(A,v)$, $m=1,\dots,$. Während der Iteration wid 
            \begin{equation*}
                V_m^*AV_m=H_{m,m}, V=[v_1,\dots,v_m] \text{ bexrechnet.}
            \end{equation*}

            $H_{m,m}$ hat obere Hessenberg-Form.

            \begin{algorithm}[H]
                \caption{Arnoldi-Verfahren für lineare Eigenwertprobleme}\label{a4.35} 
                \textbf{Input:} $A\in\C^{n\times n},0\neq v\in\C^n,m\in N$\\
                \textbf{Output:} Approximative Eigenpaare $(\tilde{\lambda_i},\tilde{u_i})_i,,i=1,\dots,m$
                \begin{algorithmic}
                    \State $v_1=\frac{v}{\left\Vert v \right\Vert_2}$
                    \For {$i=1,2,\dots,m$}
                        \State Bestimme $V_j,H_{j,j}$ mittels Arnoldi-Verfahren
                        \State (Stop bei Abbruch)
                        \State Löse $H_{j,j}y_j=\tilde{\lambda}y_j$ mittels $QR$-Algoritmus
                        \State $\rightsquigarrow$ EP $(\tilde{\lambda_i},\tilde{y_{ji}}),i=1,\dots,j$
                        \State EP von $A$ sind approximativ $(\tilde{\lambda_i},V\tilde{y_i}),i=1,\dots,j$
                        \State Konvergenztest
                        \EndFor 
                \end{algorithmic}
                \end{algorithm}
            
            \begin{remark}\label{b4.36}
                \begin{enumerate}
                    \item $H_{j,j}=V^*_jAV_j$ gemäß Lemma \ref{l3.14}
                    \item $H_{j,j}$ hat obere Hessenberg-Form, wir können also direkt mit der zweite 
                        Stufe des $QR$-Algorithmus anfangen.
                    \item Die Größe des Eigenwertproblems wächst mit jedem Iterationsschritt, ebenso der Speicherbedarf. 
                    \item Lemma \ref{l4.33} gilt weiterhin. Das bedeutet: Falls das Arnoldi-Verfahren abbricht, dann ist der Krylovraum invariant unter $A$ (Lemma \ref{l3.15}) und die 
                        bisher berechnten EP sind exakt.
                \end{enumerate}
            \end{remark}

            \underline{\textbf{Frage:}} Wie sieht ein geeigneter Konvergenztest aus?

            \begin{lemma}\label{l4.37}
                Sei $(\tilde{\lambda_i}^{(m)},y_i^{(m)})$ ein EP von $H_{m,m}$ und $\tilde{u_i}^{(m)}=Vy_i^{(m)}$.
                Dann gilt:
                \begin{equation*}
                    (A-\tilde{\lambda_i}^{(m)}I)u_i^{(m)}=h_{m+1,m}e_m^* y_i^{(m)}v_{m+1} 
                \end{equation*}

                und 

                \begin{equation*}
                    \left\Vert  (A-\tilde{\lambda_i}^{(m)}I)u_i^{(m)}\right\Vert_2=h_{m+1,m}\left\vert e_m^*y_i^{(m)} \right\vert 
                \end{equation*}
            \end{lemma}

            \begin{remark}\label{b4.38}
                Dieses Lemma ist  das Analogon zu Lemma \ref{l3.17},
                mit $x_m=\tilde{u_i}^{(m)}$, $b=\tilde{\lambda_i}\tilde{u_i}^{(m)}$.
            \end{remark}

            \begin{proof}
                Analog zu Lemma \ref{l3.17}.
            \end{proof}

            \underline{\textbf{Frage:}} Wie viele Schritte brauchen wir bis zur Konvergenz? Wie gut sind 
            unsere Approxumationen?

            \begin{remark}\label{b4.39}
                \begin{enumerate}
                    \item Eine Konvergenanalyse des Arnoldi-Verfahrens für den betragsmäßig größten EW ist schwierig, und für 
                        alle anderen EW noch schwieriger. In der Praxis werden die größten EW am besten approximiert.
                    \item In der Praxis wird das Arnoldi-Verfahren mit einem Restart-Mechanismus angewendet um Speicherplatz zu sparen. 
                \end{enumerate}
            \end{remark}

        \noindent
        \xrfill[0.7ex]{1pt}Ende von Vorlesung 18 am 13.12.2022\xrfill[0.7ex]{1pt}
        
            \underline{\textbf{Frage:}} Was können wir über die Approximation der Eigenwerte 
            von Algorithmus \ref{a4.35} für hermitische Matrizen sagen?

            \begin{remark}\label{b4.40}
                Für hermitische Matrizen kann Algorithmus \ref{a4.35} zum Lanczos-Verfahren vereinfacht werden.
                Falls die EV von $A$ nicht berechnet werden sollen, spart dieser Schritt Speicherplatz und Rechenzeit.
            \end{remark}

            \begin{algorithm}[H]
                        \caption{Lanczos-Verfahren für lineare Eigenwertprobleme}\label{a4.41}
                    \textbf{Input:} $A\in\C^{n\times n}$ hermitsch, $v\in\C^n,v\neq 0,m\in N$\\
                    \textbf{Output:} Approximierte Eigenwerte von $A$
                    \begin{algorithmic}
                    \State $v_1=\frac{v}{\left\Vert v \right\Vert_2}$
                    \For {$j=1,2,\dots,m$}
                        \State Bestimme $T_j$ mittels Lanczos-Verfahren
                        \State (Stop bei Abbruch)
                        \State Löse $T_{j}y_j=\tilde{\lambda}^{(j)}y_j$
                        \State $\rightsquigarrow$ EP $(\tilde{\lambda_i},\tilde{y_{ji}}),i=1,\dots,j$
                        \State Konvergenztest
                    \EndFor 
                    \end{algorithmic}
                \end{algorithm}
            
            \begin{theorem}\label{s4.42}
                Sei $A\in\C^{n\times n}$ hermitsch mit $\lambda_1\geq\dots\geq \lambda_n$
                und orthonormalen EV $\eta_1,\dots,\eta_n$.
                Seien $\tilde{\lambda}^{(j)}$ die EW von $T_j$ in Algorithmus \ref{a4.41} und $v_1=\frac{v}{\left\Vert v \right\Vert_2}$ der erste ONB-Basisvektor 
                von $K_j(A,v)$.
                
                Dann gilt \begin{equation*}
                    \lambda_1\geq \tilde{\lambda_1}^{(j)}\geq \lambda_1-(\lambda_1-\lambda_n)\frac{\tan^2(\angle(v_1,\eta_1))}{T_{j-1}^2(1+2\rho_1)}
                \end{equation*}
                mit $\rho_1=\frac{\lambda_1-\lambda_2}{\lambda_2-\lambda_n}$
            \end{theorem}

            \begin{proof}
                Bemerke: \textbf{a)} $K_j(A,v)=\text{span}(v_1,Av_1,\dots ,A^{j-1}v_1)=\{P(A)v_1,P\in\Pi_{j-1}\}$
                
                \textbf{b)} $v_1=\sum_{j=1}^n\xi_i\eta_i,\xi_i\stackrel{\text{Satz } \ref{s1.16}}{=}\langle v_1,\eta_i \rangle_2=\cos(\angle(v_1,\eta_1))$

                \textbf{c)} Der größte Eigenwert $\zeta$ einer hermitsche Matrix $B\in\C^{n\times n}$ ist gegeben durch

                \[
                    \zeta=\max_{x\in C^n,x\neq 0}\frac{x^* B x}{x^* x}    \text{  Blatt 6, A4}
                \]

                \begin{align*}
                    \tilde{\lambda_1}^{(j)}&=\max_{x\in C^n,x\neq 0}\frac{x^* T_j x}{x^* x}\\
                    &=\max_{x\in C^n,x\neq 0}\frac{x^* V_j^*AV_j x}{x^*V^*_j V_j x}\\
                    &=\max_{x\in K_j(A,v),x\neq 0}\frac{x^* Ax}{x^*x}\leq \max_{x\in C^n,x\neq 0}\frac{x^*Ax}{x^* x}\\
                    &= \max_{P\in \Pi_{j-1},P\neq 0}\underbrace{\frac{v_1^* P(A)AP(A)v_1}{v_1^*P(A)^*P(A)v_1}}_{=(\star)}\\
                    &(\star) = \frac{(\sum_{i=1}^n\xi_iP(A)\eta_i)^*A(\sum_{i=1}^n\xi_iP(A)\eta_i)}{(\sum_{i=1}^n\xi_iP(A)\eta_i)^*(\sum_{i=1}^n\xi_iP(A)\eta_i)}\\
                    &=\frac{\sum_{i=1}^n \left\vert \xi_i \right\vert^2 P(\lambda_i)^2 \lambda_i}{\sum_{i=1}^n \left\vert \xi_i \right\vert^2 P(\lambda_i)^2}\\
                    &=\lambda_1 + \frac{\sum_{i=1}^n \left\vert \xi_i \right\vert^2 P(\lambda_i)^2 (\lambda_i-\lambda_1)}{\sum_{i=1}^n \left\vert \xi_i \right\vert^2 P(\lambda_i)^2}\\
                    &=(\star\star)
                \end{align*}

                \begin{align*}
                    &(\star\star)\geq \lambda_1 +(\lambda_n-\lambda_i)\frac{\sum_{i=2}^n\left\vert \xi_i \right\vert^2P(\lambda_i)^2}{\left\vert \xi_1 \right\vert^2P(\lambda_1)^2+\sum_{i=2}^n\left\vert \xi_i \right\vert^2P(\lambda_i)^2}\\ 
                    &\implies \max_{P\in \Pi_{j-1},P\neq 0}\frac{v_1^* P(A)AP(A)v_1}{v_1^*P(A)^*P(A)v_1}\geq \lambda_1 + \underbrace{(\lambda_n-\lambda_1)}_{\leq 0}\min_{P\in \Pi_{j-1},P\neq 0}\frac{\sum_{i=2}^n\left\vert \xi_i \right\vert^2P(\lambda_i)^2}{\left\vert \xi_1 \right\vert^2P(\lambda_1)^2+\sum_{i=2}^n\left\vert \xi_i \right\vert^2P(\lambda_i)^2}\\ 
                \end{align*}

                \underline{\textbf{Idee:}} Ersetze das Maximum durch ein speziell gewähltes Polynom $P\in\Pi_{j-1}$ mit $\left\vert P(t) \right\vert$ 
                möglichst klein auf $[\lambda_n,\lambda_2]$.
                $\stackrel{\text{Satz }\ref{s3.36}}{\implies}$ Wähle $P(t)=T_{j-1}(x(t))$ mit 
                \[x(t)=1+2\frac{t-\lambda_2}{\lambda_2-\lambda_n}\]
                und Tschebyscheff-Polynom 
                \[T_{j-1}(t)=\cos((j-1)\cos^{-1}(z)),t\in[-1,1]\]
                \begin{equation*}
                    \left\vert P(x(t)) \right\vert\leq 1 \forall t\in[\lambda_n,\lambda_2]
                \end{equation*}

                \begin{align*}
                    &\implies \max_{P\in \Pi_{j-1},P\neq 0}\frac{v_1^* P(A)AP(A)v_1}{v_1^*P(A)^*P(A)v_1}\geq \lambda_1+(\lambda_n-\lambda_1)\frac{\sum_{i=2}^n\left\vert \xi_i \right\vert^2}{\left\vert \xi_1 \right\vert^2P(\lambda_1)^2}\\
                    &\geq \lambda_1+(\lambda_n-\lambda_1)\underbrace{\frac{\sum_{j=2}^n \left\vert \xi_i \right\vert^2}{\left\vert \xi_1 \right\vert^2}}_{=\frac{1-\vert\xi_1\vert^2}{\vert\xi_1vert^2}}\frac{1}{P(\lambda_1)^2}
                \end{align*}

                \begin{align*}
                        \implies \frac{\sum_{j=2}^n \left\vert \xi_i \right\vert^2}{\left\vert \xi_1 \right\vert^2}=\frac{1-\cos^2(\angle(v_1,\eta_1))}{cos^2(\angle(v_1,\eta_1))}=\tan(\angle(v_1,\eta_1))
                \end{align*}

            \end{proof}

            \begin{remark}\label{b4.43}
                \begin{enumerate}
                    \item Der Satz \ref{s4.42} gilt bei exakter Arithmetik und unter der Annahme, das die EW von $T_j$ exakt berechnet werden.
                    \item $\rho_1=\frac{\lambda_1-\lambda_2}{\lambda_2-\lambda_n}\geq 0$ ist möglich, d.h. stenggenommen ist $T_{j-1}(1+2\rho_1)$ über die Cosinus-Darstellung nicht definiert.
                        Hier benutzt man die alternative Darstellung \[T_{j-1}(t)=\frac{1}{2}\left(\left(t+\sqrt{t^2-1}\right)^{j-1}+\left(t-\sqrt{t^2-1}\right)^{j-1}\right)\]
                        Wir sehen, dass $T_{j-1}(1+2\rho_1)$ exponentiell steigt, somit fällt der Kehrwert exponentiell.
                    \item Die Aussage des Satzes gilt für den größten EW. Analoge Aussagen für die übrigen EW sind ebenfalls möglich und können dann mit Hilfe der Conrant-Fischer Darstellung gezeigt werden.
                \end{enumerate}
            \end{remark}

            % TODO: GGF Übersicht


            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 19 am 20.12.2022\xrfill[0.7ex]{1pt}
            
    \chapter{Numerische Integration, Revisited}                

        \section{Definition, Wiederholung und Fragestellung}

            \underline{\textbf{Aufgabenstellung:}} Berechne $\int_a^bf(x)w(x)dx, w(x)>0$. mit Hilfe 
            von Punktauswertungen von $f,w$.

            \begin{tcolorbox}[enhanced,breakable,
            title=Motivation]
            Wir können dieses Integral (nach Skalierung) als Erwartungswert von $f$ interpretieren.
            \end{tcolorbox}
            
            \begin{definition}\label{d51}
                Eine \underline{Quadraturformel} zur Approximation von $\int_a^b f(x)w(x)dx$ mit 
                Stützstellen $(x_i)$ und Gewichten $(w_i)$ ist ein gewichteter Mittelwert.
                \begin{equation}
                    Q[f]=\sum_{i=1}^mw_if(x_i)\label{g51}
                \end{equation}
                Wir sprechen von einer \underline{zusammengesetzten Quadraturformel} $Q_n[f]$ auf $[a,b]$, wenn 
                $[a,b]$ in $n$ gleich große Teilintrvalle aufgeteilt, und das Integral auf jedem Teilintervall mit $Q[f]$ berechnet wird.
            \end{definition}

            \begin{definition}\label{d52}
                Eine Quadraturformel $Q[f]$ auf $[a,b]$ hat \underline{Exaktheitsgrad $q$}, falls 
                \begin{equation*}
                    Q[p]=\int_a^b p(x)w(x)dx,p\in\Pi_q.
                \end{equation*}
                Eine zusammengesetzte Quadraturformel $Q_n[f]$ konvergiert mit \underline{Ordnung $s$}, falls 
                \begin{equation*}
                    \left\vert Q_n[f]-\int_a^b f(x)w(x)dx \right\vert = O(n^{-s}),n\to\infty
                \end{equation*}
            \end{definition}

            \underline{\textbf{Idee:}} Konstruktion von QUadraturformeln mit Exaktheitsgrad $q$: Ersetzte $f$ durch ein
            Lagrange-Interplolationplynom

            \begin{equation*}
                f(x)\approx \sum_{i=0}^q f(x_i)L_i(x)
            \end{equation*}

            von Grad $q$ an Stützstellen $x_0,\dots,x_q$

            \begin{align*}
                \implies \int_a^b f(x)w(x)&\approx \int_a^b\sum_{i=0}^qf(x)w(x)dx\\
            \end{align*}

            \begin{equation}
                \sum_{i=0}^q f(x_i)\underbrace{\int_a^b L_i(x)w(x)dx}_{\coloneqq w_i}\label{g52}
            \end{equation}

            Für $x_0=a,x_q=b$, dazwischen equidistant, heißen diese Quadraurformeln \underline{{Newton-Cotes-Formeln}}.

            \begin{theorem}\label{s53}
                Sei $f\in C^{(q+1)}([a,b])$. Die Quadraturformel mit $w=1$ hat Ordnung $q$ und es gilt:
                \begin{equation*}
                    \left\vert \int_a^b f(x)dx-Q[f] \right\vert\leq \frac{\left\Vert f^{(q+1)} \right\Vert_{C([a,b])}}{(q+1)!}\int_a^b v(x)dx,
                \end{equation*}
                wobei $v\in\Pi_{q+1}$ das \underline{Knotenplynom}
                \[v(x)=(x-x_0)\cdot\dots\cdot (x-x_q)\]
                zu $x_0,\dots,x_q$ ist.
            \end{theorem}

            \begin{proof}
                Siehe Alma 2.
            \end{proof}

            \underline{\textbf{Frage:}} Oft müssen wir sehr viele Integrale numerisch ausgerechnet werden, oder 
            die Auswertung von $f$ ist sehr teuer. Können wir bessere Quadraturformeln finden, d.h. mit höhrem Exaktheitsgrad / höherer Ordnung,
            bei gleicher Anzahl von Funktionsauswertungen?

            \begin{theorem}\label{s54}
                Der Exaktheitsgrad der Quadraturformel (\ref{g51}) ist maximal $q=2m-1$.
            \end{theorem}

            \begin{proof}
                Sei $v(x)=(x-x_1)\cdot\dots\cdot(x-x_m)$ das Knotenpolynom zu $x_1,\dots,x_m$.
                \[p(x)=v(x)^2=\prod_{i=1}^m(x-x_i)^2\in\Pi_{2m}.\]
                \[\implies Q[p]=\sum_{i=1}^m w_i \underbrace{p(x_i)}_{=0}=0\]
                \[\int_a^b p(x)w(x)dx=\int_a^b\prod_{i=1}^m(x-x_i)^2 \underbrace{w(x)}_{>0}dx\neq 0\]
            \end{proof}

            \underline{\textbf{Frage:}} Können wir Quadraturformeln (\ref{g51}) mit Exaktheitsgrad $q=2m-1$ finden 
            und systematisch konstruieren?

            \underline{\textbf{Idee:}} Sei 
            \[v(x)=(x-x_1)\cdot\dots\cdot(x-x_m)\] 
            das Knotenplynom und $p\in\Pi_{m-1}$ beliebig.
            \begin{align*}
                vp\in\Pi_{2m-1}, (vp)(x_i)=0,i=1,\dots,m
            \end{align*}

            Annahme: $Q[f]$ hat Exaktheitsgrad $2m-1$

            \begin{equation*}
               (\star) = Q[vp]=\sum_{i=1}^m w_i\underbrace{(vp)(x_i)}_{=0} =0
            \end{equation*}

            \begin{align*}
                &(\star) = \underbrace{\int_a^b v(x)p(x)w(x)dx}_{=\langle v,p \rangle_w \text{ ist ein Skalarprodukt}} \text{ für alle } p\in\Pi_{m-1}\\
                &\implies v\perp \Pi_{m-1} \text{ bzgl. } \langle \cdot,\cdot \rangle_w\\
                &\implies x_1,\dots,x_m \text{ müssen Nst eines Polynomes } v \in \Pi_m, v \perp \Pi_{m-1}
            \end{align*}

        \section{Orthogonalpolynome}

            \underline{\textbf{Generalvoraussetzung:}} Sei $I\subset \R$ (ein Intervall), $w:I\to\R_{>0}$ eine Gewichtsfunktion mit 
            \[\int_I w(x)dx<\infty.\]
            Wir schreiben 
            \[\langle f,g \rangle_w=\int_I f(x)g(x)w(x)dx,\]
            \[\left\Vert f \right\Vert_w=\sqrt{\langle f,f \rangle_w},\]
            und definieren 
            \[L_w^2(I)\coloneqq \{f:I\to\R,\left\Vert f \right\Vert_w<\infty\}\]


            \begin{theorem}\label{s55}
                Zu jeder Gewichtfunktion $w$ und zugehörigem Skalarprodukt existiert eine 
                eindeutige Folge $\{u_n\}_{n=0}^\infty$ von Polynomen $u_n\in\Pi_n$ mit
                \begin{align*}
                    u_n(x)=\gamma_nx^n + \dots, \gamma_n>0
                \end{align*} 
                und 
                \begin{align*}
                    \langle u_n,u_m \rangle_w=\delta_{nm}.
                \end{align*}

                Außerdem gilt mit $u_{-1}\coloneqq 0$, dass 
                \[u_0=\frac{1}{\left\Vert 1 \right\Vert_w}\]
                \begin{equation}
                    a_{n+1}u_{n+1}(x)=(x-b_n)u_n(x)-a_nu_{n-1}(x),n\geq 0\label{5.4}
                \end{equation}

                mit $a_n=\frac{\gamma_{n-1}}{\gamma_n}$ und $b_n=\langle xu_n, u_n \rangle_w$

            \end{theorem}

            \begin{proof}
                Per Induktion.
                \underline{\textbf{IV}}: $n=0$ klar.

                \underline{\textbf{IS}}: $n\implies n+1$ Seien $u_{-1},u_0,\dots,u_n$ wie im Satz, d.h. 
                $u_0,\dots,u_n$ ist eine ONB.

                \begin{equation}
                    p_{n+1} = (x-b_n)u_n(x)-a_nu_{n-1} \label{g5.5}
                \end{equation}

                \begin{align*}
                    \implies \langle p_{m+1},u_n \rangle_w&=\langle xu_{n},u_n \rangle_w-b_n \langle u_n,u_n \rangle_w-a_n \langle u_{n-1},u_n \rangle_w \\
                    &=0
                \end{align*}

                \begin{align*}
                    \langle p_{n+1},u_{n-1} \rangle_w&=\langle xu_n,u_{n-1} \rangle_w-b_n \langle u_n,u_{n-1} \rangle_w-a_n \langle u_{n-1},u_{n-1} \rangle_w\\
                    &=\langle u_n,xu_{n-1}, \rangle_w\\
                    &=\langle u_n,xu_{n-1} \rangle_w-a_n\\
                    &\stackrel{(\ref{g5.4})}{=} \langle u_n,a_nu_n \rangle_w+\langle u_n,b_{n-1}u_{n-1} \rangle_+\langle u_n,a_{n-1}u_{n-2} \rangle_w -a_n\\
                    &=0
                \end{align*}

                Für $0\leq k\leq n-2$:

                \[p_{n+1},u_k=\langle xu_n,u_k \rangle_w-b_n \langle u_n,u_k \rangle_w-a_n \langle u_{n-1},u_k \rangle_w=\langle u_n,\underbrace{xu_k}_{\in\Pi_{k+1}=\text{span}\{u_0,\dots,u_{k+1}\} \perp u_n} \rangle_w=0\]

                \underline{\textbf{Außerdem:}} $p_{n+1}=\gamma_n x^{n+1}+\dots$

                \[\implies u_{n+1}(x)=\frac{p_{n+1}(x)}{\left\Vert p_{n+1} \right\Vert_w}\]

                $\implies \gamma_{n+1}>0$

                \underline{\textbf{z.z.}} $p_{n+1}=a_{n+1}u_{n+1}$:

                \[\frac{\gamma_n}{\left\Vert p_{n+1} \right\Vert_w}=\gamma_{n+1}\implies \left\Vert p_{n+1} \right\Vert_w=\frac{\gamma_n}{\gamma_{n+1}}\]

            \end{proof}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 20 am 10.01.2023\xrfill[0.7ex]{1pt}

            \begin{remark}\label{b56}
                Die Drei-Term-Rekursion ist nicht notwendig für die Eindeutigkeit:

                Seien $u_n^{(1)},u_n^{(2)}\in\Pi_n$ mit $u_n^{(i)}\perp \Pi_{n-1}=\text{span}\{u_0,\dots,u_{n-1}\}$.
                \begin{align*}
                    &\implies\Pi_n=\text{span}\{u_0,\dots,u_n^{(1)}\}\\
                    &\implies u_n^{(2)}=\sum_{i=1}^{n-1}\langle u_n^{(2)},u_i \rangle u_i + \langle u_n^{(2)},u_n^{(1)}u_n^{(1)}=\alpha u_n^{(1)} \rangle
                \end{align*}
                Durch die Normierung des ersten Koeffizienten folgt dann Gleichheit. 
            \end{remark}

            \begin{example}\label{b57}
                \begin{enumerate}
                    \item Die Tschebyscheff-Polynome $T_n(x)=\cos(n\cos^{-1}(x)),x\in[-1,1]$ sind Orthogonalpolynome auf 
                          auf $I=[-1,1]$ mit $w(x)=\frac{1}{\sqrt{1-x^2}}$.
                          Insb. ist hier \[b_n=\langle xu_n,u_n \rangle=\int_{-1}^1x\frac{u_n(x)^2}{\sqrt{1+x^2}}=0.\]
                    \item Die Legendre-Polynome ergeben sich aus $I=[-1,1],w=1$.
                    \item Die Hermite-Polynome ergeben sich aus $I=\R,w(x)=e^{-x^2}$.
                \end{enumerate}
            \end{example}

            \underline{\textbf{Ausgangslage}}: (\ref{g53}) sagt, dass wir die Nullstellen $x_1,\dots,x_m$ des Polynoms $v\in\Pi_m,v\perp\Pi_{m-1}$, bestimmen
            müssen. Bemerkung \ref{b56} sagt, $v=\alpha u_m$, mit $u_m$ aus Satz (\ref{s55}).

            \underline{\textbf{Frage:}} Hat $v=\alpha u_m$, d.h. $u_m$, überhaupt $m$ verschiedene Nullstellen.

            \begin{theorem}\label{s58}
                Das Orthogonalpolynom $u_m, m\in\N_0$, aus Satz \ref{s55} hat genau $m$ einfache Nullstellen in $I$.
            \end{theorem}

            \begin{proof}
                \underline{\textbf{$m=0$:}} $u_0=\text{const}\neq 0$, klar.
            
                \underline{\textbf{$m>0$:}} Seien $\xi_1,\dots,\xi_k\in I$ die Pnkte in $I$ an denen sich das Vorzeichen von $u_m$ 
                ändert.$\implies u_m(\xi_i)=1,i=1,\dots,k$.

                \underline{\textbf{z.z.:}} $k=m$

                Definiere 
                \[q(x)=\begin{cases}
                    1&\text{falls } k=0\\
                    (x-\xi_1)\cdot\dots\cdot(x-\xi_k), & \text{sonst}
                \end{cases}\]
                $\implies q$ wechselt an den gleichen Stellen wie $u_m$ das Vorzeichen in $I$.

                $\implies q(x)u_m(x)w(x)$ wechselt das Vorzeichen nicht in $I$. 

                $\implies 0 \neq \int_i q(x)u_m(x)w(x)dx=\langle q,u_m \rangle_w$
                
                $\implies \text{Grad } q\geq m$

                $\stackrel{u_m\in\Pi_m}{\implies} \text{Grad }q =k =m$.
            \end{proof}

            \underline{\textbf{Also:}} Falls es eine Quadraturformel mit $m\in\N$ Stützstellen und Exaktheitsgrad $q=2m-1$
            gibt, dann sind die Stützstellen $x_1,\dots,x_m$ die Nullstellen des Orthogonalpolynoms aus Satz \ref{s55}.

            \underline{\textbf{Frage:}} Finden wir die entsprechenden Gewichte?

            \begin{theorem}\label{s59}
                Eine Quadraturformel (\ref{g51}) zu den Stützstellen $x_1,\dots,x_m,x_i\neq x_j$ habe den Exaktheitsgrad $q=m-1$.
                Dann sind die Gewichte gegeben durch
                \begin{equation}
                    w_i=\int_I L_i(x)w(x)dx,i=1,\dots,m\label{g56}
                \end{equation}
                mit den Lagrange-Polynomen $L_i\in\Pi_{m-1}$ zu den Stützstellen $x_1,\dots,x_m$.
            \end{theorem}

            \begin{proof}
                $Q$ hat Exaktheitsgrad $q=m-1$. $\implies L_i\in\Pi_{m-1}$ wird exakt integriert

                \begin{align*}
                    \implies \int_I L_i(x)w(x)dx&=Q[L_i]\\
                    &=\sum_{j=1}^m w_j\underbrace{L_i(x_j)}_{=\delta_{ij}}=w_i
                \end{align*}
            \end{proof}

        \section{Gauß-Quadratur}

            \begin{definition}\label{d510}
                Sei $I\subset\R$ ein Intervall und $w$ eine Gewichtsfunktion $w:I\to\R,w>0$. Die $m$-te \textbf{Gaus-Quadraturformel}
                $Q_w^m[f]$ ist definiert durch 
                \begin{equation*}
                    Q_w^m[f]=\sum_{i=1}^m w_if(x_i)\approx \int_I f(x)w(x)dx,
                \end{equation*}
                mit $x_1,\dots,x_m$ den Nullstellen des $m$-ten Orthogonalpolynoms $u_m$ aus Satz \ref{s55} und Gewichten 
                $w_i$ wie in (\ref{g56}).
            \end{definition}

            \begin{corollary}\label{k511}
                Die $m$-te Gauss-Quadraturformel $Q_w^m[f]$ hat tatsächlich Exaktheitsgrad $q=2m-1$.
            \end{corollary}

            \begin{proof}
                Sei $p\in \Pi_{2m-1}$, setze 
                \[\tilde{p}(x)=\sum_{i=1}^m p(x_i)L_i(x)\in\Pi_{m-1}\]

                \underline{\textbf{Fall 1:}} $p\in \Pi_{m-1}\implies p=\tilde{p}$.

                \begin{equation*}
                    \int_Ip(x)w(x)dx=\int_I\tilde{p}(x)w(x)dx = \sum_{i=1}^m p(x_i)\underbrace{\int_I L_i(x)w(x)dx}_{=w_i}=Q_w^m[p]
                \end{equation*}

                \underline{\textbf{Fall 2:}} $p\in \Pi_{2m-1}\setminus\Pi_{m-1}$:

                Definiere 
                \begin{align*}
                    &q(x)=p(x)-\tilde{p}(x)\\
                    &\implies q(x_i)=p(x_i)-\tilde{p}(x_i)=0, i=1,\dots,m\\
                    &\implies q(x)=v(x)r(x) \text{ mit } v(x)=(x_1)\cdot\dots\cdot(x-x_m) \text{ und }\\
                    &r\in\Pi_{m-1}\\
                    &\implies \int_I q(x)w(x)dx =\int_I v(x)r(x)w(x)dx=\langle v,r \rangle_w=\alpha \langle u_m,r \rangle_w=0
                \end{align*}

                \begin{align*}
                    \int_I p(x)w(x)dx=\underbrace{\int_I q(x)w(x)dx}_{=0} + \underbrace{\int_I \tilde{p}(x)w(x)dx}_{=Q_w^m[p]}=Q_w^m[p]
                \end{align*}
            \end{proof}

            \begin{example}\label{b511}
                Die Gaus-Quadraturformel auf $I=[-1,1]$ mit $w=1$ heißt auch \underline{\textbf{Gauss-Legendre-Quadratur}}. Wir haben 
                \begin{tabular}{|c|c|c|}
                    m & $x_i$ & $w_i$\\
                    1 & 0 & 2\\
                    2 & $-\frac{1}{\sqrt{3}},\frac{1}{\sqrt{3}}$ & 1,1\\
                    3 & $-\frac{\sqrt{15}}{5}$,0,$\frac{\sqrt{15}}{5}$ &$\frac{5}{9}$,$\frac{8}{9}$,$\frac{5}{9}$
                \end{tabular} 
            \end{example}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 21 am 12.01.2023\xrfill[0.7ex]{1pt}
            
            \begin{theorem}\label{s512}
                Sei $f\in\C^{2m}(I)$. Dann gilt 
                \[\left\vert \int_I f(x)w(x)dx-Q_w^m(f) \right\vert\leq \frac{\left\Vert f^{(2m)} \right\Vert_{C(I)}}{(2m)!\gamma_m^2}\]
                mit $\gamma_m>0$ aus Satz \ref{s55}.
            \end{theorem}

            \begin{proof}
                \underline{\textbf{1.:}} Finde ``geeignete'' Interpolation: 

                Sei $p\in\Pi_{2m-1}$ , mit 
                \begin{equation}\label{g57}
                    p(x_i)=f(x_i),p'(x_i)=f'(x_i), i=1,\dots,m
                \end{equation}
                Das heißt auch Hermit-Interpolation.

                \underline{\textbf{Z.z.:}} $p$ existiert und ist eindeutig.

                \begin{align*}
                    &\mu:\Pi_{2m-1}\to \R^{2m}\\
                    &p\mapsto (p(x_1),\dots,p(x_n),p'(x_1),\dots,p'(x_n))
                \end{align*}

                \begin{itemize}
                    \item $\mu$ ist linear
                    \item $\mu(p)=0 \implies x_1,\dots,x_n$ sind doppelte Nullstellen
                        $\implies p$ hat $2m$ Nullstellen, d.h. es ist konstant $0$. D.h. $\mu$ is injektiv.
                    \item $\implies \mu$ ist bijektiv (Rang-Defekt Formel). D.h. $p$ existiert und ist eindeutig. 
                \end{itemize}

                \underline{\textbf{2.: Zeige}}  $f(x)-p(x)=\frac{f^{(2m)}(\xi)}{(2m)!}\underbrace{\prod_{i=1}^{m} (x-x_i)^2}_{=s(x)\in\Pi_{2m}}$.

                \underline{\textbf{Definiere:}}
                \[h(t)=f(t)-p(t)-\frac{s(t)}{s(x)}(f(x)-p(x))\]
                \begin{align*}
                    &\implies h \text{ hat doppelte Nullstellen } x_1,\dots, x_n \text{ und eine zusätzliche Nst } t=x\\
                    &\implies h \text{ hat } 2m+1 \text{ Nullstellen}\\
                    &\implies h' \text{ hat } 2m \text{ Nullstellen}\\
                    &\implies h'' \text{ hat } 2m -1\text{ Nullstellen}\\
                    & \vdots \\
                    & \implies h^{(2m)} \text{ hat } 1 \text{ Nullstelle } \xi \in I\\
                    &\implies  0=h^{(2m)}(\xi)=f^{(2m)}(\xi)-\underbrace{p^{(2m)}(\xi)}_{=0}-\frac{(2m)!}{s(x)}(f(x)-p(x))
                \end{align*}

                \begin{equation}\label{g58}
                    \implies f(x)-p(x)=\frac{f^(2m)(\xi)}{(2m)!}s(x)
                \end{equation}

                \underline{\textbf{3.: Fehlerabschätzung}}

                \begin{equation*}
                    \left\vert \int_I f(x)w(x)dx-\underbrace{Q_w^m(f)}_{\stackrel{(\ref{g57})}{=}Q_w^m(p)=\int_I p(x)w(x)dx} \right\vert = (\star)
                \end{equation*}

                \begin{align*}
                    (*) &= \left\vert \int_I (f(x)-p(x))w(x)dx \right\vert \\
                    &\leq \frac{\left\Vert f^{(2m)} \right\Vert}{(2m)!} \int_I \underbrace{s(x)w(x)}_{\geq 0}dx
                \end{align*}

                $s$ hat genau $2m$ Nst und $u_m^2$ hat genau $2m$ Nst, beide haben Grad $2m\implies s=\frac{1}{\gamma_n^2}u_m^2$:

                \begin{align*}
                    &\int_I s(x)w(x)dx\\
                    &=\int_I\frac{u_m(x)^2}{\gamma_m^2} w(x)dx\\
                    &=\frac{1}{\gamma_m}^2 \underbrace{\langle u_m,u_m \rangle_w}_{=\left\Vert u_m \right\Vert_w^2=1}=\frac{1}{\gamma_n^2}
                \end{align*}

            \end{proof}

            \begin{remark}\label{b512}
                Für die Gaus-Legendre-Formel (d.h. $I=[-1,1],w=1$) ist $\gamma_m^2 \approx \frac{1}{\pi}4^m$
            \end{remark}

            \begin{tcolorbox}[enhanced,breakable,
            title=Unabhängig von der Länge von $I$?]
            In der obigen Abschätzung kommt die Länge des Intervalles nicht direkt vor, was einem zunächst komisch vorkommen sollte. 
            Allerdings ist die Abschätzung versteckt doch abhängig von der Länge $L$, da die Leitkoeffizieten $\gamma_n$ kleiner werden müssen, wenn $L$ größer wird damit die Polynome normiert bleiben!
            \end{tcolorbox}
            
        \section{Numerische Berechnung von Gauß-Quadraturformeln}

            \underline{\textbf{Frage:}} Wie bestimmen wir Stützstellen $x_i$ und Gewichte $w_i$ von Gauß-Quadraturformeln numerisch?

            $x_i\to$ Nullstellen der Orthgonal-Polynome

            $w_i=\int_I L_i(x)w(x)dx$

            \underline{\textbf{Erinnerung:}} $u_i(x)=\gamma_ix^i+\dots+$ erfüllen gemäß Satz \ref{s55}:
            \begin{align*}
                &=u_{-1}=0\\
                &u_0=\frac{1}{\left\Vert 1 \right\Vert_w}\\
                a_{i+1}u_{i+1}=(x-b_i)u_i(x)-a_iu_{i-1}(x),i\geq 0
            \end{align*}

            \begin{theorem}\label{s514} % hier war mal 5.12
                Die Nulltellen von $u_i$ stimmen mit den Eigenwerten von 
                \begin{equation*}
                    J_i=\begin{bmatrix}
                        b_1 a_1 & &&\\
                        a_1 & \ddots & \ddots & &\\
                        &\ddots & \ddots & \ddots & \\
                        &&&a_{i-1} & b
                    \end{bmatrix}
                \end{equation*}

                überein. Ist $\lambda$ ein Eigenwert von $J_i$ und $v=\begin{bmatrix}
                    v_1\\
                    \vdots\\
                    v_n
                \end{bmatrix}$ ein zugehöriger Eigenvektor mit $\left\Vert v \right\Vert_2=1$ und $v_1\geq 0$.

                Dann gilt: 

                \begin{equation*}
                    v_k=\gamma u_{k-1}(\lambda), k=1\dots,i
                \end{equation*}

                mit 
                \begin{equation*}
                    \gamma=\left(\sum_{k=0}^{i-1}u_k(\lambda)^2\right)^{-\frac{1}{2}}.
                \end{equation*}
            \end{theorem}

            \begin{proof}
                Satz \ref{s58} sagt, dass die $u_i$ $i$ verschiedene, einfache Nullstellen hat. 
                
                Sei $\lambda$ eine dieser Nullstellen und $v_kP\gamma u_{k-1}(\lambda)$ mit $\gamma\in\R\setminus\{0\}$ beliebig.
            
                Überlegung:
                 
                \begin{align*}
                    (J_jv)_{k} &= b_k v_k+ a_{k-1} v_{k-1}+a_{k} v_{k+1}\stackrel{!}{=}\\
                    &=\gamma(\underbrace{b_k u_{k-1}({\lambda}) + a_{k-1}u_{k-2}(\lambda)+a_ku_k(\lambda)}_{\stackrel{(\ref{g54})}{=}\lambda u_{k-1}(\lambda)}) \\
                    &= \lambda v_k, k=2,\dots, i-1
                \end{align*}

                $k=1$: $v_0=\gamma u_{-1}(\lambda)=0$

                $k=i$: $v_{i+1}=\gamma u_i(\lambda)=0$ (da, Nst).

                $\implies (\lambda, v)$ ist EP von $J_i$.

                \underline{\textbf{Z.z}} $\gamma=\left(\sum_{k=0}^{i-1}u_k(\lambda)^2\right)^{-\frac{1}{2}}, v_1\geq 0$.

                $1\stackrel{!}{=}\left\Vert v \right\Vert_2^2 = \sum_{k=1}^{i}(\gamma u_{k-1}(\lambda))^2 = \gamma^2 \sum_{k=1}^i u_{k-1}(\lambda)^2$

                $\implies \gamma=\left(\sum_{k=1}^{i-1}u_k(\lambda)^2\right)^{-\frac{1}{2}}$

                $v_1=\gamma u_0(\lambda)>0$

            \end{proof}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 22 am 17.01.2023\xrfill[0.7ex]{1pt}
            
            \begin{theorem}\label{s513}
                Die Gewichte der Gauß-Quadratur $Q_w^m[f]$ sind positiv, mit 
                \begin{equation*}
                    w_j=\left(u_m{k=0}^{m-1}u_k(x_j)^2\right)^{-1}, j=1,\dots, m
                \end{equation*}
            \end{theorem}
            \begin{proof}
                Sei $L_j\in\Pi_{m-1}$ das $j$-te Lagrange-Polynom zu $x_1,\dots,x_m$.
                $\implies L_j(x)=\sum_{k=0}^{m-1} \alpha_k u_k(x)$
            
                \begin{align*}
                    \alpha_k&=\langle L_j,u_k \rangle_w
                &=\int_I \underbrace{\underbrace{L_j(x)}_{\in \Pi_{m-1}}\underbrace{u_k(x)}_{\in \Pi_{m-1}}}_{\in \Pi_{2m-2}}w(x)\\
                &=Q_w^m[f]=\sum_{l=0}^m w_l L_j(x_l) u_k(x_k)=w_j u_k(x_j)
                \end{align*}

                Außerdem: 
                \begin{align*}
                    &\left\Vert L_j \right\Vert_w^2 = \sum_{k=0}^{m-1} a_k^2 = \sum_{k=0}^{m-1} w_j^2u_k(x_j)^2=\sum_{k=1}^{m-1}u_k(x_j)^2\\
                    &=\int_I{L_j}(x)^2w(x)dx=\sum_{k=1}^mw_kL_j(x_k)^2 = w_j
                \end{align*}

            \end{proof}

            \begin{corollary}\label{k514}
                Gegeben eine Gewichtsfunktion und die Koeffizienten der zugehörigen 3-Term-Rekursion der Orthogonalpolynome. Seien $(\lambda_i,v_i)$ die EP von der Matrix 
                ()\ref{g5.9)}. Seien die $v_i$ so, dass $\left\Vert v_i \right\Vert_2=1(v_i)_1\geq 0$. Dann sind:
                \begin{align*}
                    &x_i=\lambda_i\\
                    &w_i=(v_i)^2_1 \left\Vert 1 \right\Vert_w^2
                \end{align*}
                Die Stützstellen und Gewichte von $Q_w^i[f]$.
            \end{corollary}

            \begin{proof}
                \underline{\textbf{}Stützstellen $x_i$:} Satz \ref{s512}

                \underline{\textbf{Gewiche $w_i$}} Satz \ref{s512} sagt $(v_i)_1=\gamma u_0(x_i)=\left(\sum_{k=0}^{m-1}u_k(x_j)^2\right)^{-1/2}\frac{1}{\left\Vert 1 \right\Vert_w}$.

            \end{proof}

            \begin{remark}\label{b515}
                $J_i$ ist symmetrisch mit einfachen EW. Das Lösen des EW-Problems ist somit gukonditioniert und kann mit dem 
                $QR$-Algorithmus erfolgen.
            \end{remark}

            \begin{remark}\label{516}
                Das Berechnen von $a_i$ und $b_i$ (und $\left\Vert 1 \right\Vert_w$) ist je nach Gewichtsfunktion nichttrivial. Für die Gauss-Legendre-Quadratur ($I=[-1,1],w=1$) gilt: $(*)$
                \begin{align*}
                    &a_n=\frac{n}{\sqrt{kn^2-1}}\\ % ist das wirklich ein k?
                    &b_n=0
                \end{align*}
            \end{remark}

        \section{Tensorprodukt-Quadratur}

            \underline{\textbf{Problem:}} Wie berechnen wir Integrale mit Integrationsbereich $D\subset \R^d,d>1$?

            \underline{\textbf{Vereinfachung:}} 
            \begin{enumerate}
                \item $D=[a_1,b_1]\times \dots \times [a_d,b_d]\subset \R^d$ hat Tensorprodukt-Struktur.
                \item $D=[0,1]^2=\square$
            \end{enumerate}

            \underline{\textbf{Neue Aufgabe:}}

            Gegeben $f\in C(\square)$, berechne 
            \begin{equation*}
                (I\odot I )[f]\coloneqq \int_\square f(x)dx=\int_0^1\int_0^1 f(y,z)dydz
            \end{equation*}

            \underline{\textbf{Idee:}} Benutze für jedes Integral eine eigene Quadraturformel (z.B. Gauß, summierte Trapeze, etc.)
            
            für $y:$ $Q_y[g]=\sum_{i=1}^m \xi g(y_i)$

            für $z:$ $Q_z[g]=\sum_{i=1}^n \zeta g(z_i)$

            \begin{equation*}
                (Q_y\otimes Q_z)[f]\coloneqq \sum_{i=1}^m\sum_{j=1}^m \underbrace{\xi_i \zeta_j}_{w_{ij}} f(y_i,z_j)
            \end{equation*}


            \begin{theorem}\label{s517}
                Für Quadraturformel $Q_y,Q_z$ mit Exaktheitsgrad mindestens $1$ und positiven Gewichten gilt:
                \begin{equation*}
                    \left\vert (I\otimes I)[f]-(Q_y\otimes Q_x)[f]\leq\sup_{y\in [0,1]} E_z[f(y_i,\cdot)]+\sup_{z\in[0,1]} E_y[f(\cdot,z)] \right\vert
                \end{equation*}
                mit 
                \begin{align*}
                    &E_y(g)=\left\vert I[g]-Q_y[g] \right\vert\\
                    &E_z(g)=\left\vert I[g]-Q_z[g] \right\vert
                \end{align*}
            \end{theorem}

            \begin{proof}
                \begin{align*}
                   & (I\otimes I)[f]-(Q_y\otimes Q_x)[f] \\
                   &=\int_0^1\int_0^1 f(y,z)dzdy -\sum_{i=1}^m\sum_{j=1}^n \xi_i\zeta_j f(y_i,z_i)\\
                   &=\underbrace{\int_0^1\int_0^1 f(y,z) dz-\sum_{j=1}^n \zeta_j f(y,z)dy}_{(I)}+\underbrace{\sum_{j=1}^n\left(\int_0^1 f(y,z_i)dy-\sum_{i=1}^m \xi_i f(y_i,z_i)\right)}_{(II)}
                \end{align*}

                \begin{align*}
                    &\left\vert (I) \right\vert \leq \int_0^1 \left\vert \int_0^1 f(y,z) dz-\sum_{j=1}^n\xi_j f(y,z_i) \right\vert dy\leq 1\cdot \sup_{y\in[0,1]} E_z(f(y,\cdot))\\
                    &\left\vert (II) \right\vert\leq \sum_{j=1}^n\zeta_i \left\vert \int_0^1 f(y,z_i dy-\sum_{i=1}^m\zeta_i f(y,z_i)) \right\vert \leq \sup_{z\in[0,1] E_y[f(\cdot,z)]}
                \end{align*}
            \end{proof}

            \begin{example}\label{b518}
                Berachte $Q_y=Q_z=T_n$,  $T_n$ summierte Trapezregel.

                $\implies E_y(g)=E_z(g)\leq \frac{h^2}{12}\left\Vert g'' \right\Vert_{C([0,1])}$, $h=1/n$.

                Für $f\in\C^2(\square)$ gitl:

                \begin{equation*}
                    \left\vert (I\otimes I)[f]-(T_N\otimes T_N)[f] \right\vert\leq \sup_{y\in[0,1] \frac{h^2}{12}}\left\Vert \frac{\partial^2 f(y,\cdot)}{\partial z^2} \right\Vert_{C^([0,1])}+\sup_{z\in[0,1] \frac{h^2}{12}}\left\Vert \frac{\partial^2 f(\cdot,z)}{\partial y^2} \right\Vert_{C^([0,1])} = (*)
                \end{equation*}

                \begin{equation*}
                    (*) \leq \frac{h^2}{12}\left(\left\Vert \frac{\partial^2 f(y,\cdot)}{\partial z^2} \right\Vert_{C^([0,1])}+\left\Vert \frac{\partial^2 f(\cdot,z)}{\partial y^2} \right\Vert_{C^([0,1])} \right)
                \end{equation*}
                
            \underline{\textbf{Beobachte:}} $T_N\otimes T_N$ hat somit die gleiche Ordnung wie $T_N$ (d.h. Ordnung 2).

            \underline{\textbf{Aber:}} $T_N$ benötigt $N+1$ Funktionsauswertungen, aber $T_N\otimes T_N$ benötigt $(N+1)^2$.
            \end{example}

            \begin{remark}\label{b519}
                Diese Tensorprodukt-Konstruktion kann man auf beliebige endlich Dimensionen mit ähnlichen Aussagen erweitert werden.
                Z.B. hat die Tensorprodukt summierte Trapezregel in $d$ Dimensionen immer noch Ordnung $2$, aber benötigt $(N+1)^d$ Auswertungen. 
                Dies bezeichnen wir als den \underline{\textbf{Fluch der Dimension}}. 
            \end{remark}

            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 23 am 19.01.2023\xrfill[0.7ex]{1pt}
            
        \section{Monte-Carlo-Quadratur, revisited}

            \underline{\textbf{Ziel:}} Wir wollen den Fluch der Dimension ``umgehen''.

            \underline{\textbf{Annahme:}} Analog zur Gauß-Quadratur:

            Sei $D\subset\R^d$ und $w:D\to\R$ eine Gewichtsfunktion, d.h. $w>0$. Sei 

            $$z=\int_D w(x)dx.$$
            
            \underline{\textbf{Beobachtung:}} Für $f:D\to\R$ ist 
            \begin{equation}\label{g59}
                \int_D f(x)w(x)dx=z\underbrace{\int_D f(x)\frac{w(x)}{z}dx}_{=\mathbb{E}[f(X)]}
            \end{equation}
            falls $X$ eine ``stetige'' Zufallsvariable mit Dichtefunktion $\frac{w(x)}{z}$.
            
            \begin{definition}\label{d520}
                Sei $(\Omega,\mathcal{A},P)$ ein Wahrscheinlichkeitsraum und $X:\Omega\to D\subset\R^d$ eine Zufallsvariable. $X$ heißt \underline{\textbf{stetig}},
                falls eine integrierbare Funktion $g:\mathbb{R}^d\to[0,\infty)$ mit 
                \begin{equation*}
                    \int_{\R^d}g(x)dx=1
                \end{equation*}
                existiert sodass
                \begin{equation*}
                    P(\{X\in A\})=\int_A g(x)dx, A\subset R^d
                \end{equation*}
                $g$ heißt Dichtefunktion. 
            \end{definition}

            \underline{\textbf{Idee:}} Analog zu Alma 2, approximiere $\mathbb{E}[P(x)]$ mittels Monte-Carlo-Verfahren.

            \begin{definition}\label{d5.21}
                Seien $X,X_i:\Omega\to D_i$ i.i.d. stetige Zufallsvariablen mit Dichtefunktion $g$ auf einem Wahrscheinlichkeitsraum $(\Omega,\mathcal{A},P)$.
                Sei $f:D\to\R$ stetig. Dann wird 
                \begin{equation*}
                    E_n[f]=\frac{1}{n}\sum_{i=1}^n f(X_i)
                \end{equation*}
                der Monte-Carlo-Schätzer von $\mathbb{E}[f(X)]$ genannt. 

                \underline{\textbf{O.B.d.A:}} Sei $Z=1\implies \int_D f(x)w(x)dx=\mathbb{E}[f(X)]$.
                
            \end{definition}

            \underline{\textbf{Frage:}} Ist $E_n[f]$ eine sinnvolle Quadraturformel?

            \underline{\textbf{Achtung:}} $E_n[f]$ ist eine Zufallsgröße.

            \underline{\textbf{Checks:}} 

            \begin{align*}
                \mathbb{E}[E_n[f]]=\mathbb{E}[\frac{1}{n}\sum_{i=1}^n f(X_i)]=\frac{1}{n}\sum_{i=1}^n\mathbb{E}[f(X_i)]=\E[f(X)]
            \end{align*}

            Varianz:

            \begin{align}\label{g510}
                \V[E_n[f]]=\V[\frac{1}{n}\sum_{i=1}^nf(X_i)]=\frac{1}{n^2}\sum_{i=1}^n \V[f(X_i)]=\frac{\V[f(X)]}{n}
            \end{align}

            
            \begin{theorem}\label{s522}
                Sei $f:D\to\R$ stetig mit 
                \[\vert \int_D f(x)w(x)dx\vert < \infty\]
                \[\int_D f^2(x)w(x)dx < \infty.\]
                Dann gilt: 
                \[\E\left[\left\vert \int_D f(x)w(x)dx-E_n[f] \right\vert\right]\leq \sqrt{\frac{\V[f(X)]}{n}}\]
            \end{theorem}

            \begin{proof}

                \begin{align*}
                    &\E\left[\left\vert \int_D f(x)w(x)dx-E_n[f] \right\vert\right]^2\\
                    &=\left(\int_D \left\vert \int_D f(x)w(x)dx-E_n[f] \right\vert w(x) dx\right)^2\\
                    &=\left(\left(\int_D \left\vert \int_D f(x)w(x)dx-E_n[f] \right\vert \sqrt{w(x)} dx\right)\sqrt{w(x)}\right)^2 = (*)\\
                    &\stackrel{C.S.}{\leq} \int_D\left\vert \int_D f(x)w(x)dx-E_n[f] \right\vert^2w(x)dx \cdot \underbrace{\int_D w(x)dx}_{=1}\\
                    &=\int_D \left\vert \underbrace{\int_D f(x) w(x)dx}_{=\E[f(X)]=\E[E_n[f]]}-E_n[f] \right\vert^2 w(x)dx\\
                    &=\V[E_n[f]]\stackrel{(\ref{g510})}{=} \frac{\V[f(X)]}{n}
                \end{align*}

                $\implies$ Abschätzung.

                \underline{\textbf{z.z.:}} $\V[f(X)]<\infty$

                \begin{align*}
                    0\leq \V[f(X)] = \E[f(X)^2]\underbrace{-\E[f(X)]^2}_{\leq 0} <\infty
                \end{align*}

                nach Vorraussetzung.

            \end{proof}

            \begin{remark}\label{b523}
                \begin{enumerate}
                    \item Satz \ref{s522} ist eine Aussage über den zu erwartenden Fehler. Eine spezifische Realisierung kann beliebig falsch sein.
                    \item Das generieren der nötien (Pseudo-)Zufallszahlen ist nicht trivial.
                    \item Die Konvergenzrate des Monte-Carlo-Schätzers ist $O(N^{\frac{-1}{2}})$, wobei $N$ die Anzahl der Funktionsauswertungstellen ist. 
                          Die Konvergenzrate ist \underline{\textbf{dimensionsunabhängig}}!
                \end{enumerate}
            \end{remark}

            \underline{\textbf{Zum Vergleich:}} Die tensorierte, summierte Trapezregel 
            \[T_n\otimes\dots,\otimes T_n\] 
            auf $[0,1]^d$ hat Konvergenzrate $O(h^2)$ bei $N=(n+1)^d$ Funktionsauswertungen. Es gilt $h=\frac{1}{n}$

            $\implies O(h^2)=O(n^{-2})=O(N^{-2/d})$

            Die Konvergenzrate wird für große $d$ also schlechter.

            \begin{tcolorbox}[enhanced,breakable,
            title=Top 500]
            Es gibt eine Liste der Top 500 Rechner, also die Top 500, welche z.B. auf \href{top500.org}{top500.org} erreichbar ist
            \end{tcolorbox}

        \section{Dünngitterquadratur}

            \underline{\textbf{Frage:}} Falls wir den Fluch der Dimension nicht besiegen können, können wir ihn dann wenigstens eträglich machen?
            
            \begin{definition}\label{d524}
                Sei 
                \[\varphi(x)=\begin{cases}1-\vert x\vert & x\in[-1,1]\\ 0 & \text{sonst}\end{cases}.\]
                    $\Delta_j=\{0,1,\dots,2^j\}$, und 
                \[\varphi_{j,k}=\varphi(2^j x-k)\vert_{[0,1]},k\in\Delta_j,j\in \N_0\]

                $\varphi_{j,k},k\in\Delta_j$ heißt \underline{\textbf{modale Basis}} im Raum der linearen Splines / stückweise stetien Funktionen. 

                \[V_j=\{f\in C([0,1]):f\vert_{2^j,2^j(k+1)}\in \Pi_1,k=0,\dots,2^j-1\}\]

            \end{definition}

            \underline{\textbf{Erinnerung:}} Wir können $f\in \C([0,1])$ mittels stückweiser linearer Interpolation durch Funktionen in $V_j$
            approximieren.

            \[f(x)\approx f_j(x)=\sum_{k\in\Delta_j} f(2^{-j}k)\varphi_{j,k}(x) \]

            \[\implies \int_0^1 f(x)dx\approx \int_0^1 f_j(x)dx=\sum_{k\in\Delta_j}f(2^{-j}k)\underbrace{\int_0^1 \varphi_{j,k}(x)dx}_{=\begin{cases}2^{-(j+1)} & k\in \{0,2^j\}\\ 2^{-j} & \text{sonst}\}\end{cases}}\]
            \[=2^{-(j+1)}(f(0)+f(1)+\sum_{k=1}^{2{j-1}})f(2^{-j}k)=T_{2^j}[f]\]

            Wie haben also die summierte Trapezregel auf komplizierte Art und Weise hergeleitet.
            \noindent
            \xrfill[0.7ex]{1pt}Ende von Vorlesung 24 am 24.01.2023\xrfill[0.7ex]{1pt}
            

\end{document}